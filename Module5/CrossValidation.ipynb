{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Cross Validation and Model Selection\n",
    "\n",
    "In a previous lab you created a model with l2 or ridge regularization and l1 or lasso regularization. In both cases, an apparently optimum value of the regularization parameter was found. This process is an example of **model selection**. The goal of model selection is to find the best performing model for the problem at hand. Model selection is a very general term and can apply to at least the following common cases:\n",
    "- Selection of optimal model **hyperparameters**. Hyperparameters are parameters which determine the characteristics of a model. Hyperparameters are distinct from the model parameters. For example, for the case of l2 regularized regression, the degree of regularization is determined by a hyperparameter, which is distinct from the regression coefficients or parameters. \n",
    "- **Feature selection** is the process of determining which features should be used in a model. \n",
    "- Comparing different model types is an obvious case of model selection. \n",
    "\n",
    "If you are thinking that the model selection process is closely related to model training, you are correct. Model selection is a component of model training. However, one must be careful, as applying a poor model selection method can lead to an over-fit model!\n",
    "\n",
    "## Overview of k-fold cross validation\n",
    "\n",
    "The questions remain, how good are the hyperparameter estimates perviously obtained for the l2 and l1 regularization parameters and are there better ways to estimate these parameters? The answer to both questions is to use **resampling methods**. Resampling methods repeat a calculation multiple times using randomly selected subsets of the complete dataset.  In fact, resampling methods are generally the best approach to model selection problems. \n",
    "\n",
    "\n",
    "**K-folod Cross validation** is a widely used resampling method. In cross validaton a dataset is divided into **k folds**. Each fold contains $\\frac{1}{k}$ cases and is created by **Bernoulli random sampling** of the full data set. A computation is performed on $k-1$ folds of the full dataset. The $k^{th}$ fold is **held back** and is used for testing the result. The compuation is performed $k$ times and model parameters are averaged (mean taken) over the results of the $k$ folds. For each iteration, $k-1$ folds are used for training and the $k^{th}$ fold is used for testing. \n",
    "\n",
    "4-fold cross validation is illustrated in the figure below. To ensure the data are randomly sampled the data is randomly shuffled at the start of the procedure. The random samples can then be efficiently sub-sampled as shown in the figure. The model is trained and tested four times. For each iteration the data is trained with three folds of the data and tested with the fold shown in the dark shading. \n",
    "\n",
    "<img src=\"img/CrossValidation.jpg\" alt=\"Drawing\" style=\"width:750px; height:400px\"/>\n",
    "<center> **Resampling scheme for 4-fold cross validation**</center>\n",
    "\n",
    "## Introduction to nested cross validation\n",
    "\n",
    "Unfortunately, simple cross validation alone does not provide an unbiased approach to model selection. The problem with evaluating model performance with simple cross validation uses the same data samples as the model selection process. This situation will lead to model over fitting wherein the model selection is learned based on the evaluation data. The result is usually unrealistically optimistic model performance estimates.\n",
    "\n",
    "To obtain unbiased estimates of expected model performance while performing model selection, it is necessary to use **nested cross validation**. As the name implies, nested cross validation is performed though a pair of nested CV loops. The outer loop uses a set of folds to perform model evaluation. The inner loop performs model selection using another randomly sampled set of  folds not used for evalution by the outer loop. This algorithm allows model selection and evaluation to proceed with randomly sampled subsets of the full data set, thereby avoiding model selection bias. \n",
    "\n",
    "## Cross validation and compuational efficiency\n",
    "\n",
    "As you may have surmised, cross validation can be compuationally intensive. Processing each fold of a cross validation requires fitting and evaluating the model. It is desireable to compute a reasonable number of folds. Since the results are averaged over the folds, a small number of folds can lead to significant variablity in the final result. However, with large data sets or complex models, the number of folds must be limited in order to complete the cross validation process in a reasonable amount of time. It is, therefore, necessary to trade off accuracy of the cross validation result with the practical consideration of the required compuatational resources. \n",
    "\n",
    "As mentioned earlier, other resampling methods exist. For example, leave-one-out resampling has the same number of folds as data cases. Such methods provide optimal unbiased estimates of model performance. Unfortunately, as you might think, such methods are compuationally intensive and are only suitable for small datasets. In practice k-fold cross validation is a reasonable way to explore bias-variance trade-off with reasonable compuational resources. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data set\n",
    "\n",
    "With the above theory in mind, you will now try an example. \n",
    "\n",
    "As a first step, execute the code in the cell below to load the packages required for this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Import packages\n",
    "library(ggplot2)\n",
    "library(repr)\n",
    "library(dplyr)\n",
    "library(caret)\n",
    "library(glmnet)\n",
    "library(ROCR)\n",
    "\n",
    "options(repr.plot.width=5, repr.plot.height=5) # Set the initial plot area dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, load the preprocessed files containing the features and the labels. The preprocessing includes the following:\n",
    "1. Cleaning missing values.\n",
    "2. Aggregate categories of certain categorical variables. \n",
    "\n",
    "Execute the code in the cell below to load the features and labels as numpy arrays for the example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 999  23\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'X'</li>\n",
       "\t<li>'Customer_ID'</li>\n",
       "\t<li>'checking_account_status'</li>\n",
       "\t<li>'loan_duration_mo'</li>\n",
       "\t<li>'credit_history'</li>\n",
       "\t<li>'purpose'</li>\n",
       "\t<li>'loan_amount'</li>\n",
       "\t<li>'savings_account_balance'</li>\n",
       "\t<li>'time_employed_yrs'</li>\n",
       "\t<li>'payment_pcnt_income'</li>\n",
       "\t<li>'gender_status'</li>\n",
       "\t<li>'other_signators'</li>\n",
       "\t<li>'time_in_residence'</li>\n",
       "\t<li>'property'</li>\n",
       "\t<li>'age_yrs'</li>\n",
       "\t<li>'other_credit_outstanding'</li>\n",
       "\t<li>'home_ownership'</li>\n",
       "\t<li>'number_loans'</li>\n",
       "\t<li>'job_category'</li>\n",
       "\t<li>'dependents'</li>\n",
       "\t<li>'telephone'</li>\n",
       "\t<li>'foreign_worker'</li>\n",
       "\t<li>'bad_credit'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'X'\n",
       "\\item 'Customer\\_ID'\n",
       "\\item 'checking\\_account\\_status'\n",
       "\\item 'loan\\_duration\\_mo'\n",
       "\\item 'credit\\_history'\n",
       "\\item 'purpose'\n",
       "\\item 'loan\\_amount'\n",
       "\\item 'savings\\_account\\_balance'\n",
       "\\item 'time\\_employed\\_yrs'\n",
       "\\item 'payment\\_pcnt\\_income'\n",
       "\\item 'gender\\_status'\n",
       "\\item 'other\\_signators'\n",
       "\\item 'time\\_in\\_residence'\n",
       "\\item 'property'\n",
       "\\item 'age\\_yrs'\n",
       "\\item 'other\\_credit\\_outstanding'\n",
       "\\item 'home\\_ownership'\n",
       "\\item 'number\\_loans'\n",
       "\\item 'job\\_category'\n",
       "\\item 'dependents'\n",
       "\\item 'telephone'\n",
       "\\item 'foreign\\_worker'\n",
       "\\item 'bad\\_credit'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'X'\n",
       "2. 'Customer_ID'\n",
       "3. 'checking_account_status'\n",
       "4. 'loan_duration_mo'\n",
       "5. 'credit_history'\n",
       "6. 'purpose'\n",
       "7. 'loan_amount'\n",
       "8. 'savings_account_balance'\n",
       "9. 'time_employed_yrs'\n",
       "10. 'payment_pcnt_income'\n",
       "11. 'gender_status'\n",
       "12. 'other_signators'\n",
       "13. 'time_in_residence'\n",
       "14. 'property'\n",
       "15. 'age_yrs'\n",
       "16. 'other_credit_outstanding'\n",
       "17. 'home_ownership'\n",
       "18. 'number_loans'\n",
       "19. 'job_category'\n",
       "20. 'dependents'\n",
       "21. 'telephone'\n",
       "22. 'foreign_worker'\n",
       "23. 'bad_credit'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"X\"                        \"Customer_ID\"             \n",
       " [3] \"checking_account_status\"  \"loan_duration_mo\"        \n",
       " [5] \"credit_history\"           \"purpose\"                 \n",
       " [7] \"loan_amount\"              \"savings_account_balance\" \n",
       " [9] \"time_employed_yrs\"        \"payment_pcnt_income\"     \n",
       "[11] \"gender_status\"            \"other_signators\"         \n",
       "[13] \"time_in_residence\"        \"property\"                \n",
       "[15] \"age_yrs\"                  \"other_credit_outstanding\"\n",
       "[17] \"home_ownership\"           \"number_loans\"            \n",
       "[19] \"job_category\"             \"dependents\"              \n",
       "[21] \"telephone\"                \"foreign_worker\"          \n",
       "[23] \"bad_credit\"              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "credit = read.csv('German_Credit_Preped.csv')\n",
    "print(dim(credit))\n",
    "names(credit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code the label\n",
    "\n",
    "The R Caret package computes most performance metrics using the positive cases. For example, recall is a measure of correct classification of positive cases. Therefore, it is important to have the coding of the label correct. The code in the cell below creates a factor (categorical) variable and coerces the levels of the label column, `bad_credit`. Execute this code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>bad</li>\n",
       "\t<li>good</li>\n",
       "\t<li>good</li>\n",
       "\t<li>bad</li>\n",
       "\t<li>good</li>\n",
       "\t<li>good</li>\n",
       "\t<li>good</li>\n",
       "\t<li>good</li>\n",
       "\t<li>bad</li>\n",
       "\t<li>bad</li>\n",
       "</ol>\n",
       "\n",
       "<details>\n",
       "\t<summary style=display:list-item;cursor:pointer>\n",
       "\t\t<strong>Levels</strong>:\n",
       "\t</summary>\n",
       "\t<ol class=list-inline>\n",
       "\t\t<li>'bad'</li>\n",
       "\t\t<li>'good'</li>\n",
       "\t</ol>\n",
       "</details>"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item bad\n",
       "\\item good\n",
       "\\item good\n",
       "\\item bad\n",
       "\\item good\n",
       "\\item good\n",
       "\\item good\n",
       "\\item good\n",
       "\\item bad\n",
       "\\item bad\n",
       "\\end{enumerate*}\n",
       "\n",
       "\\emph{Levels}: \\begin{enumerate*}\n",
       "\\item 'bad'\n",
       "\\item 'good'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. bad\n",
       "2. good\n",
       "3. good\n",
       "4. bad\n",
       "5. good\n",
       "6. good\n",
       "7. good\n",
       "8. good\n",
       "9. bad\n",
       "10. bad\n",
       "\n",
       "\n",
       "\n",
       "**Levels**: 1. 'bad'\n",
       "2. 'good'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] bad  good good bad  good good good good bad  bad \n",
       "Levels: bad good"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp = credit$bad_credit\n",
    "credit$bad_credit <- ifelse(credit$bad_credit == 0, 'good', 'bad')\n",
    "credit$bad_credit <- factor(credit$bad_credit, levels = c(\"bad\", \"good\"))\n",
    "credit$bad_credit[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label is not coded as factors in the order required. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale numeric features\n",
    "\n",
    "Cross validation will be used to train the model. Since folds will be selected from the entire dataset the numeric features are scaled in batch. Execute the code in the cell below to accomplish this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>loan_duration_mo</th><th scope=col>loan_amount</th><th scope=col>payment_pcnt_income</th><th scope=col>time_in_residence</th><th scope=col>age_yrs</th><th scope=col>number_loans</th><th scope=col>dependents</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td> 2.2464282 </td><td> 0.9483849 </td><td>-0.86876113</td><td>-0.7645835 </td><td>-1.19202026</td><td>-0.7035652 </td><td>-0.4283287 </td></tr>\n",
       "\t<tr><td>-0.7397312 </td><td>-0.4170067 </td><td>-0.86876113</td><td> 0.1414888 </td><td> 1.18945982</td><td>-0.7035652 </td><td> 2.3323187 </td></tr>\n",
       "\t<tr><td> 1.7487350 </td><td> 1.6323204 </td><td>-0.86876113</td><td> 1.0475610 </td><td> 0.83664795</td><td>-0.7035652 </td><td> 2.3323187 </td></tr>\n",
       "\t<tr><td> 0.2556552 </td><td> 0.5655086 </td><td> 0.02505181</td><td> 1.0475610 </td><td> 1.54227168</td><td> 1.0276211 </td><td> 2.3323187 </td></tr>\n",
       "\t<tr><td> 1.2510417 </td><td> 2.0477820 </td><td>-0.86876113</td><td> 1.0475610 </td><td>-0.04538171</td><td>-0.7035652 </td><td> 2.3323187 </td></tr>\n",
       "\t<tr><td> 0.2556552 </td><td>-0.1552623 </td><td> 0.02505181</td><td> 1.0475610 </td><td> 1.54227168</td><td>-0.7035652 </td><td>-0.4283287 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllll}\n",
       " loan\\_duration\\_mo & loan\\_amount & payment\\_pcnt\\_income & time\\_in\\_residence & age\\_yrs & number\\_loans & dependents\\\\\n",
       "\\hline\n",
       "\t  2.2464282  &  0.9483849  & -0.86876113 & -0.7645835  & -1.19202026 & -0.7035652  & -0.4283287 \\\\\n",
       "\t -0.7397312  & -0.4170067  & -0.86876113 &  0.1414888  &  1.18945982 & -0.7035652  &  2.3323187 \\\\\n",
       "\t  1.7487350  &  1.6323204  & -0.86876113 &  1.0475610  &  0.83664795 & -0.7035652  &  2.3323187 \\\\\n",
       "\t  0.2556552  &  0.5655086  &  0.02505181 &  1.0475610  &  1.54227168 &  1.0276211  &  2.3323187 \\\\\n",
       "\t  1.2510417  &  2.0477820  & -0.86876113 &  1.0475610  & -0.04538171 & -0.7035652  &  2.3323187 \\\\\n",
       "\t  0.2556552  & -0.1552623  &  0.02505181 &  1.0475610  &  1.54227168 & -0.7035652  & -0.4283287 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "loan_duration_mo | loan_amount | payment_pcnt_income | time_in_residence | age_yrs | number_loans | dependents | \n",
       "|---|---|---|---|---|---|\n",
       "|  2.2464282  |  0.9483849  | -0.86876113 | -0.7645835  | -1.19202026 | -0.7035652  | -0.4283287  | \n",
       "| -0.7397312  | -0.4170067  | -0.86876113 |  0.1414888  |  1.18945982 | -0.7035652  |  2.3323187  | \n",
       "|  1.7487350  |  1.6323204  | -0.86876113 |  1.0475610  |  0.83664795 | -0.7035652  |  2.3323187  | \n",
       "|  0.2556552  |  0.5655086  |  0.02505181 |  1.0475610  |  1.54227168 |  1.0276211  |  2.3323187  | \n",
       "|  1.2510417  |  2.0477820  | -0.86876113 |  1.0475610  | -0.04538171 | -0.7035652  |  2.3323187  | \n",
       "|  0.2556552  | -0.1552623  |  0.02505181 |  1.0475610  |  1.54227168 | -0.7035652  | -0.4283287  | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  loan_duration_mo loan_amount payment_pcnt_income time_in_residence\n",
       "1  2.2464282        0.9483849  -0.86876113         -0.7645835       \n",
       "2 -0.7397312       -0.4170067  -0.86876113          0.1414888       \n",
       "3  1.7487350        1.6323204  -0.86876113          1.0475610       \n",
       "4  0.2556552        0.5655086   0.02505181          1.0475610       \n",
       "5  1.2510417        2.0477820  -0.86876113          1.0475610       \n",
       "6  0.2556552       -0.1552623   0.02505181          1.0475610       \n",
       "  age_yrs     number_loans dependents\n",
       "1 -1.19202026 -0.7035652   -0.4283287\n",
       "2  1.18945982 -0.7035652    2.3323187\n",
       "3  0.83664795 -0.7035652    2.3323187\n",
       "4  1.54227168  1.0276211    2.3323187\n",
       "5 -0.04538171 -0.7035652    2.3323187\n",
       "6  1.54227168 -0.7035652   -0.4283287"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_cols = c('loan_duration_mo', 'loan_amount', 'payment_pcnt_income',\n",
    "             'time_in_residence', 'age_yrs', 'number_loans', 'dependents')\n",
    "\n",
    "preProcValues <- preProcess(credit[,num_cols], method = c(\"center\", \"scale\"))\n",
    "credit[,num_cols] = predict(preProcValues, credit[,num_cols])\n",
    "head(credit[,num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize hyperparameters with nested cross validation\n",
    "\n",
    "Given often observed variability in metrics in cross validation, performing model selection from a single training and evaluation is an uncertain proposition at best. Fortunately, the nested cross validation approach provides a better way to perform model selection. However, there is no guarantee that a model selection process will, in fact, improve a model. In some cases, it may be that model selection has minimal impact. \n",
    "\n",
    "The inner cross validation loop is used to find the optimal hyperparameters. The general process is:\n",
    "1. A grid of hyperparameters is defined. The model selection will be performed by comparing performance metrics over this grid. \n",
    "2. The folds for the cross validation are defined.\n",
    "3. For each set of folds models are computed for each combination of hyperparameters from the grid. \n",
    "4. The Average metrics of each model (hyperparameter combination) is computed. The best model is selected based on these averages.\n",
    "\n",
    "Once the inner cross validation determines optimal hyperparameters, a final cross validation is performed to determine how well the final model is expected to perform. This process is know as the inner loop of the nested CV. Notice that by creating these independent fold objects there is no need to actually create nested loops for this process. While conceptually nesting the inner and out loops is not hard, is computationally intensive and is generally avoided.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inner loop\n",
    "\n",
    "The code in the cell below uses the capabilities of the Caret package to perform the inner loop of the cross validation as follows:\n",
    "1. The 'trainControl' object defines a 10 fold cross validation. The `twoClassSummary` function defines the ROC metric as th one used for model selection. \n",
    "2. The `train` function defines the following:\n",
    "  - The model is defined with the R modeling language.\n",
    "  - The data frame to be used specified.\n",
    "  - The `method` argument specifies the R model to use.\n",
    "  - Optional arguments, such as `weights` for the model are defined. \n",
    "  - The metric to be used for model evaluation is specified. \n",
    "  - The `trainControl` object is specified.\n",
    "\n",
    "Execute this code and examine the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "glmnet \n",
       "\n",
       "999 samples\n",
       " 10 predictor\n",
       "  2 classes: 'bad', 'good' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold) \n",
       "Summary of sample sizes: 899, 899, 899, 899, 899, 899, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  alpha  lambda       ROC        Sens       Spec     \n",
       "  0.10   0.000296246  0.7701511  0.6366667  0.7409524\n",
       "  0.10   0.002962460  0.7709641  0.6400000  0.7395238\n",
       "  0.10   0.029624601  0.7725673  0.6366667  0.7495238\n",
       "  0.55   0.000296246  0.7704838  0.6400000  0.7409524\n",
       "  0.55   0.002962460  0.7715832  0.6500000  0.7409524\n",
       "  0.55   0.029624601  0.7700469  0.6466667  0.7266460\n",
       "  1.00   0.000296246  0.7708661  0.6433333  0.7395238\n",
       "  1.00   0.002962460  0.7722919  0.6500000  0.7395238\n",
       "  1.00   0.029624601  0.7592747  0.6700000  0.6966460\n",
       "\n",
       "ROC was used to select the optimal model using the largest value.\n",
       "The final values used for the model were alpha = 0.1 and lambda = 0.0296246."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Create a weight vector for the training cases.\n",
    "weights = ifelse(credit$bad_credit == 'bad', 0.66, 0.34)\n",
    "\n",
    "fitControl = trainControl(method = 'cv',\n",
    "                         number = 10,\n",
    "                         classProbs = TRUE,\n",
    "                         summaryFunction = twoClassSummary)\n",
    "\n",
    "set.seed(9999)\n",
    "cv_mod_roc = train(bad_credit ~ loan_duration_mo + loan_amount +  \n",
    "                                 payment_pcnt_income + age_yrs + \n",
    "                                 checking_account_status + credit_history + \n",
    "                                 purpose + gender_status + time_in_residence +\n",
    "                                 property,\n",
    "                 data = credit, \n",
    "                 method = \"glmnet\", \n",
    "                 weights = weights, \n",
    "                 metric=\"ROC\",\n",
    "                 trControl = fitControl)\n",
    "    \n",
    "cv_mod_roc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grid of hyperpameters searched by the Caret package is over `alpha` and `ambda`. The printed tables shows the values of the metrics as a function of the parameters in the search grid. Sens is short for sensitivity which is the same as global recall and Spec is specificity which is the true negative rate $= \\frac{TN}{TN + FP}$\n",
    "\n",
    "As an alternative, recall can be used as the model selection metric. The code in the cell below uses the `prSummary` function in the `trainControl` object and the `Recall` as the `metric`. Execute this code and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "glmnet \n",
       "\n",
       "999 samples\n",
       " 10 predictor\n",
       "  2 classes: 'bad', 'good' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold, repeated 5 times) \n",
       "Summary of sample sizes: 899, 899, 899, 899, 899, 899, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  alpha  lambda       AUC        Precision  Recall     F        \n",
       "  0.10   0.000296246  0.5753613  0.5253972  0.6526667  0.5797238\n",
       "  0.10   0.002962460  0.5757260  0.5244736  0.6533333  0.5794721\n",
       "  0.10   0.029624601  0.5719016  0.5284786  0.6460000  0.5789299\n",
       "  0.55   0.000296246  0.5751138  0.5251588  0.6526667  0.5795396\n",
       "  0.55   0.002962460  0.5758161  0.5228870  0.6486667  0.5766209\n",
       "  0.55   0.029624601  0.5630046  0.5163738  0.6600000  0.5773171\n",
       "  1.00   0.000296246  0.5753606  0.5249237  0.6526667  0.5794589\n",
       "  1.00   0.002962460  0.5758664  0.5255511  0.6506667  0.5787695\n",
       "  1.00   0.029624601  0.5408680  0.5033709  0.6826667  0.5777324\n",
       "\n",
       "Recall was used to select the optimal model using the largest value.\n",
       "The final values used for the model were alpha = 1 and lambda = 0.0296246."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Create a weight vector for the training cases.\n",
    "weights = ifelse(credit$bad_credit == 'bad', 0.66, 0.34)\n",
    "\n",
    "fitControl = trainControl(method = 'repeatedcv',\n",
    "                         number = 10,\n",
    "                         repeats = 5,\n",
    "                         classProbs = TRUE,\n",
    "                         summaryFunction = prSummary)\n",
    "\n",
    "set.seed(9999)\n",
    "cv_mod_recall = train(bad_credit ~ loan_duration_mo + loan_amount +  \n",
    "                                 payment_pcnt_income + age_yrs + \n",
    "                                 checking_account_status + credit_history + \n",
    "                                 purpose + gender_status + time_in_residence +\n",
    "                                 property,\n",
    "                 data = credit, \n",
    "                 method = \"glmnet\", \n",
    "                 weights = weights, \n",
    "                 metric=\"Recall\",\n",
    "                 trControl = fitControl)\n",
    "    \n",
    "cv_mod_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the second CV are nearly the same as the first one.\n",
    "\n",
    "You can illustrate the performance of the different models over the hyperparameter grid. Execute the code in the cell below to create this display. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlgAAAJYCAMAAACJuGjuAAAAYFBMVEUAAABNTU1oaGh8fHyA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\nKCAWROG/Aea4SmOR2OY3AAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(cv_mod_roc, metric = \"ROC\", plotType = \"level\",\n",
    "     scales = list(x = list(rot = 90)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the best model in the upper left. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outer loop\n",
    "\n",
    "With the model selected, you will now perform the outer loop of the cross validation to verify the performance of the model. Consistent performance across the folds indicates that the model is likely to generalize well when faced with new data values. \n",
    "\n",
    "The code in the cell below executes the outside CV loop:\n",
    "1. The parameter grid is specified to have only the optimal hyperparameters. \n",
    "2. The `trainControl` object uses the `savePredictions` and `returnResamp` arguments to save the results of each of the folds. \n",
    "3. The `tuneGrid` argument of the `train` function is used to specify the parameter grid. \n",
    "\n",
    "Execute this code and examine the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>ROC</th><th scope=col>Sens</th><th scope=col>Spec</th><th scope=col>alpha</th><th scope=col>lambda</th><th scope=col>Resample</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.832    </td><td>0.800    </td><td>0.700    </td><td>1        </td><td>0.0296246</td><td>Fold01   </td></tr>\n",
       "\t<tr><td>0.716    </td><td>0.567    </td><td>0.686    </td><td>1        </td><td>0.0296246</td><td>Fold02   </td></tr>\n",
       "\t<tr><td>0.819    </td><td>0.767    </td><td>0.743    </td><td>1        </td><td>0.0296246</td><td>Fold03   </td></tr>\n",
       "\t<tr><td>0.786    </td><td>0.600    </td><td>0.771    </td><td>1        </td><td>0.0296246</td><td>Fold04   </td></tr>\n",
       "\t<tr><td>0.781    </td><td>0.633    </td><td>0.743    </td><td>1        </td><td>0.0296246</td><td>Fold05   </td></tr>\n",
       "\t<tr><td>0.827    </td><td>0.767    </td><td>0.729    </td><td>1        </td><td>0.0296246</td><td>Fold06   </td></tr>\n",
       "\t<tr><td>0.722    </td><td>0.600    </td><td>0.743    </td><td>1        </td><td>0.0296246</td><td>Fold07   </td></tr>\n",
       "\t<tr><td>0.726    </td><td>0.667    </td><td>0.652    </td><td>1        </td><td>0.0296246</td><td>Fold08   </td></tr>\n",
       "\t<tr><td>0.746    </td><td>0.700    </td><td>0.657    </td><td>1        </td><td>0.0296246</td><td>Fold09   </td></tr>\n",
       "\t<tr><td>0.640    </td><td>0.600    </td><td>0.543    </td><td>1        </td><td>0.0296246</td><td>Fold10   </td></tr>\n",
       "\t<tr><td>0.759    </td><td>0.670    </td><td>0.697    </td><td>1        </td><td>0.0296246</td><td>Mean     </td></tr>\n",
       "\t<tr><td>0.061    </td><td>0.084    </td><td>0.067    </td><td>1        </td><td>0.0296246</td><td>STD      </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllll}\n",
       " ROC & Sens & Spec & alpha & lambda & Resample\\\\\n",
       "\\hline\n",
       "\t 0.832     & 0.800     & 0.700     & 1         & 0.0296246 & Fold01   \\\\\n",
       "\t 0.716     & 0.567     & 0.686     & 1         & 0.0296246 & Fold02   \\\\\n",
       "\t 0.819     & 0.767     & 0.743     & 1         & 0.0296246 & Fold03   \\\\\n",
       "\t 0.786     & 0.600     & 0.771     & 1         & 0.0296246 & Fold04   \\\\\n",
       "\t 0.781     & 0.633     & 0.743     & 1         & 0.0296246 & Fold05   \\\\\n",
       "\t 0.827     & 0.767     & 0.729     & 1         & 0.0296246 & Fold06   \\\\\n",
       "\t 0.722     & 0.600     & 0.743     & 1         & 0.0296246 & Fold07   \\\\\n",
       "\t 0.726     & 0.667     & 0.652     & 1         & 0.0296246 & Fold08   \\\\\n",
       "\t 0.746     & 0.700     & 0.657     & 1         & 0.0296246 & Fold09   \\\\\n",
       "\t 0.640     & 0.600     & 0.543     & 1         & 0.0296246 & Fold10   \\\\\n",
       "\t 0.759     & 0.670     & 0.697     & 1         & 0.0296246 & Mean     \\\\\n",
       "\t 0.061     & 0.084     & 0.067     & 1         & 0.0296246 & STD      \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "ROC | Sens | Spec | alpha | lambda | Resample | \n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 0.832     | 0.800     | 0.700     | 1         | 0.0296246 | Fold01    | \n",
       "| 0.716     | 0.567     | 0.686     | 1         | 0.0296246 | Fold02    | \n",
       "| 0.819     | 0.767     | 0.743     | 1         | 0.0296246 | Fold03    | \n",
       "| 0.786     | 0.600     | 0.771     | 1         | 0.0296246 | Fold04    | \n",
       "| 0.781     | 0.633     | 0.743     | 1         | 0.0296246 | Fold05    | \n",
       "| 0.827     | 0.767     | 0.729     | 1         | 0.0296246 | Fold06    | \n",
       "| 0.722     | 0.600     | 0.743     | 1         | 0.0296246 | Fold07    | \n",
       "| 0.726     | 0.667     | 0.652     | 1         | 0.0296246 | Fold08    | \n",
       "| 0.746     | 0.700     | 0.657     | 1         | 0.0296246 | Fold09    | \n",
       "| 0.640     | 0.600     | 0.543     | 1         | 0.0296246 | Fold10    | \n",
       "| 0.759     | 0.670     | 0.697     | 1         | 0.0296246 | Mean      | \n",
       "| 0.061     | 0.084     | 0.067     | 1         | 0.0296246 | STD       | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "   ROC   Sens  Spec  alpha lambda    Resample\n",
       "1  0.832 0.800 0.700 1     0.0296246 Fold01  \n",
       "2  0.716 0.567 0.686 1     0.0296246 Fold02  \n",
       "3  0.819 0.767 0.743 1     0.0296246 Fold03  \n",
       "4  0.786 0.600 0.771 1     0.0296246 Fold04  \n",
       "5  0.781 0.633 0.743 1     0.0296246 Fold05  \n",
       "6  0.827 0.767 0.729 1     0.0296246 Fold06  \n",
       "7  0.722 0.600 0.743 1     0.0296246 Fold07  \n",
       "8  0.726 0.667 0.652 1     0.0296246 Fold08  \n",
       "9  0.746 0.700 0.657 1     0.0296246 Fold09  \n",
       "10 0.640 0.600 0.543 1     0.0296246 Fold10  \n",
       "11 0.759 0.670 0.697 1     0.0296246 Mean    \n",
       "12 0.061 0.084 0.067 1     0.0296246 STD     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Set the hyperparameter grid to the optimal values from the inside loop\n",
    "paramGrid <- expand.grid(alpha = c(1), lambda = c(0.0296246))\n",
    "\n",
    "fitControl = trainControl(method = 'cv',\n",
    "                         number = 10,\n",
    "                         returnResamp=\"all\",\n",
    "                         savePredictions = TRUE,\n",
    "                         classProbs = TRUE,\n",
    "                         summaryFunction = twoClassSummary)\n",
    "\n",
    "set.seed(9999)\n",
    "cv_mod_outer = train(bad_credit ~ loan_duration_mo + loan_amount +  \n",
    "                                 payment_pcnt_income + age_yrs + \n",
    "                                 checking_account_status + credit_history + \n",
    "                                 purpose + gender_status + time_in_residence +\n",
    "                                 property,\n",
    "                 data = credit, \n",
    "                 method = \"glmnet\", \n",
    "                 weights = weights, , \n",
    "                 tuneGrid = paramGrid, \n",
    "                 metric=\"ROC\",\n",
    "                 trControl = fitControl)\n",
    "    \n",
    "print_metrics = function(mod){\n",
    "    means = c(apply(mod$resample[,1:3], 2, mean), alpha = mod$resample[1,4], \n",
    "                lambda = mod$resample[1,5], Resample = 'Mean')\n",
    "    stds = c(apply(mod$resample[,1:3], 2, sd), alpha = mod$resample[1,4], \n",
    "                lambda = mod$resample[1,5], Resample = 'STD')\n",
    "    out = rbind(mod$resample, means, stds)\n",
    "    out[,1:3] = lapply(out[,1:3], function(x) round(as.numeric(x), 3))\n",
    "    out\n",
    "}\n",
    "print_metrics(cv_mod_outer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the following about these results: \n",
    "1. There is variation in the three performance metrics from fold to fold. This variation gives you an idea of the variability to be expected when the model is placed in production.\n",
    "2. The mean of each metric summarizes the expected performance of the model. \n",
    "3. The standard deviation of each metric summarizes the variability. You can see that the standard deviation is approximately an order of magnitude less than the metric. \n",
    "\n",
    "In summary, these results indicate this model is likely to generalize well since the variation in the performance metrics is limited. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab you have performed by simple cross validation and nested cross validation. Key points and observations are:\n",
    "1. Model selection should be done using a resampling procedure such as nested cross validation. The nested sampling structure is required to prevent bias in model selection wherein the model selected learns the best hyperparameters for the samples used, rather than a model that generalizes well. \n",
    "2. There is significant variation in model performance from fold to fold in cross validation. This variation arrises from the sampling of the data alone and is not a property of any particular mdoel.\n",
    "3. Given the expected sampling variation in cross validation, there is generally considerable uncertainty as to which model is best when performing model selection.  \n",
    "4. The results of the outer loop of the nested cross validation is indicative of the ability of a model to generalize. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
