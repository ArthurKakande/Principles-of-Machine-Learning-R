{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Cross Validation and Model Selection\n",
    "\n",
    "In a previous lab you created a model with l2 or ridge regularization and l1 or lasso regularization. In both cases, an apparently optimum value of the regularization parameter was found. This process is an example of **model selection**. The goal of model selection is to find the best performing model for the problem at hand. Model selection is a very general term and can apply to at least the following common cases:\n",
    "- Selection of optimal model **hyperparameters**. Hyperparameters are parameters which determine the characteristics of a model. Hyperparameters are distinct from the model parameters. For example, for the case of l2 regularized regression, the degree of regularization is determined by a hyperparameter, which is distinct from the regression coefficients or parameters. \n",
    "- **Feature selection** is the process of determining which features should be used in a model. \n",
    "- Comparing different model types is an obvious case of model selection. \n",
    "\n",
    "If you are thinking that the model selection process is closely related to model training, you are correct. Model selection is a component of model training. However, one must be careful, as applying a poor model selection method can lead to an over-fit model!\n",
    "\n",
    "## Overview of k-fold cross validation\n",
    "\n",
    "The questions remain, how good are the hyperparameter estimates perviously obtained for the l2 and l1 regularization parameters and are there better ways to estimate these parameters? The answer to both questions is to use **resampling methods**. Resampling methods repeat a calculation multiple times using randomly selected subsets of the complete dataset.  In fact, resampling methods are generally the best approach to model selection problems. \n",
    "\n",
    "\n",
    "**K-folod Cross validation** is a widely used resampling method. In cross validaton a dataset is divided into **k folds**. Each fold contains $\\frac{1}{k}$ cases and is created by **Bernoulli random sampling** of the full data set. A computation is performed on $k-1$ folds of the full dataset. The $k^{th}$ fold is **held back** and is used for testing the result. The compuation is performed $k$ times and model parameters are averaged (mean taken) over the results of the $k$ folds. For each iteration, $k-1$ folds are used for training and the $k^{th}$ fold is used for testing. \n",
    "\n",
    "4-fold cross validation is illustrated in the figure below. To ensure the data are randomly sampled the data is randomly shuffled at the start of the procedure. The random samples can then be efficiently sub-sampled as shown in the figure. The model is trained and tested four times. For each iteration the data is trained with three folds of the data and tested with the fold shown in the dark shading. \n",
    "\n",
    "<img src=\"img/CrossValidation.jpg\" alt=\"Drawing\" style=\"width:750px; height:400px\"/>\n",
    "<center> **Resampling scheme for 4-fold cross validation**</center>\n",
    "\n",
    "## Introduction to nested cross validation\n",
    "\n",
    "Unfortunately, simple cross validation alone does not provide an unbiased approach to model selection. The problem with evaluating model performance with simple cross validation uses the same data samples as the model selection process. This situation will lead to model over fitting wherein the model selection is learned based on the evaluation data. The result is usually unrealistically optimistic model performance estimates.\n",
    "\n",
    "To obtain unbiased estimates of expected model performance while performing model selection, it is necessary to use **nested cross validation**. As the name implies, nested cross validation is performed though a pair of nested CV loops. The outer loop uses a set of folds to perform model evaluation. The inner loop performs model selection using another randomly sampled set of  folds not used for evalution by the outer loop. This algorithm allows model selection and evaluation to proceed with randomly sampled subsets of the full data set, thereby avoiding model selection bias. \n",
    "\n",
    "## Cross validation and compuational efficiency\n",
    "\n",
    "As you may have surmised, cross validation can be compuationally intensive. Processing each fold of a cross validation requires fitting and evaluating the model. It is desireable to compute a reasonable number of folds. Since the results are averaged over the folds, a small number of folds can lead to significant variablity in the final result. However, with large data sets or complex models, the number of folds must be limited in order to complete the cross validation process in a reasonable amount of time. It is, therefore, necessary to trade off accuracy of the cross validation result with the practical consideration of the required compuatational resources. \n",
    "\n",
    "As mentioned earlier, other resampling methods exist. For example, leave-one-out resampling has the same number of folds as data cases. Such methods provide optimal unbiased estimates of model performance. Unfortunately, as you might think, such methods are compuationally intensive and are only suitable for small datasets. In practice k-fold cross validation is a reasonable way to explore bias-variance trade-off with reasonable compuational resources. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Features and Labels\n",
    "\n",
    "With the above theory in mind, you will now try an example. \n",
    "\n",
    "As a first step, execute the code in the cell below to load the packages required for this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Import packages\n",
    "library(ggplot2)\n",
    "library(repr)\n",
    "library(dplyr)\n",
    "library(caret)\n",
    "library(glmnet)\n",
    "library(ROCR)\n",
    "\n",
    "options(repr.plot.width=5, repr.plot.height=5) # Set the initial plot area dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, load the preprocessed files containing the features and the labels. The preprocessing includes the following:\n",
    "1. Cleaning missing values.\n",
    "2. Aggregate categories of certain categorical variables. \n",
    "\n",
    "Execute the code in the cell below to load the features and labels as numpy arrays for the example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 999  23\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'X'</li>\n",
       "\t<li>'Customer_ID'</li>\n",
       "\t<li>'checking_account_status'</li>\n",
       "\t<li>'loan_duration_mo'</li>\n",
       "\t<li>'credit_history'</li>\n",
       "\t<li>'purpose'</li>\n",
       "\t<li>'loan_amount'</li>\n",
       "\t<li>'savings_account_balance'</li>\n",
       "\t<li>'time_employed_yrs'</li>\n",
       "\t<li>'payment_pcnt_income'</li>\n",
       "\t<li>'gender_status'</li>\n",
       "\t<li>'other_signators'</li>\n",
       "\t<li>'time_in_residence'</li>\n",
       "\t<li>'property'</li>\n",
       "\t<li>'age_yrs'</li>\n",
       "\t<li>'other_credit_outstanding'</li>\n",
       "\t<li>'home_ownership'</li>\n",
       "\t<li>'number_loans'</li>\n",
       "\t<li>'job_category'</li>\n",
       "\t<li>'dependents'</li>\n",
       "\t<li>'telephone'</li>\n",
       "\t<li>'foreign_worker'</li>\n",
       "\t<li>'bad_credit'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'X'\n",
       "\\item 'Customer\\_ID'\n",
       "\\item 'checking\\_account\\_status'\n",
       "\\item 'loan\\_duration\\_mo'\n",
       "\\item 'credit\\_history'\n",
       "\\item 'purpose'\n",
       "\\item 'loan\\_amount'\n",
       "\\item 'savings\\_account\\_balance'\n",
       "\\item 'time\\_employed\\_yrs'\n",
       "\\item 'payment\\_pcnt\\_income'\n",
       "\\item 'gender\\_status'\n",
       "\\item 'other\\_signators'\n",
       "\\item 'time\\_in\\_residence'\n",
       "\\item 'property'\n",
       "\\item 'age\\_yrs'\n",
       "\\item 'other\\_credit\\_outstanding'\n",
       "\\item 'home\\_ownership'\n",
       "\\item 'number\\_loans'\n",
       "\\item 'job\\_category'\n",
       "\\item 'dependents'\n",
       "\\item 'telephone'\n",
       "\\item 'foreign\\_worker'\n",
       "\\item 'bad\\_credit'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'X'\n",
       "2. 'Customer_ID'\n",
       "3. 'checking_account_status'\n",
       "4. 'loan_duration_mo'\n",
       "5. 'credit_history'\n",
       "6. 'purpose'\n",
       "7. 'loan_amount'\n",
       "8. 'savings_account_balance'\n",
       "9. 'time_employed_yrs'\n",
       "10. 'payment_pcnt_income'\n",
       "11. 'gender_status'\n",
       "12. 'other_signators'\n",
       "13. 'time_in_residence'\n",
       "14. 'property'\n",
       "15. 'age_yrs'\n",
       "16. 'other_credit_outstanding'\n",
       "17. 'home_ownership'\n",
       "18. 'number_loans'\n",
       "19. 'job_category'\n",
       "20. 'dependents'\n",
       "21. 'telephone'\n",
       "22. 'foreign_worker'\n",
       "23. 'bad_credit'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"X\"                        \"Customer_ID\"             \n",
       " [3] \"checking_account_status\"  \"loan_duration_mo\"        \n",
       " [5] \"credit_history\"           \"purpose\"                 \n",
       " [7] \"loan_amount\"              \"savings_account_balance\" \n",
       " [9] \"time_employed_yrs\"        \"payment_pcnt_income\"     \n",
       "[11] \"gender_status\"            \"other_signators\"         \n",
       "[13] \"time_in_residence\"        \"property\"                \n",
       "[15] \"age_yrs\"                  \"other_credit_outstanding\"\n",
       "[17] \"home_ownership\"           \"number_loans\"            \n",
       "[19] \"job_category\"             \"dependents\"              \n",
       "[21] \"telephone\"                \"foreign_worker\"          \n",
       "[23] \"bad_credit\"              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "credit = read.csv('German_Credit_Preped.csv')\n",
    "print(dim(credit))\n",
    "names(credit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset\n",
    "\n",
    "You must now create randomly sampled training and test data sets. The `createDataPartition` function from the R caret package is used  to create indices for the training data sample. Execute this code and note the dimensions of the resulting data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>700</li>\n",
       "\t<li>23</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 700\n",
       "\\item 23\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 700\n",
       "2. 23\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 700  23"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>299</li>\n",
       "\t<li>23</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 299\n",
       "\\item 23\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 299\n",
       "2. 23\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 299  23"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set.seed(1955)\n",
    "## Randomly sample cases to create independent training and test data\n",
    "partition = createDataPartition(credit[,'bad_credit'], times = 1, p = 0.7, list = FALSE)\n",
    "training = credit[partition,] # Create the training sample\n",
    "dim(training)\n",
    "test = credit[-partition,] # Create the test sample\n",
    "dim(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale numeric features\n",
    "\n",
    "Numeric features must be rescaled so they have a similar range of values. Rescaling prevents features from having an undue influence on model training simply because then have a larger range of numeric variables. \n",
    "\n",
    "The code in the cell below uses the `preProcess` function from the caret function. The processing is as follows:\n",
    "1. The preprocessing model object is computed. In this case the processing includes centering and scaling the numeric feature. Notice that this model is fit only ot the training data.\n",
    "2. The scalling is appled both the test and training partitions.\n",
    "\n",
    "Execute the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>loan_duration_mo</th><th scope=col>loan_amount</th><th scope=col>payment_pcnt_income</th><th scope=col>time_in_residence</th><th scope=col>age_yrs</th><th scope=col>number_loans</th><th scope=col>dependents</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td> 2.2422240 </td><td> 0.9130877 </td><td>-0.8632953 </td><td>-0.7888462 </td><td>-1.20616846</td><td>-0.7009378 </td><td>-0.429150  </td></tr>\n",
       "\t<tr><td>-0.7420395 </td><td>-0.4105541 </td><td>-0.8632953 </td><td> 0.1208611 </td><td> 1.25859356</td><td>-0.7009378 </td><td> 2.326859  </td></tr>\n",
       "\t<tr><td> 1.7448468 </td><td> 1.5761103 </td><td>-0.8632953 </td><td> 1.0305684 </td><td> 0.89344363</td><td>-0.7009378 </td><td> 2.326859  </td></tr>\n",
       "\t<tr><td> 0.2527150 </td><td> 0.5419186 </td><td> 0.0319739 </td><td> 1.0305684 </td><td> 1.62374349</td><td> 1.0768031 </td><td> 2.326859  </td></tr>\n",
       "\t<tr><td> 1.2474695 </td><td> 1.9788682 </td><td>-0.8632953 </td><td> 1.0305684 </td><td>-0.01943119</td><td>-0.7009378 </td><td> 2.326859  </td></tr>\n",
       "\t<tr><td> 0.2527150 </td><td>-0.1568132 </td><td> 0.0319739 </td><td> 1.0305684 </td><td> 1.62374349</td><td>-0.7009378 </td><td>-0.429150  </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllll}\n",
       " loan\\_duration\\_mo & loan\\_amount & payment\\_pcnt\\_income & time\\_in\\_residence & age\\_yrs & number\\_loans & dependents\\\\\n",
       "\\hline\n",
       "\t  2.2422240  &  0.9130877  & -0.8632953  & -0.7888462  & -1.20616846 & -0.7009378  & -0.429150  \\\\\n",
       "\t -0.7420395  & -0.4105541  & -0.8632953  &  0.1208611  &  1.25859356 & -0.7009378  &  2.326859  \\\\\n",
       "\t  1.7448468  &  1.5761103  & -0.8632953  &  1.0305684  &  0.89344363 & -0.7009378  &  2.326859  \\\\\n",
       "\t  0.2527150  &  0.5419186  &  0.0319739  &  1.0305684  &  1.62374349 &  1.0768031  &  2.326859  \\\\\n",
       "\t  1.2474695  &  1.9788682  & -0.8632953  &  1.0305684  & -0.01943119 & -0.7009378  &  2.326859  \\\\\n",
       "\t  0.2527150  & -0.1568132  &  0.0319739  &  1.0305684  &  1.62374349 & -0.7009378  & -0.429150  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "loan_duration_mo | loan_amount | payment_pcnt_income | time_in_residence | age_yrs | number_loans | dependents | \n",
       "|---|---|---|---|---|---|\n",
       "|  2.2422240  |  0.9130877  | -0.8632953  | -0.7888462  | -1.20616846 | -0.7009378  | -0.429150   | \n",
       "| -0.7420395  | -0.4105541  | -0.8632953  |  0.1208611  |  1.25859356 | -0.7009378  |  2.326859   | \n",
       "|  1.7448468  |  1.5761103  | -0.8632953  |  1.0305684  |  0.89344363 | -0.7009378  |  2.326859   | \n",
       "|  0.2527150  |  0.5419186  |  0.0319739  |  1.0305684  |  1.62374349 |  1.0768031  |  2.326859   | \n",
       "|  1.2474695  |  1.9788682  | -0.8632953  |  1.0305684  | -0.01943119 | -0.7009378  |  2.326859   | \n",
       "|  0.2527150  | -0.1568132  |  0.0319739  |  1.0305684  |  1.62374349 | -0.7009378  | -0.429150   | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  loan_duration_mo loan_amount payment_pcnt_income time_in_residence\n",
       "1  2.2422240        0.9130877  -0.8632953          -0.7888462       \n",
       "2 -0.7420395       -0.4105541  -0.8632953           0.1208611       \n",
       "3  1.7448468        1.5761103  -0.8632953           1.0305684       \n",
       "4  0.2527150        0.5419186   0.0319739           1.0305684       \n",
       "5  1.2474695        1.9788682  -0.8632953           1.0305684       \n",
       "6  0.2527150       -0.1568132   0.0319739           1.0305684       \n",
       "  age_yrs     number_loans dependents\n",
       "1 -1.20616846 -0.7009378   -0.429150 \n",
       "2  1.25859356 -0.7009378    2.326859 \n",
       "3  0.89344363 -0.7009378    2.326859 \n",
       "4  1.62374349  1.0768031    2.326859 \n",
       "5 -0.01943119 -0.7009378    2.326859 \n",
       "6  1.62374349 -0.7009378   -0.429150 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_cols = c('loan_duration_mo', 'loan_amount', 'payment_pcnt_income',\n",
    "             'time_in_residence', 'age_yrs', 'number_loans', 'dependents')\n",
    "preProcValues <- preProcess(training[,num_cols], method = c(\"center\", \"scale\"))\n",
    "\n",
    "training[,num_cols] = predict(preProcValues, training[,num_cols])\n",
    "test[,num_cols] = predict(preProcValues, test[,num_cols])\n",
    "head(training[,num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the logistic regression model\n",
    "\n",
    "To create a baseline for comparison you will now create a logistic regression model without cross validation. This model uses a fixed set of hyperparameters. You will compare the performance of this model with the cross validation results computed subsequently. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is time to compute the logistic regression model. The code in the cell below does the following:\n",
    "1. Define a logistic regression model object using the R generalized linear model or `glm` method. The folowing arguments a are used:\n",
    "  - An R model formula. \n",
    "  - Weights for the cases to help compensate of the class imbalance.\n",
    "  - The distribution family of the response is specified as `quasibinomial`. A quasibinomial distribution is similar to a binomial distribution but accounts for greater dispursion or larger errors. \n",
    "  -\n",
    "2. The `predict` method is used to compute the log-likelihoods for each case in the test data.\n",
    "3. A threshold is applied to the log-likelihoods to compute categorical scores.\n",
    "\n",
    "Execute this code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>bad_credit</th><th scope=col>score</th><th scope=col>probs</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>13</th><td>1         </td><td>1         </td><td>0.55602394</td></tr>\n",
       "\t<tr><th scope=row>14</th><td>0         </td><td>1         </td><td>0.74821455</td></tr>\n",
       "\t<tr><th scope=row>17</th><td>0         </td><td>1         </td><td>0.92552325</td></tr>\n",
       "\t<tr><th scope=row>19</th><td>0         </td><td>0         </td><td>0.21258303</td></tr>\n",
       "\t<tr><th scope=row>21</th><td>0         </td><td>0         </td><td>0.26270505</td></tr>\n",
       "\t<tr><th scope=row>22</th><td>0         </td><td>0         </td><td>0.23380177</td></tr>\n",
       "\t<tr><th scope=row>23</th><td>0         </td><td>0         </td><td>0.10091975</td></tr>\n",
       "\t<tr><th scope=row>29</th><td>1         </td><td>1         </td><td>0.80682452</td></tr>\n",
       "\t<tr><th scope=row>30</th><td>0         </td><td>0         </td><td>0.44129136</td></tr>\n",
       "\t<tr><th scope=row>39</th><td>0         </td><td>0         </td><td>0.25283724</td></tr>\n",
       "\t<tr><th scope=row>41</th><td>0         </td><td>1         </td><td>0.56323496</td></tr>\n",
       "\t<tr><th scope=row>43</th><td>0         </td><td>0         </td><td>0.49298946</td></tr>\n",
       "\t<tr><th scope=row>48</th><td>0         </td><td>0         </td><td>0.16974205</td></tr>\n",
       "\t<tr><th scope=row>51</th><td>0         </td><td>0         </td><td>0.28741479</td></tr>\n",
       "\t<tr><th scope=row>59</th><td>1         </td><td>1         </td><td>0.78145762</td></tr>\n",
       "\t<tr><th scope=row>67</th><td>0         </td><td>1         </td><td>0.72053343</td></tr>\n",
       "\t<tr><th scope=row>68</th><td>1         </td><td>1         </td><td>0.54847243</td></tr>\n",
       "\t<tr><th scope=row>70</th><td>0         </td><td>0         </td><td>0.24685527</td></tr>\n",
       "\t<tr><th scope=row>71</th><td>0         </td><td>0         </td><td>0.06939973</td></tr>\n",
       "\t<tr><th scope=row>78</th><td>0         </td><td>1         </td><td>0.59294611</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       "  & bad\\_credit & score & probs\\\\\n",
       "\\hline\n",
       "\t13 & 1          & 1          & 0.55602394\\\\\n",
       "\t14 & 0          & 1          & 0.74821455\\\\\n",
       "\t17 & 0          & 1          & 0.92552325\\\\\n",
       "\t19 & 0          & 0          & 0.21258303\\\\\n",
       "\t21 & 0          & 0          & 0.26270505\\\\\n",
       "\t22 & 0          & 0          & 0.23380177\\\\\n",
       "\t23 & 0          & 0          & 0.10091975\\\\\n",
       "\t29 & 1          & 1          & 0.80682452\\\\\n",
       "\t30 & 0          & 0          & 0.44129136\\\\\n",
       "\t39 & 0          & 0          & 0.25283724\\\\\n",
       "\t41 & 0          & 1          & 0.56323496\\\\\n",
       "\t43 & 0          & 0          & 0.49298946\\\\\n",
       "\t48 & 0          & 0          & 0.16974205\\\\\n",
       "\t51 & 0          & 0          & 0.28741479\\\\\n",
       "\t59 & 1          & 1          & 0.78145762\\\\\n",
       "\t67 & 0          & 1          & 0.72053343\\\\\n",
       "\t68 & 1          & 1          & 0.54847243\\\\\n",
       "\t70 & 0          & 0          & 0.24685527\\\\\n",
       "\t71 & 0          & 0          & 0.06939973\\\\\n",
       "\t78 & 0          & 1          & 0.59294611\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | bad_credit | score | probs | \n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 13 | 1          | 1          | 0.55602394 | \n",
       "| 14 | 0          | 1          | 0.74821455 | \n",
       "| 17 | 0          | 1          | 0.92552325 | \n",
       "| 19 | 0          | 0          | 0.21258303 | \n",
       "| 21 | 0          | 0          | 0.26270505 | \n",
       "| 22 | 0          | 0          | 0.23380177 | \n",
       "| 23 | 0          | 0          | 0.10091975 | \n",
       "| 29 | 1          | 1          | 0.80682452 | \n",
       "| 30 | 0          | 0          | 0.44129136 | \n",
       "| 39 | 0          | 0          | 0.25283724 | \n",
       "| 41 | 0          | 1          | 0.56323496 | \n",
       "| 43 | 0          | 0          | 0.49298946 | \n",
       "| 48 | 0          | 0          | 0.16974205 | \n",
       "| 51 | 0          | 0          | 0.28741479 | \n",
       "| 59 | 1          | 1          | 0.78145762 | \n",
       "| 67 | 0          | 1          | 0.72053343 | \n",
       "| 68 | 1          | 1          | 0.54847243 | \n",
       "| 70 | 0          | 0          | 0.24685527 | \n",
       "| 71 | 0          | 0          | 0.06939973 | \n",
       "| 78 | 0          | 1          | 0.59294611 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "   bad_credit score probs     \n",
       "13 1          1     0.55602394\n",
       "14 0          1     0.74821455\n",
       "17 0          1     0.92552325\n",
       "19 0          0     0.21258303\n",
       "21 0          0     0.26270505\n",
       "22 0          0     0.23380177\n",
       "23 0          0     0.10091975\n",
       "29 1          1     0.80682452\n",
       "30 0          0     0.44129136\n",
       "39 0          0     0.25283724\n",
       "41 0          1     0.56323496\n",
       "43 0          0     0.49298946\n",
       "48 0          0     0.16974205\n",
       "51 0          0     0.28741479\n",
       "59 1          1     0.78145762\n",
       "67 0          1     0.72053343\n",
       "68 1          1     0.54847243\n",
       "70 0          0     0.24685527\n",
       "71 0          0     0.06939973\n",
       "78 0          1     0.59294611"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "score_model = function(df, threshold){\n",
    "    df$score = ifelse(df$probs > threshold, 1, 0)\n",
    "    df\n",
    "}\n",
    "\n",
    "## Create a weight vector for the training cases.\n",
    "weights = ifelse(training$bad_credit == 1, 0.66, 0.34)\n",
    "\n",
    "set.seed(5566)\n",
    "logistic_mod = glm(bad_credit ~ loan_duration_mo + loan_amount +  \n",
    "                                 payment_pcnt_income + age_yrs + \n",
    "                                 checking_account_status + credit_history + \n",
    "                                 purpose + gender_status + time_in_residence +\n",
    "                                 property, \n",
    "                    weights = weights,\n",
    "                    family = quasibinomial, data = training)\n",
    "\n",
    "test$probs = predict(logistic_mod, newdata = test, type = 'response')\n",
    "test = score_model(test, 0.5)\n",
    "test[1:20, c('bad_credit','score', 'probs')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the log-likelihoods in the right column, the scores in the center column and the labels on the left. A number of errors in the classification are evident. \n",
    "\n",
    "The code in the cell below computes and displays metrics and the ROC curve for the model using the test data subset. This code follows the same receipe used in pervious labs. Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Negative Positive\n",
      "TrueNeg      163       56\n",
      "TruePos       27       53\n",
      "\n",
      "accuracy  = 0.722 \n",
      "precision = 0.486 \n",
      "recall    = 0.662 \n",
      "F1        =  0.561 \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlgAAAJYCAMAAACJuGjuAAAAM1BMVEUAAABNTU1oaGh8fHyM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\nAhEQC0RALBABsUAExAIR/gMECGnhb55deAAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logistic.eval <- function(df){ \n",
    "  # First step is to find the TP, FP, TN, FN cases\n",
    "  df$conf = ifelse(df$bad_credit == 1 & df$score == 1, 'TP',\n",
    "                    ifelse(df$bad_credit == 0 & df$score == 1, 'FP',\n",
    "                           ifelse(df$bad_credit == 0 & df$score == 0, 'TN', 'FN')))\n",
    "\n",
    "  # Elements of the confusion matrix\n",
    "  TP = length(df[df$conf == 'TP', 'conf'])\n",
    "  FP = length(df[df$conf == 'FP', 'conf'])\n",
    "  TN = length(df[df$conf == 'TN', 'conf'])\n",
    "  FN = length(df[df$conf == 'FN', 'conf'])\n",
    "  \n",
    "  ## Confusion matrix as data frame\n",
    "  out = data.frame(Negative = c(TN, FN), Positive = c(FP, TP))\n",
    "  row.names(out) = c('TrueNeg', 'TruePos')\n",
    "  print(out)  \n",
    "  \n",
    "  # Compute and print metrics\n",
    "  P = TP/(TP + FP)\n",
    "  R = TP/(TP + FN)  \n",
    "  F1 = 2*P*R/(P+R)  \n",
    "  cat('\\n')\n",
    "  cat(paste('accuracy  =', as.character(round((TP + TN)/(TP + TN + FP + FN), 3)), '\\n'))      \n",
    "  cat(paste('precision =', as.character(round(P, 3)), '\\n'))     \n",
    "  cat(paste('recall    =', as.character(round(R, 3)), '\\n'))\n",
    "  cat(paste('F1        = ', as.character(round(F1,3)),'\\n'))\n",
    "}\n",
    "\n",
    "ROC_AUC = function(df){\n",
    "    options(repr.plot.width=5, repr.plot.height=5)\n",
    "    pred_obj = prediction(df$probs, df$bad_credit)\n",
    "    perf_obj <- performance(pred_obj, measure = \"tpr\", x.measure = \"fpr\")\n",
    "    AUC = performance(pred_obj,\"auc\")@y.values[[1]] # Access the AUC from the slot of the S4 object\n",
    "    plot(perf_obj)\n",
    "    abline(a=0, b= 1, col = 'red')\n",
    "    text(0.8, 0.2, paste('AUC = ', as.character(round(AUC, 3))))\n",
    "}\n",
    "\n",
    "logistic.eval(test)\n",
    "ROC_AUC(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results look promisting, with most of the metrics having resonable values. The question is now, how will these performance estimates hold up to cross validaton?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validate model\n",
    "\n",
    "To compute a better estimate of model performance, you can perform simple cross validation. The code in the cell performs the following processing:\n",
    "1. The `binary.eval` function returns a data frame with a single row, containing the performance metrics for the fold being evaluated. This code is similar to evaluation code used previously. \n",
    "2. The `Create_Folds` function creates a vector of fold numbers with equal numbers of cases in each fold. The result is an integer index used to determine which case belong to each fold. \n",
    "3. The `Fit_Mod` function is called when training and scoring the model for each fold of the CV. The arguments are the training and test data sets for each fold along with the training case weights. \n",
    "4. The `Cross_Validate_Mod` function performs the following operations:\n",
    " - The fold indices are computed.\n",
    " - The data frame is randomly suffled to ensure the fold are randomized.\n",
    " - A loop over the folds of the CV calls the `Fit_Mod` function with training and test data partitions determined by the fold indices. \n",
    " - Summary statistics for the CV are computed and appended to the data frame. \n",
    " - The data frame is returned by the function. \n",
    "\n",
    "Execute this code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>fold</th><th scope=col>accuracy</th><th scope=col>precision</th><th scope=col>recall</th><th scope=col>F1</th><th scope=col>AUC</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>1         </td><td>0.77777778</td><td>0.60000000</td><td>0.7500000 </td><td>0.66666667</td><td>0.8301809 </td></tr>\n",
       "\t<tr><td>2         </td><td>0.69696970</td><td>0.61538462</td><td>0.4444444 </td><td>0.51612903</td><td>0.7270723 </td></tr>\n",
       "\t<tr><td>3         </td><td>0.75757576</td><td>0.58823529</td><td>0.6666667 </td><td>0.62500000</td><td>0.7942029 </td></tr>\n",
       "\t<tr><td>4         </td><td>0.76767677</td><td>0.59090909</td><td>0.8387097 </td><td>0.69333333</td><td>0.8690702 </td></tr>\n",
       "\t<tr><td>5         </td><td>0.71717172</td><td>0.57894737</td><td>0.6470588 </td><td>0.61111111</td><td>0.7859729 </td></tr>\n",
       "\t<tr><td>6         </td><td>0.70707071</td><td>0.40540541</td><td>0.6818182 </td><td>0.50847458</td><td>0.7880756 </td></tr>\n",
       "\t<tr><td>7         </td><td>0.64646465</td><td>0.40000000</td><td>0.5925926 </td><td>0.47761194</td><td>0.7268519 </td></tr>\n",
       "\t<tr><td>8         </td><td>0.69696970</td><td>0.57142857</td><td>0.6666667 </td><td>0.61538462</td><td>0.7826279 </td></tr>\n",
       "\t<tr><td>9         </td><td>0.63636364</td><td>0.35483871</td><td>0.4074074 </td><td>0.37931034</td><td>0.6723251 </td></tr>\n",
       "\t<tr><td>10        </td><td>0.72727273</td><td>0.47222222</td><td>0.6800000 </td><td>0.55737705</td><td>0.7670270 </td></tr>\n",
       "\t<tr><td>Mean      </td><td>0.71313131</td><td>0.51773713</td><td>0.6375364 </td><td>0.56503987</td><td>0.7743407 </td></tr>\n",
       "\t<tr><td>std       </td><td>0.04499204</td><td>0.09399136</td><td>0.1228554 </td><td>0.09101061</td><td>0.0527533 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllll}\n",
       " fold & accuracy & precision & recall & F1 & AUC\\\\\n",
       "\\hline\n",
       "\t 1          & 0.77777778 & 0.60000000 & 0.7500000  & 0.66666667 & 0.8301809 \\\\\n",
       "\t 2          & 0.69696970 & 0.61538462 & 0.4444444  & 0.51612903 & 0.7270723 \\\\\n",
       "\t 3          & 0.75757576 & 0.58823529 & 0.6666667  & 0.62500000 & 0.7942029 \\\\\n",
       "\t 4          & 0.76767677 & 0.59090909 & 0.8387097  & 0.69333333 & 0.8690702 \\\\\n",
       "\t 5          & 0.71717172 & 0.57894737 & 0.6470588  & 0.61111111 & 0.7859729 \\\\\n",
       "\t 6          & 0.70707071 & 0.40540541 & 0.6818182  & 0.50847458 & 0.7880756 \\\\\n",
       "\t 7          & 0.64646465 & 0.40000000 & 0.5925926  & 0.47761194 & 0.7268519 \\\\\n",
       "\t 8          & 0.69696970 & 0.57142857 & 0.6666667  & 0.61538462 & 0.7826279 \\\\\n",
       "\t 9          & 0.63636364 & 0.35483871 & 0.4074074  & 0.37931034 & 0.6723251 \\\\\n",
       "\t 10         & 0.72727273 & 0.47222222 & 0.6800000  & 0.55737705 & 0.7670270 \\\\\n",
       "\t Mean       & 0.71313131 & 0.51773713 & 0.6375364  & 0.56503987 & 0.7743407 \\\\\n",
       "\t std        & 0.04499204 & 0.09399136 & 0.1228554  & 0.09101061 & 0.0527533 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "fold | accuracy | precision | recall | F1 | AUC | \n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1          | 0.77777778 | 0.60000000 | 0.7500000  | 0.66666667 | 0.8301809  | \n",
       "| 2          | 0.69696970 | 0.61538462 | 0.4444444  | 0.51612903 | 0.7270723  | \n",
       "| 3          | 0.75757576 | 0.58823529 | 0.6666667  | 0.62500000 | 0.7942029  | \n",
       "| 4          | 0.76767677 | 0.59090909 | 0.8387097  | 0.69333333 | 0.8690702  | \n",
       "| 5          | 0.71717172 | 0.57894737 | 0.6470588  | 0.61111111 | 0.7859729  | \n",
       "| 6          | 0.70707071 | 0.40540541 | 0.6818182  | 0.50847458 | 0.7880756  | \n",
       "| 7          | 0.64646465 | 0.40000000 | 0.5925926  | 0.47761194 | 0.7268519  | \n",
       "| 8          | 0.69696970 | 0.57142857 | 0.6666667  | 0.61538462 | 0.7826279  | \n",
       "| 9          | 0.63636364 | 0.35483871 | 0.4074074  | 0.37931034 | 0.6723251  | \n",
       "| 10         | 0.72727273 | 0.47222222 | 0.6800000  | 0.55737705 | 0.7670270  | \n",
       "| Mean       | 0.71313131 | 0.51773713 | 0.6375364  | 0.56503987 | 0.7743407  | \n",
       "| std        | 0.04499204 | 0.09399136 | 0.1228554  | 0.09101061 | 0.0527533  | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "   fold accuracy   precision  recall    F1         AUC      \n",
       "1  1    0.77777778 0.60000000 0.7500000 0.66666667 0.8301809\n",
       "2  2    0.69696970 0.61538462 0.4444444 0.51612903 0.7270723\n",
       "3  3    0.75757576 0.58823529 0.6666667 0.62500000 0.7942029\n",
       "4  4    0.76767677 0.59090909 0.8387097 0.69333333 0.8690702\n",
       "5  5    0.71717172 0.57894737 0.6470588 0.61111111 0.7859729\n",
       "6  6    0.70707071 0.40540541 0.6818182 0.50847458 0.7880756\n",
       "7  7    0.64646465 0.40000000 0.5925926 0.47761194 0.7268519\n",
       "8  8    0.69696970 0.57142857 0.6666667 0.61538462 0.7826279\n",
       "9  9    0.63636364 0.35483871 0.4074074 0.37931034 0.6723251\n",
       "10 10   0.72727273 0.47222222 0.6800000 0.55737705 0.7670270\n",
       "11 Mean 0.71313131 0.51773713 0.6375364 0.56503987 0.7743407\n",
       "12 std  0.04499204 0.09399136 0.1228554 0.09101061 0.0527533"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "binary.eval <- function(df, fold){ \n",
    "  # First step is to find the TP, FP, TN, FN cases\n",
    "  df$conf = ifelse(df$bad_credit == 1 & df$score == 1, 'TP',\n",
    "                    ifelse(df$bad_credit == 0 & df$score == 1, 'FP',\n",
    "                           ifelse(df$bad_credit == 0 & df$score == 0, 'TN', 'FN')))\n",
    "\n",
    "  # Elements of the confusion matrix\n",
    "  TP = length(df[df$conf == 'TP', 'conf'])\n",
    "  FP = length(df[df$conf == 'FP', 'conf'])\n",
    "  TN = length(df[df$conf == 'TN', 'conf'])\n",
    "  FN = length(df[df$conf == 'FN', 'conf'])\n",
    "  \n",
    "  ## Confusion matrix as data frame\n",
    "  out = data.frame(Negative = c(TN, FN), Positive = c(FP, TP))\n",
    "  row.names(out) = c('TrueNeg', 'TruePos')\n",
    "    \n",
    "  # Compute AUC with ROCR package\n",
    "  pred_obj = prediction(df$probs, df$bad_credit)\n",
    "  AUC = performance(pred_obj,\"auc\")@y.values[[1]]  \n",
    "  \n",
    "  # Compute and print metrics\n",
    "  P = TP/(TP + FP)\n",
    "  R = TP/(TP + FN)  \n",
    "  F1 = 2*P*R/(P+R) \n",
    "  data.frame = data.frame(fold = as.character(fold),\n",
    "                          accuracy = (TP + TN)/(TP + TN + FP + FN),\n",
    "                          precision = P,\n",
    "                          recall = R,\n",
    "                          F1 = F1,\n",
    "                          AUC = AUC)\n",
    " }\n",
    "\n",
    "\n",
    "Create_Folds = function(df, folds){\n",
    "    ## Create a vector of the fold assignments\n",
    "    nrows = nrow(df)\n",
    "    ncount = nrows/folds\n",
    "    ## Concatenate vectors of fold number\n",
    "    fold = rep(1, ncount)\n",
    "    for(i in seq(2, folds, by = 1)){\n",
    "        fold = c(fold, rep(i, ncount))\n",
    "    }\n",
    "    fold\n",
    "}\n",
    "\n",
    "Fit_Mod = function(training, test, weights){\n",
    "    set.seed(5566)\n",
    "    logistic_mod = glm(bad_credit ~ loan_duration_mo + loan_amount +  \n",
    "                                 payment_pcnt_income + age_yrs + \n",
    "                                 checking_account_status + credit_history + \n",
    "                                 purpose + gender_status + time_in_residence +\n",
    "                                 property, \n",
    "                    weights = weights,\n",
    "                    family = quasibinomial, data = training)\n",
    "    test$probs = predict(logistic_mod, newdata = test, type = 'response')\n",
    "    test = score_model(test, 0.5)\n",
    "    test\n",
    "}\n",
    "\n",
    "Cross_Validate_Mod = function(df, folds){\n",
    "    ## Create a vector of the fold assignments\n",
    "    fold = Create_Folds(df, folds)\n",
    "    \n",
    "    ## Randomly shuffle the rows of the data frame\n",
    "    shuffle = sample(seq(1, nrow(df), by = 1))\n",
    "    df = df[shuffle,]\n",
    "    \n",
    "    ## Loop over number of folds to fit and evaluate the model\n",
    "    training = df[fold != 1,]\n",
    "    test = df[fold == 1, ]\n",
    "    test = Fit_Mod(training, test, \n",
    "                   weights = ifelse(training$bad_credit == 1, 0.66, 0.34))\n",
    "    evals = binary.eval(test, 1)\n",
    "    for(i in seq(2, folds, by = 1)){\n",
    "        training = df[fold != i,]\n",
    "        test = df[fold == i, ]\n",
    "        test = Fit_Mod(training, test, \n",
    "                       weights = ifelse(training$bad_credit == 1, 0.66, 0.34))\n",
    "        evals = rbind(evals, binary.eval(test, i))\n",
    "    }\n",
    "    \n",
    "    ## Compute some summary statistics and append to the data rame\n",
    "    evals = rbind(evals, data.frame(fold = 'Mean',\n",
    "                          accuracy = mean(evals[,2]),\n",
    "                          precision = mean(evals[,3]),\n",
    "                          recall = mean(evals[,4]),\n",
    "                          F1 = mean(evals[,5]),\n",
    "                          AUC = mean(evals[,6])))\n",
    "    \n",
    "    evals = rbind(evals, data.frame(fold = 'std',\n",
    "                          accuracy = sd(evals[,2]),\n",
    "                          precision = sd(evals[,3]),\n",
    "                          recall = sd(evals[,4]),\n",
    "                          F1 = sd(evals[,5]),\n",
    "                          AUC = sd(evals[,6])))\n",
    "    evals\n",
    "}\n",
    "\n",
    "Cross_Validate_Mod(credit, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there is considerable variability in each of the performance metrics from fold to fold. In several cases the standard deviation is the same order of magnitude as the mean. It is clear that **any one fold does not provide a representative value of the performance metrics**. The later is a key point as to why cross validation is important when evaluating a machine learning model.  \n",
    "\n",
    "Compare the performance metric values to the values obtained for the baseline model you created above. In general the metrics obtained by cross validation are lower. However, the metrics obtained for the baseline model are mostly within 1 standard deviation of the average metrics from cross validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize hyperparameters with nested cross validation\n",
    "\n",
    "Given the variability observed in cross validation, it should be clear that performing model selection from a single training and evauation is an uncertain proposition at best. Fortunately, the nested cross validation approach provides a better way to perform model selection. However, there is no guarantee that a model selection process will, in fact, improve a model. In some cases, it may prove to be that model selection has minimal impact. \n",
    "\n",
    "The inner cross validation loop is used to find the optimal hyperparameters. The code in the cell below uses the facilities of the R caret package as follows:\n",
    "1. The label is transformed to a factor to prevent caret from interpreting the formula as a regression problem.\n",
    "2. A caret `trainControl` object is created which defines repeated cross validation with 10 folds and 5 repeats. This spcification is compuationally intensive and must be reduced with few folds and repeats for large scale problems. \n",
    "3. The caret `train` function is used to train the model, testing combinationns of hyperparameters using cross validation. The model formula, data, model type and trainControl object are all spcified.\n",
    "\n",
    "Once the inner cross validation determines optimal hyperparameters, a finally cross validation is performed to determine how well the final model is expected to perform. Notice that by creating these independent fold objects there is no need to actually create nested loops for this process. While conceptually nesting the inner and out loops is not hard, it is compuationally intensive and generally avoided. \n",
    "\n",
    "Execute this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "glmnet \n",
       "\n",
       "999 samples\n",
       " 10 predictor\n",
       "  2 classes: '0', '1' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold, repeated 5 times) \n",
       "Summary of sample sizes: 899, 899, 899, 900, 899, 899, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  alpha  lambda       Accuracy   Kappa    \n",
       "  0.10   0.000296246  0.7525717  0.3563847\n",
       "  0.10   0.002962460  0.7521697  0.3532461\n",
       "  0.10   0.029624601  0.7473556  0.3107086\n",
       "  0.55   0.000296246  0.7523717  0.3560001\n",
       "  0.55   0.002962460  0.7517697  0.3523025\n",
       "  0.55   0.029624601  0.7423515  0.2613406\n",
       "  1.00   0.000296246  0.7523717  0.3560001\n",
       "  1.00   0.002962460  0.7499717  0.3454687\n",
       "  1.00   0.029624601  0.7237313  0.1691376\n",
       "\n",
       "Accuracy was used to select the optimal model using  the largest value.\n",
       "The final values used for the model were alpha = 0.1 and lambda = 0.000296246."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "credit[,'bad_credit_factor'] = as.factor(credit[,'bad_credit'])\n",
    "\n",
    "fitControl = trainControl(method = 'repeatedcv',\n",
    "                         number = 10,\n",
    "                         repeats = 5)\n",
    "\n",
    "set.seed(9999)\n",
    "cv_mod = train(bad_credit_factor ~ loan_duration_mo + loan_amount +  \n",
    "                                 payment_pcnt_income + age_yrs + \n",
    "                                 checking_account_status + credit_history + \n",
    "                                 purpose + gender_status + time_in_residence +\n",
    "                                 property,\n",
    "                 data = credit, \n",
    "                 method = \"glmnet\", \n",
    "                 trControl = fitControl)\n",
    "    \n",
    "cv_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The printed model object displays a table with the values of the hyperparameters (alpha, lambda) and the metrics used to find the optimal hyperparameters (Accuracy, Kappa). The optimal values of  the hyperparameters are printed in the last line. \n",
    "\n",
    "Rembering that there is both a class imbalance and that that false negatives cost the bank five times more than false positives, the accuracy metric is unlikely to be a good choice to optimize hyperparameters. Recall is likely to be a better a better metric for this situation. The code in the cell below adds the following to the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "glmnet \n",
       "\n",
       "999 samples\n",
       " 10 predictor\n",
       "  2 classes: '0', '1' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold, repeated 5 times) \n",
       "Summary of sample sizes: 899, 899, 899, 900, 899, 899, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  alpha  lambda  recall   \n",
       "  0.20   0.05    0.8304486\n",
       "  0.20   0.10    0.8306829\n",
       "  0.20   0.20    0.8077472\n",
       "  0.20   0.30    0.7768135\n",
       "  0.50   0.05    0.8342439\n",
       "  0.50   0.10    0.8324676\n",
       "  0.50   0.20    0.7276858\n",
       "  0.50   0.30    0.6996970\n",
       "  0.75   0.05    0.8396539\n",
       "  0.75   0.10    0.8362149\n",
       "  0.75   0.20    0.6996970\n",
       "  0.75   0.30    0.6996970\n",
       "  1.00   0.05    0.8494352\n",
       "  1.00   0.10    0.8621558\n",
       "  1.00   0.20    0.6996970\n",
       "  1.00   0.30    0.6996970\n",
       "\n",
       "recall was used to select the optimal model using  the largest value.\n",
       "The final values used for the model were alpha = 1 and lambda = 0.1."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "credit[,'bad_credit_factor'] = as.factor(credit[,'bad_credit'])\n",
    "\n",
    "recallSummary = function (data, lev = NULL, model = NULL) {\n",
    "                    out = recall(data$obs, data$pred)  \n",
    "                    names(out) <- \"recall\"\n",
    "                    out\n",
    "}\n",
    "\n",
    "fitControl = trainControl(method = 'repeatedcv',\n",
    "                         number = 10,\n",
    "                         repeats = 5,\n",
    "                         summaryFunction = recallSummary)\n",
    "\n",
    "trGrid <-  expand.grid(alpha = c(0.2, 0.5, 0.75, 1.0), \n",
    "                       lambda = c(0.05, 0.1, 0.2, 0.3))\n",
    "\n",
    "set.seed(9999)\n",
    "cv_mod = train(bad_credit_factor ~ loan_duration_mo + loan_amount +  \n",
    "                                 payment_pcnt_income + age_yrs + \n",
    "                                 checking_account_status + credit_history + \n",
    "                                 purpose + gender_status + time_in_residence +\n",
    "                                 property,\n",
    "                 data = credit, \n",
    "                 method = \"glmnet\", \n",
    "                 metric = \"recall\",\n",
    "                 trControl = fitControl,\n",
    "                 tuneGrid = trGrid,\n",
    "                 weights = ifelse(credit$bad_credit == 1, 0.66, 0.34))\n",
    "    \n",
    "cv_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlgAAAJYCAMAAACJuGjuAAAAYFBMVEUAAABNTU1oaGh8fHyA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\nIALEAiJALCDC/wJ0Cf5bNnsmkgAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(cv_mod, metric = \"recall\", plotType = \"level\",\n",
    "     scales = list(x = list(rot = 90)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>fold</th><th scope=col>accuracy</th><th scope=col>precision</th><th scope=col>recall</th><th scope=col>F1</th><th scope=col>AUC</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>1         </td><td>0.59259259</td><td>0.3728814 </td><td>0.7586207 </td><td>0.50000000</td><td>0.65866434</td></tr>\n",
       "\t<tr><td>2         </td><td>0.73737374</td><td>0.5750000 </td><td>0.7187500 </td><td>0.63888889</td><td>0.82625933</td></tr>\n",
       "\t<tr><td>3         </td><td>0.54545455</td><td>0.3387097 </td><td>0.8400000 </td><td>0.48275862</td><td>0.64297297</td></tr>\n",
       "\t<tr><td>4         </td><td>0.58585859</td><td>0.3833333 </td><td>0.8518519 </td><td>0.52873563</td><td>0.71116255</td></tr>\n",
       "\t<tr><td>5         </td><td>0.54545455</td><td>0.3620690 </td><td>0.7241379 </td><td>0.48275862</td><td>0.63423645</td></tr>\n",
       "\t<tr><td>6         </td><td>0.68686869</td><td>0.7058824 </td><td>0.3157895 </td><td>0.43636364</td><td>0.74978430</td></tr>\n",
       "\t<tr><td>7         </td><td>0.66666667</td><td>0.4489796 </td><td>0.7857143 </td><td>0.57142857</td><td>0.73767606</td></tr>\n",
       "\t<tr><td>8         </td><td>0.69696970</td><td>0.5238095 </td><td>0.6875000 </td><td>0.59459459</td><td>0.76236007</td></tr>\n",
       "\t<tr><td>9         </td><td>0.54545455</td><td>0.3606557 </td><td>0.7857143 </td><td>0.49438202</td><td>0.61820926</td></tr>\n",
       "\t<tr><td>10        </td><td>0.55555556</td><td>0.4090909 </td><td>0.8437500 </td><td>0.55102041</td><td>0.63083022</td></tr>\n",
       "\t<tr><td>Mean      </td><td>0.61582492</td><td>0.4480411 </td><td>0.7311829 </td><td>0.52809310</td><td>0.69721556</td></tr>\n",
       "\t<tr><td>std       </td><td>0.06994053</td><td>0.1125506 </td><td>0.1484715 </td><td>0.05792459</td><td>0.06673453</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllll}\n",
       " fold & accuracy & precision & recall & F1 & AUC\\\\\n",
       "\\hline\n",
       "\t 1          & 0.59259259 & 0.3728814  & 0.7586207  & 0.50000000 & 0.65866434\\\\\n",
       "\t 2          & 0.73737374 & 0.5750000  & 0.7187500  & 0.63888889 & 0.82625933\\\\\n",
       "\t 3          & 0.54545455 & 0.3387097  & 0.8400000  & 0.48275862 & 0.64297297\\\\\n",
       "\t 4          & 0.58585859 & 0.3833333  & 0.8518519  & 0.52873563 & 0.71116255\\\\\n",
       "\t 5          & 0.54545455 & 0.3620690  & 0.7241379  & 0.48275862 & 0.63423645\\\\\n",
       "\t 6          & 0.68686869 & 0.7058824  & 0.3157895  & 0.43636364 & 0.74978430\\\\\n",
       "\t 7          & 0.66666667 & 0.4489796  & 0.7857143  & 0.57142857 & 0.73767606\\\\\n",
       "\t 8          & 0.69696970 & 0.5238095  & 0.6875000  & 0.59459459 & 0.76236007\\\\\n",
       "\t 9          & 0.54545455 & 0.3606557  & 0.7857143  & 0.49438202 & 0.61820926\\\\\n",
       "\t 10         & 0.55555556 & 0.4090909  & 0.8437500  & 0.55102041 & 0.63083022\\\\\n",
       "\t Mean       & 0.61582492 & 0.4480411  & 0.7311829  & 0.52809310 & 0.69721556\\\\\n",
       "\t std        & 0.06994053 & 0.1125506  & 0.1484715  & 0.05792459 & 0.06673453\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "fold | accuracy | precision | recall | F1 | AUC | \n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1          | 0.59259259 | 0.3728814  | 0.7586207  | 0.50000000 | 0.65866434 | \n",
       "| 2          | 0.73737374 | 0.5750000  | 0.7187500  | 0.63888889 | 0.82625933 | \n",
       "| 3          | 0.54545455 | 0.3387097  | 0.8400000  | 0.48275862 | 0.64297297 | \n",
       "| 4          | 0.58585859 | 0.3833333  | 0.8518519  | 0.52873563 | 0.71116255 | \n",
       "| 5          | 0.54545455 | 0.3620690  | 0.7241379  | 0.48275862 | 0.63423645 | \n",
       "| 6          | 0.68686869 | 0.7058824  | 0.3157895  | 0.43636364 | 0.74978430 | \n",
       "| 7          | 0.66666667 | 0.4489796  | 0.7857143  | 0.57142857 | 0.73767606 | \n",
       "| 8          | 0.69696970 | 0.5238095  | 0.6875000  | 0.59459459 | 0.76236007 | \n",
       "| 9          | 0.54545455 | 0.3606557  | 0.7857143  | 0.49438202 | 0.61820926 | \n",
       "| 10         | 0.55555556 | 0.4090909  | 0.8437500  | 0.55102041 | 0.63083022 | \n",
       "| Mean       | 0.61582492 | 0.4480411  | 0.7311829  | 0.52809310 | 0.69721556 | \n",
       "| std        | 0.06994053 | 0.1125506  | 0.1484715  | 0.05792459 | 0.06673453 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "   fold accuracy   precision recall    F1         AUC       \n",
       "1  1    0.59259259 0.3728814 0.7586207 0.50000000 0.65866434\n",
       "2  2    0.73737374 0.5750000 0.7187500 0.63888889 0.82625933\n",
       "3  3    0.54545455 0.3387097 0.8400000 0.48275862 0.64297297\n",
       "4  4    0.58585859 0.3833333 0.8518519 0.52873563 0.71116255\n",
       "5  5    0.54545455 0.3620690 0.7241379 0.48275862 0.63423645\n",
       "6  6    0.68686869 0.7058824 0.3157895 0.43636364 0.74978430\n",
       "7  7    0.66666667 0.4489796 0.7857143 0.57142857 0.73767606\n",
       "8  8    0.69696970 0.5238095 0.6875000 0.59459459 0.76236007\n",
       "9  9    0.54545455 0.3606557 0.7857143 0.49438202 0.61820926\n",
       "10 10   0.55555556 0.4090909 0.8437500 0.55102041 0.63083022\n",
       "11 Mean 0.61582492 0.4480411 0.7311829 0.52809310 0.69721556\n",
       "12 std  0.06994053 0.1125506 0.1484715 0.05792459 0.06673453"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Fit_Mod = function(training, test, weights){\n",
    "    set.seed(5566)\n",
    "    x_train = model.matrix(bad_credit ~ loan_duration_mo + loan_amount +  \n",
    "                                 payment_pcnt_income + age_yrs + \n",
    "                                 checking_account_status + credit_history + \n",
    "                                 purpose + gender_status + time_in_residence +\n",
    "                                 property, \n",
    "                           data = training)\n",
    "    y_train = as.matrix(training[, 'bad_credit'])\n",
    "    \n",
    "    \n",
    "    x_test = model.matrix(bad_credit ~ loan_duration_mo + loan_amount +  \n",
    "                                 payment_pcnt_income + age_yrs + \n",
    "                                 checking_account_status + credit_history + \n",
    "                                 purpose + gender_status + time_in_residence +\n",
    "                                 property, \n",
    "                           data = test)\n",
    "    \n",
    "    logistic_mod = glmnet(x_train, y_train, \n",
    "                          weights = weights,\n",
    "                          alpha = 1.0,\n",
    "                          lambda = 0.1,\n",
    "                          family = 'binomial')\n",
    "    test$probs = predict(logistic_mod, newx = x_test, type = 'response')\n",
    "    test = score_model(test, 0.5)\n",
    "    test\n",
    "}\n",
    "\n",
    "set.seed(7777)\n",
    "Cross_Validate_Mod(credit, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important decision in model selection searches is the choice of performance metric used to find the best model. For classification probems Scikit Learn uses accuracy as the default metric. However, as you have seen previously, accuracy is not necessarily the best metric, particularly when there is a class imbalance as is the case here. There are a number of alternatives which one could choose for such a situation. In this case AUC will be used. \n",
    "\n",
    "The code below uses the `inside` k-fold object to execute the inside loop of the nested cross validation. Specifically, the steps are:\n",
    "1. Define a dictionary with the grid of parameter values to search over. In this case there is only one parameter, `C`, with a list of values to try. In a more general case, the dictionary can contain values from multiple parameters, creating a multi-dimensional grid that the cross validation process will iterate over. In this case there are 5 hyperparameter values in the grid and 10-fold cross validation is being used. Thus, the model will be trained and evaluated 50 times. \n",
    "2. The logistic regression model object is defined. \n",
    "3. The cross validation search over the parameter grid is performed using the `GridSearch` function from the Scikit Learn `model_selection` package. Notice that the cross validation folds are computed using the `inside` k-fold object.\n",
    "\n",
    "\n",
    "****\n",
    "**Note:** Somewhat confusingly, the Scikit Learn `LogisticRegression` function uses a regularization parameter `C` which is the inverse of the usual l2 regularization parameter $\\lambda$. Thus, the smaller the parameter the stronger the regulation \n",
    "****\n",
    "\n",
    "Execute this code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross validated grid search obect, `clf`, has been creted. \n",
    "\n",
    "The code in the cell below fits the cross validated model using the `fit`method. The AUC for each hyperparameter and fold is displayed as an array. Finally, the hyperparameter for the model with the best average AUC is displayed.  Execute this code and  examine the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The array of AUC metrics has dimensions 10 folds X  hyperparameter values. As you might expect by now, there is considerable variation in the AUC from fold to fold for each hyperparamter value, or column. \n",
    "\n",
    "Evidently, the optimal hyperparameter value is 1.0. \n",
    "\n",
    "To help understand this behavior a bit more, the code in the cell below does the following:\n",
    "1. Compute and display the mean and standard deviation of the AUC for each hyperparameter value.\n",
    "2. Plot the AUC values for each fold vs. the hyperparameter values. The mean AUC for each hyperparameter value is shown with a red +. \n",
    "\n",
    "Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of points to notice here:\n",
    "1. The mean AUC for each value of the hyperparameter are all within 1 standard deviation of each other. This result indicates that model performance is not sensitive to the choice of hyperparamter. \n",
    "2. Graphically you can see that there is a noticeable variation in the AUC from metric to metric, regardless of hyperparameter. Keep in mind that **this variation is simply a result of random sampling of the data!**\n",
    "\n",
    "Finally, it is time to try execute the outer loop of the nested cross validation to evaluate the performance of the 'best' model selected by the inner loop. In this case, 'best' is quite approximate, since as already noted, the differences in performance between the models is not significant. \n",
    "\n",
    "The code in the cell below executes the outer loop of the nested cross validation using the `cross_val_scores` function from the Scikit Learn `model_selection` package. The folds are determined by the `outside` k-fold object. The mean and standard deviation of the AUC is printed along with the value estimated for each fold. Execute this code and examine the result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, there is considerable variation in AUC across the folds. The mean AUC is a bit lower than estimated for the inner loop of the nested cross validation and the baseline model. However, all of these values are within 1 standard deviation of each other, and thus these differences cannot be considered significant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab you have performed by simple cross validation and nested cross validation. Key points and observations are:\n",
    "1. Model selection should be done using a resampling proceedure such as nested cross validation. The nested sampling structure is required to prevent bias in model selection wherein the model selected learns the best hyperparameters for the samples used, rather than a model that generalizes well. \n",
    "2. There is significant variation in model performance from fold to fold in cross validation. This variation arrises from the sampling of the data alone and is not a property of any particular mdoel.\n",
    "3. Given the expected sampling variation in cross validation, there is generally considerable uncertainty as to which model is best when performing model selection.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
