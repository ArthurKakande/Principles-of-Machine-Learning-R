{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting and the AdaBoost Method\n",
    "\n",
    "Using **ensemble methods** can greatly improve the results achieved with weak machine learning algorithms, also called **weak learners**. Ensemble methods achieve better performance by aggregating the results of many statistically independent models. This process averages out the errors and produces a better, final, prediction. \n",
    "\n",
    "In this lab you will work with widely used ensemble method known as **boosting**. Boosting is a meta-algorithm since the method can be applied to many types of machine learning algorithms. In summary, boosting iteratively improves the learning of the N models by giving greater weight to training cases with larger errors. The basic boosting procedure is simple and follows these steps:\n",
    "1. N learners (machine learning models) are defined.\n",
    "2. Each of i training data cases is given an initial equal weight of 1/i.\n",
    "3. The N learners are trained on the weighted training data cases.\n",
    "4. The prediction is computed based on a aggregation of the learners; averaging over the hypothesis of the N learners. \n",
    "5. Weights for the training data cases are updated based on the aggregated errors made by the learners. Cases with larger errors are given larger weights. \n",
    "6. Steps 3, 4, and 5 are repeated until a convergence criteria is met.\n",
    "\n",
    "**Classification and regression tree models** are the weak learners most commonly used with boosting. In this lab you will work with one of the most widely used and successful boosted methods, known as **AdaBoost** or **adaptive boosting**. AdaBoost uses some large number, N, tree models. The rate at which weights are updated is **adaptive** with the errors. \n",
    "\n",
    "It is important to keep in mind that boosted machine learning is not robust to significant noise or outliers in the training data. The reweighting process gives greater weight to the large errors, and therefore can give undue weight to outliers and errors. In cases where data is noisy, the random forest algorithm may prove to be more robust. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Iris dataset\n",
    "\n",
    "As a first example you will use AdaBoost to classify the species of iris flowers. \n",
    "\n",
    "As a first step, execute the code in the cell below to load the required packages to run the rest of this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: 'dplyr'\n",
      "\n",
      "The following object is masked from 'package:gridExtra':\n",
      "\n",
      "    combine\n",
      "\n",
      "The following objects are masked from 'package:stats':\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "The following objects are masked from 'package:base':\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "Loading required package: lattice\n",
      "Loading required package: survival\n",
      "\n",
      "Attaching package: 'survival'\n",
      "\n",
      "The following object is masked from 'package:caret':\n",
      "\n",
      "    cluster\n",
      "\n",
      "Loading required package: splines\n",
      "Loading required package: parallel\n",
      "Loaded gbm 2.1.3\n",
      "Loading required package: gplots\n",
      "\n",
      "Attaching package: 'gplots'\n",
      "\n",
      "The following object is masked from 'package:stats':\n",
      "\n",
      "    lowess\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Import packages\n",
    "library(ggplot2)\n",
    "library(gridExtra)\n",
    "library(repr)\n",
    "library(dplyr)\n",
    "library(caret)\n",
    "library(gbm)\n",
    "library(e1071)\n",
    "library(ROCR)\n",
    "\n",
    "options(repr.plot.width=4, repr.plot.height=4) # Set the initial plot area dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below displays the head of the data frame and plots all pairwise combinations of the features with the species of the iris flower in colors. Execute this code and examine the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Sepal.Length</th><th scope=col>Sepal.Width</th><th scope=col>Petal.Length</th><th scope=col>Petal.Width</th><th scope=col>Species</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>5.1   </td><td>3.5   </td><td>1.4   </td><td>0.2   </td><td>setosa</td></tr>\n",
       "\t<tr><td>4.9   </td><td>3.0   </td><td>1.4   </td><td>0.2   </td><td>setosa</td></tr>\n",
       "\t<tr><td>4.7   </td><td>3.2   </td><td>1.3   </td><td>0.2   </td><td>setosa</td></tr>\n",
       "\t<tr><td>4.6   </td><td>3.1   </td><td>1.5   </td><td>0.2   </td><td>setosa</td></tr>\n",
       "\t<tr><td>5.0   </td><td>3.6   </td><td>1.4   </td><td>0.2   </td><td>setosa</td></tr>\n",
       "\t<tr><td>5.4   </td><td>3.9   </td><td>1.7   </td><td>0.4   </td><td>setosa</td></tr>\n",
       "\t<tr><td>4.6   </td><td>3.4   </td><td>1.4   </td><td>0.3   </td><td>setosa</td></tr>\n",
       "\t<tr><td>5.0   </td><td>3.4   </td><td>1.5   </td><td>0.2   </td><td>setosa</td></tr>\n",
       "\t<tr><td>4.4   </td><td>2.9   </td><td>1.4   </td><td>0.2   </td><td>setosa</td></tr>\n",
       "\t<tr><td>4.9   </td><td>3.1   </td><td>1.5   </td><td>0.1   </td><td>setosa</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllll}\n",
       " Sepal.Length & Sepal.Width & Petal.Length & Petal.Width & Species\\\\\n",
       "\\hline\n",
       "\t 5.1    & 3.5    & 1.4    & 0.2    & setosa\\\\\n",
       "\t 4.9    & 3.0    & 1.4    & 0.2    & setosa\\\\\n",
       "\t 4.7    & 3.2    & 1.3    & 0.2    & setosa\\\\\n",
       "\t 4.6    & 3.1    & 1.5    & 0.2    & setosa\\\\\n",
       "\t 5.0    & 3.6    & 1.4    & 0.2    & setosa\\\\\n",
       "\t 5.4    & 3.9    & 1.7    & 0.4    & setosa\\\\\n",
       "\t 4.6    & 3.4    & 1.4    & 0.3    & setosa\\\\\n",
       "\t 5.0    & 3.4    & 1.5    & 0.2    & setosa\\\\\n",
       "\t 4.4    & 2.9    & 1.4    & 0.2    & setosa\\\\\n",
       "\t 4.9    & 3.1    & 1.5    & 0.1    & setosa\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "Sepal.Length | Sepal.Width | Petal.Length | Petal.Width | Species | \n",
       "|---|---|---|---|---|---|---|---|---|---|\n",
       "| 5.1    | 3.5    | 1.4    | 0.2    | setosa | \n",
       "| 4.9    | 3.0    | 1.4    | 0.2    | setosa | \n",
       "| 4.7    | 3.2    | 1.3    | 0.2    | setosa | \n",
       "| 4.6    | 3.1    | 1.5    | 0.2    | setosa | \n",
       "| 5.0    | 3.6    | 1.4    | 0.2    | setosa | \n",
       "| 5.4    | 3.9    | 1.7    | 0.4    | setosa | \n",
       "| 4.6    | 3.4    | 1.4    | 0.3    | setosa | \n",
       "| 5.0    | 3.4    | 1.5    | 0.2    | setosa | \n",
       "| 4.4    | 2.9    | 1.4    | 0.2    | setosa | \n",
       "| 4.9    | 3.1    | 1.5    | 0.1    | setosa | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n",
       "1  5.1          3.5         1.4          0.2         setosa \n",
       "2  4.9          3.0         1.4          0.2         setosa \n",
       "3  4.7          3.2         1.3          0.2         setosa \n",
       "4  4.6          3.1         1.5          0.2         setosa \n",
       "5  5.0          3.6         1.4          0.2         setosa \n",
       "6  5.4          3.9         1.7          0.4         setosa \n",
       "7  4.6          3.4         1.4          0.3         setosa \n",
       "8  5.0          3.4         1.5          0.2         setosa \n",
       "9  4.4          2.9         1.4          0.2         setosa \n",
       "10 4.9          3.1         1.5          0.1         setosa "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA8AAAAJYCAMAAACaSn8zAAABKVBMVEUAAAABuzkEuzsEuzwH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\nZQYb3AAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "single_plot = function(df, colx, coly){\n",
    "    ggplot(df, aes_string(colx,coly)) +\n",
    "          geom_point(aes(color = factor(df$Species)), alpha = 0.4)\n",
    "}\n",
    "\n",
    "plot_iris = function(df){\n",
    "    options(repr.plot.width=8, repr.plot.height=5)\n",
    "    grid.arrange(\n",
    "        single_plot(df, 'Sepal.Length', 'Sepal.Width'),\n",
    "        single_plot(df, 'Sepal.Length', 'Petal.Length'),\n",
    "        single_plot(df, 'Petal.Length', 'Petal.Width'),\n",
    "        single_plot(df, 'Sepal.Width', 'Petal.Length'),\n",
    "        nrow = 2)\n",
    "}\n",
    "\n",
    "head(iris, 10)   \n",
    "plot_iris(iris) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that Setosa (in blue) is well separated from the other two categories. The Versicolor (in orange) and the Virginica (in green) show considerable overlap. The question is how well our classifier will separate these categories. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, execute the code in the cell below to split the dataset into test and training set. Notice that unusually, 67% of the cases are being used as the test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>51</li>\n",
       "\t<li>5</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 51\n",
       "\\item 5\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 51\n",
       "2. 5\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 51  5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>99</li>\n",
       "\t<li>5</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 99\n",
       "\\item 5\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 99\n",
       "2. 5\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 99  5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set.seed(1955)\n",
    "## Randomly sample cases to create independent training and test data\n",
    "partition = createDataPartition(iris[,'Species'], times = 1, p = 0.33, list = FALSE)\n",
    "training = iris[partition,] # Create the training sample\n",
    "dim(training)\n",
    "test = iris[-partition,] # Create the test sample\n",
    "dim(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is always the case with machine learning, numeric features  must be scaled. Execute the code in the cell below to scale the training and test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>Sepal.Length</th><th scope=col>Sepal.Width</th><th scope=col>Petal.Length</th><th scope=col>Petal.Width</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>3</th><td>-1.2646501</td><td> 0.4563512</td><td>-1.348274 </td><td>-1.287390 </td></tr>\n",
       "\t<tr><th scope=row>5</th><td>-0.9016267</td><td> 1.4161001</td><td>-1.293220 </td><td>-1.287390 </td></tr>\n",
       "\t<tr><th scope=row>6</th><td>-0.4175955</td><td> 2.1359118</td><td>-1.128059 </td><td>-1.027876 </td></tr>\n",
       "\t<tr><th scope=row>9</th><td>-1.6276734</td><td>-0.2634605</td><td>-1.293220 </td><td>-1.287390 </td></tr>\n",
       "\t<tr><th scope=row>10</th><td>-1.0226345</td><td> 0.2164140</td><td>-1.238167 </td><td>-1.417146 </td></tr>\n",
       "\t<tr><th scope=row>11</th><td>-0.4175955</td><td> 1.6560373</td><td>-1.238167 </td><td>-1.287390 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       "  & Sepal.Length & Sepal.Width & Petal.Length & Petal.Width\\\\\n",
       "\\hline\n",
       "\t3 & -1.2646501 &  0.4563512 & -1.348274  & -1.287390 \\\\\n",
       "\t5 & -0.9016267 &  1.4161001 & -1.293220  & -1.287390 \\\\\n",
       "\t6 & -0.4175955 &  2.1359118 & -1.128059  & -1.027876 \\\\\n",
       "\t9 & -1.6276734 & -0.2634605 & -1.293220  & -1.287390 \\\\\n",
       "\t10 & -1.0226345 &  0.2164140 & -1.238167  & -1.417146 \\\\\n",
       "\t11 & -0.4175955 &  1.6560373 & -1.238167  & -1.287390 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | Sepal.Length | Sepal.Width | Petal.Length | Petal.Width | \n",
       "|---|---|---|---|---|---|\n",
       "| 3 | -1.2646501 |  0.4563512 | -1.348274  | -1.287390  | \n",
       "| 5 | -0.9016267 |  1.4161001 | -1.293220  | -1.287390  | \n",
       "| 6 | -0.4175955 |  2.1359118 | -1.128059  | -1.027876  | \n",
       "| 9 | -1.6276734 | -0.2634605 | -1.293220  | -1.287390  | \n",
       "| 10 | -1.0226345 |  0.2164140 | -1.238167  | -1.417146  | \n",
       "| 11 | -0.4175955 |  1.6560373 | -1.238167  | -1.287390  | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "   Sepal.Length Sepal.Width Petal.Length Petal.Width\n",
       "3  -1.2646501    0.4563512  -1.348274    -1.287390  \n",
       "5  -0.9016267    1.4161001  -1.293220    -1.287390  \n",
       "6  -0.4175955    2.1359118  -1.128059    -1.027876  \n",
       "9  -1.6276734   -0.2634605  -1.293220    -1.287390  \n",
       "10 -1.0226345    0.2164140  -1.238167    -1.417146  \n",
       "11 -0.4175955    1.6560373  -1.238167    -1.287390  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_cols = c('Sepal.Length', 'Sepal.Width', 'Petal.Length', 'Petal.Width')\n",
    "preProcValues <- preProcess(training[,num_cols], method = c(\"center\", \"scale\"))\n",
    "\n",
    "training[,num_cols] = predict(preProcValues, training[,num_cols])\n",
    "test[,num_cols] = predict(preProcValues, test[,num_cols])\n",
    "head(training[,num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will define and fit an AdaBoosted tree model. The code in the cell below defines the model with 100 estimators (trees) using the `gbm` function from the R gbm  package, and then fits the model. Since this is a multi-class classification problem a multinomial distribution is specified for the model response. Execute this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(3355)\n",
    "gbm_mod = gbm(Species ~ ., data = training, distribution = \"multinomial\", n.trees = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the code in the cell below uses the `predict` method is used to compute the multinomial class probabilities from the scaled features. Execute this code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>Sepal.Length</th><th scope=col>Sepal.Width</th><th scope=col>Petal.Length</th><th scope=col>Petal.Width</th><th scope=col>Species</th><th scope=col>probs</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>-0.78061890 </td><td> 1.17616289 </td><td>-1.293220   </td><td>-1.287390   </td><td>setosa      </td><td> 0.011800578</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>-1.02263448 </td><td>-0.02352326 </td><td>-1.293220   </td><td>-1.287390   </td><td>setosa      </td><td> 0.011800578</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>-1.38565786 </td><td> 0.21641397 </td><td>-1.238167   </td><td>-1.287390   </td><td>setosa      </td><td> 0.011800578</td></tr>\n",
       "\t<tr><th scope=row>7</th><td>-1.38565786 </td><td> 0.93622566 </td><td>-1.293220   </td><td>-1.157633   </td><td>setosa      </td><td> 0.011800578</td></tr>\n",
       "\t<tr><th scope=row>8</th><td>-0.90162669 </td><td> 0.93622566 </td><td>-1.238167   </td><td>-1.287390   </td><td>setosa      </td><td> 0.011800578</td></tr>\n",
       "\t<tr><th scope=row>12</th><td>-1.14364227 </td><td> 0.93622566 </td><td>-1.183113   </td><td>-1.287390   </td><td>setosa      </td><td> 0.011800578</td></tr>\n",
       "\t<tr><th scope=row>14</th><td>-1.74868124 </td><td>-0.02352326 </td><td>-1.458381   </td><td>-1.417146   </td><td>setosa      </td><td> 0.011800578</td></tr>\n",
       "\t<tr><th scope=row>15</th><td> 0.06643565 </td><td> 2.37584904 </td><td>-1.403328   </td><td>-1.287390   </td><td>setosa      </td><td>-0.003465665</td></tr>\n",
       "\t<tr><th scope=row>16</th><td>-0.05457214 </td><td> 3.33559795 </td><td>-1.238167   </td><td>-1.027876   </td><td>setosa      </td><td>-0.003465665</td></tr>\n",
       "\t<tr><th scope=row>19</th><td>-0.05457214 </td><td> 1.89597458 </td><td>-1.128059   </td><td>-1.157633   </td><td>setosa      </td><td>-0.003465665</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllll}\n",
       "  & Sepal.Length & Sepal.Width & Petal.Length & Petal.Width & Species & probs\\\\\n",
       "\\hline\n",
       "\t1 & -0.78061890  &  1.17616289  & -1.293220    & -1.287390    & setosa       &  0.011800578\\\\\n",
       "\t2 & -1.02263448  & -0.02352326  & -1.293220    & -1.287390    & setosa       &  0.011800578\\\\\n",
       "\t4 & -1.38565786  &  0.21641397  & -1.238167    & -1.287390    & setosa       &  0.011800578\\\\\n",
       "\t7 & -1.38565786  &  0.93622566  & -1.293220    & -1.157633    & setosa       &  0.011800578\\\\\n",
       "\t8 & -0.90162669  &  0.93622566  & -1.238167    & -1.287390    & setosa       &  0.011800578\\\\\n",
       "\t12 & -1.14364227  &  0.93622566  & -1.183113    & -1.287390    & setosa       &  0.011800578\\\\\n",
       "\t14 & -1.74868124  & -0.02352326  & -1.458381    & -1.417146    & setosa       &  0.011800578\\\\\n",
       "\t15 &  0.06643565  &  2.37584904  & -1.403328    & -1.287390    & setosa       & -0.003465665\\\\\n",
       "\t16 & -0.05457214  &  3.33559795  & -1.238167    & -1.027876    & setosa       & -0.003465665\\\\\n",
       "\t19 & -0.05457214  &  1.89597458  & -1.128059    & -1.157633    & setosa       & -0.003465665\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | Sepal.Length | Sepal.Width | Petal.Length | Petal.Width | Species | probs | \n",
       "|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1 | -0.78061890  |  1.17616289  | -1.293220    | -1.287390    | setosa       |  0.011800578 | \n",
       "| 2 | -1.02263448  | -0.02352326  | -1.293220    | -1.287390    | setosa       |  0.011800578 | \n",
       "| 4 | -1.38565786  |  0.21641397  | -1.238167    | -1.287390    | setosa       |  0.011800578 | \n",
       "| 7 | -1.38565786  |  0.93622566  | -1.293220    | -1.157633    | setosa       |  0.011800578 | \n",
       "| 8 | -0.90162669  |  0.93622566  | -1.238167    | -1.287390    | setosa       |  0.011800578 | \n",
       "| 12 | -1.14364227  |  0.93622566  | -1.183113    | -1.287390    | setosa       |  0.011800578 | \n",
       "| 14 | -1.74868124  | -0.02352326  | -1.458381    | -1.417146    | setosa       |  0.011800578 | \n",
       "| 15 |  0.06643565  |  2.37584904  | -1.403328    | -1.287390    | setosa       | -0.003465665 | \n",
       "| 16 | -0.05457214  |  3.33559795  | -1.238167    | -1.027876    | setosa       | -0.003465665 | \n",
       "| 19 | -0.05457214  |  1.89597458  | -1.128059    | -1.157633    | setosa       | -0.003465665 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "   Sepal.Length Sepal.Width Petal.Length Petal.Width Species probs       \n",
       "1  -0.78061890   1.17616289 -1.293220    -1.287390   setosa   0.011800578\n",
       "2  -1.02263448  -0.02352326 -1.293220    -1.287390   setosa   0.011800578\n",
       "4  -1.38565786   0.21641397 -1.238167    -1.287390   setosa   0.011800578\n",
       "7  -1.38565786   0.93622566 -1.293220    -1.157633   setosa   0.011800578\n",
       "8  -0.90162669   0.93622566 -1.238167    -1.287390   setosa   0.011800578\n",
       "12 -1.14364227   0.93622566 -1.183113    -1.287390   setosa   0.011800578\n",
       "14 -1.74868124  -0.02352326 -1.458381    -1.417146   setosa   0.011800578\n",
       "15  0.06643565   2.37584904 -1.403328    -1.287390   setosa  -0.003465665\n",
       "16 -0.05457214   3.33559795 -1.238167    -1.027876   setosa  -0.003465665\n",
       "19 -0.05457214   1.89597458 -1.128059    -1.157633   setosa  -0.003465665"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test$probs = predict(gbm_mod, newdata = test, n.trees = 5)\n",
    "test[1:10,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the species and multinomial probabilities for the classes, along with the species of the the flower. \n",
    "\n",
    "The code in the cell below uses the R `which.max` function to compute classes from the maximum of the probabilities. Execute this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>Sepal.Length</th><th scope=col>Sepal.Width</th><th scope=col>Petal.Length</th><th scope=col>Petal.Width</th><th scope=col>Species</th><th scope=col>probs</th><th scope=col>scores</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>-0.78061890 </td><td> 1.17616289 </td><td>-1.293220   </td><td>-1.287390   </td><td>setosa      </td><td> 0.011800578</td><td>sentosa     </td></tr>\n",
       "\t<tr><th scope=row>2</th><td>-1.02263448 </td><td>-0.02352326 </td><td>-1.293220   </td><td>-1.287390   </td><td>setosa      </td><td> 0.011800578</td><td>sentosa     </td></tr>\n",
       "\t<tr><th scope=row>4</th><td>-1.38565786 </td><td> 0.21641397 </td><td>-1.238167   </td><td>-1.287390   </td><td>setosa      </td><td> 0.011800578</td><td>sentosa     </td></tr>\n",
       "\t<tr><th scope=row>7</th><td>-1.38565786 </td><td> 0.93622566 </td><td>-1.293220   </td><td>-1.157633   </td><td>setosa      </td><td> 0.011800578</td><td>sentosa     </td></tr>\n",
       "\t<tr><th scope=row>8</th><td>-0.90162669 </td><td> 0.93622566 </td><td>-1.238167   </td><td>-1.287390   </td><td>setosa      </td><td> 0.011800578</td><td>sentosa     </td></tr>\n",
       "\t<tr><th scope=row>12</th><td>-1.14364227 </td><td> 0.93622566 </td><td>-1.183113   </td><td>-1.287390   </td><td>setosa      </td><td> 0.011800578</td><td>sentosa     </td></tr>\n",
       "\t<tr><th scope=row>14</th><td>-1.74868124 </td><td>-0.02352326 </td><td>-1.458381   </td><td>-1.417146   </td><td>setosa      </td><td> 0.011800578</td><td>sentosa     </td></tr>\n",
       "\t<tr><th scope=row>15</th><td> 0.06643565 </td><td> 2.37584904 </td><td>-1.403328   </td><td>-1.287390   </td><td>setosa      </td><td>-0.003465665</td><td>versicolor  </td></tr>\n",
       "\t<tr><th scope=row>16</th><td>-0.05457214 </td><td> 3.33559795 </td><td>-1.238167   </td><td>-1.027876   </td><td>setosa      </td><td>-0.003465665</td><td>versicolor  </td></tr>\n",
       "\t<tr><th scope=row>19</th><td>-0.05457214 </td><td> 1.89597458 </td><td>-1.128059   </td><td>-1.157633   </td><td>setosa      </td><td>-0.003465665</td><td>versicolor  </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllll}\n",
       "  & Sepal.Length & Sepal.Width & Petal.Length & Petal.Width & Species & probs & scores\\\\\n",
       "\\hline\n",
       "\t1 & -0.78061890  &  1.17616289  & -1.293220    & -1.287390    & setosa       &  0.011800578 & sentosa     \\\\\n",
       "\t2 & -1.02263448  & -0.02352326  & -1.293220    & -1.287390    & setosa       &  0.011800578 & sentosa     \\\\\n",
       "\t4 & -1.38565786  &  0.21641397  & -1.238167    & -1.287390    & setosa       &  0.011800578 & sentosa     \\\\\n",
       "\t7 & -1.38565786  &  0.93622566  & -1.293220    & -1.157633    & setosa       &  0.011800578 & sentosa     \\\\\n",
       "\t8 & -0.90162669  &  0.93622566  & -1.238167    & -1.287390    & setosa       &  0.011800578 & sentosa     \\\\\n",
       "\t12 & -1.14364227  &  0.93622566  & -1.183113    & -1.287390    & setosa       &  0.011800578 & sentosa     \\\\\n",
       "\t14 & -1.74868124  & -0.02352326  & -1.458381    & -1.417146    & setosa       &  0.011800578 & sentosa     \\\\\n",
       "\t15 &  0.06643565  &  2.37584904  & -1.403328    & -1.287390    & setosa       & -0.003465665 & versicolor  \\\\\n",
       "\t16 & -0.05457214  &  3.33559795  & -1.238167    & -1.027876    & setosa       & -0.003465665 & versicolor  \\\\\n",
       "\t19 & -0.05457214  &  1.89597458  & -1.128059    & -1.157633    & setosa       & -0.003465665 & versicolor  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | Sepal.Length | Sepal.Width | Petal.Length | Petal.Width | Species | probs | scores | \n",
       "|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1 | -0.78061890  |  1.17616289  | -1.293220    | -1.287390    | setosa       |  0.011800578 | sentosa      | \n",
       "| 2 | -1.02263448  | -0.02352326  | -1.293220    | -1.287390    | setosa       |  0.011800578 | sentosa      | \n",
       "| 4 | -1.38565786  |  0.21641397  | -1.238167    | -1.287390    | setosa       |  0.011800578 | sentosa      | \n",
       "| 7 | -1.38565786  |  0.93622566  | -1.293220    | -1.157633    | setosa       |  0.011800578 | sentosa      | \n",
       "| 8 | -0.90162669  |  0.93622566  | -1.238167    | -1.287390    | setosa       |  0.011800578 | sentosa      | \n",
       "| 12 | -1.14364227  |  0.93622566  | -1.183113    | -1.287390    | setosa       |  0.011800578 | sentosa      | \n",
       "| 14 | -1.74868124  | -0.02352326  | -1.458381    | -1.417146    | setosa       |  0.011800578 | sentosa      | \n",
       "| 15 |  0.06643565  |  2.37584904  | -1.403328    | -1.287390    | setosa       | -0.003465665 | versicolor   | \n",
       "| 16 | -0.05457214  |  3.33559795  | -1.238167    | -1.027876    | setosa       | -0.003465665 | versicolor   | \n",
       "| 19 | -0.05457214  |  1.89597458  | -1.128059    | -1.157633    | setosa       | -0.003465665 | versicolor   | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "   Sepal.Length Sepal.Width Petal.Length Petal.Width Species probs       \n",
       "1  -0.78061890   1.17616289 -1.293220    -1.287390   setosa   0.011800578\n",
       "2  -1.02263448  -0.02352326 -1.293220    -1.287390   setosa   0.011800578\n",
       "4  -1.38565786   0.21641397 -1.238167    -1.287390   setosa   0.011800578\n",
       "7  -1.38565786   0.93622566 -1.293220    -1.157633   setosa   0.011800578\n",
       "8  -0.90162669   0.93622566 -1.238167    -1.287390   setosa   0.011800578\n",
       "12 -1.14364227   0.93622566 -1.183113    -1.287390   setosa   0.011800578\n",
       "14 -1.74868124  -0.02352326 -1.458381    -1.417146   setosa   0.011800578\n",
       "15  0.06643565   2.37584904 -1.403328    -1.287390   setosa  -0.003465665\n",
       "16 -0.05457214   3.33559795 -1.238167    -1.027876   setosa  -0.003465665\n",
       "19 -0.05457214   1.89597458 -1.128059    -1.157633   setosa  -0.003465665\n",
       "   scores    \n",
       "1  sentosa   \n",
       "2  sentosa   \n",
       "4  sentosa   \n",
       "7  sentosa   \n",
       "8  sentosa   \n",
       "12 sentosa   \n",
       "14 sentosa   \n",
       "15 versicolor\n",
       "16 versicolor\n",
       "19 versicolor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test$scores = apply(test$probs, 1, which.max) # Find the class with the maximum probability\n",
    "test$scores = ifelse(test$scores == 1, 'sentosa', ifelse(test$scores == 2, 'versicolor', 'virginica'))\n",
    "test[1:10,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to evaluate the model results. Keep in mind that the problem has been made deliberately difficult, by having more test cases than training cases. The iris data has three species categories. Therefore it is necessary to use evaluation code for a three category problem. The function in the cell below extends code from previous labs to deal with a three category problem. Execute this code and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Predicted\n",
      "Actual       sentosa versicolor virginica\n",
      "  setosa          30          3         0\n",
      "  versicolor       4         20         9\n",
      "  virginica        1          0        32\n",
      "\n",
      "Accuracy =  0.828 \n",
      " \n",
      "          setosa versicolor virginica\n",
      "Precision  0.909      0.606     0.970\n",
      "Recall     0.857      0.870     0.780\n",
      "F1         0.882      0.714     0.865\n"
     ]
    }
   ],
   "source": [
    "print_metrics = function(df, label){\n",
    "    ## Compute and print the confusion matrix\n",
    "    cm = as.matrix(table(Actual = df$Species, Predicted = df$scores))\n",
    "    print(cm)\n",
    "\n",
    "    ## Compute and print accuracy \n",
    "    accuracy = round(sum(sapply(1:nrow(cm), function(i) cm[i,i]))/sum(cm), 3)\n",
    "    cat('\\n')\n",
    "    cat(paste('Accuracy = ', as.character(accuracy)), '\\n \\n')                           \n",
    "\n",
    "    ## Compute and print precision, recall and F1\n",
    "    precision = sapply(1:nrow(cm), function(i) cm[i,i]/sum(cm[i,]))\n",
    "    recall = sapply(1:nrow(cm), function(i) cm[i,i]/sum(cm[,i]))    \n",
    "    F1 = sapply(1:nrow(cm), function(i) 2*(recall[i] * precision[i])/(recall[i] + precision[i]))    \n",
    "    metrics = sapply(c(precision, recall, F1), round, 3)        \n",
    "    metrics = t(matrix(metrics, nrow = nrow(cm), ncol = ncol(cm)))       \n",
    "    dimnames(metrics) = list(c('Precision', 'Recall', 'F1'), unique(test$Species))      \n",
    "    print(metrics)\n",
    "}  \n",
    "print_metrics(test, 'Species')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine these results. Notice the following:\n",
    "1. The confusion matrix has dimension 3X3. You can see that most cases are correctly classified, but with a few errors. \n",
    "2. The overall accuracy is 0.83. Since the classes are roughly balanced, this metric indicates relatively good performance of the classifier, particularly since it was only trained on 50 cases. \n",
    "3. The precision, recall and  F1 for each of the classes are reasonable.\n",
    "\n",
    "The above results were computed using an ensemble with only 5 trees. Will more trees in the ensemble help? To find out, execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Predicted\n",
      "Actual       sentosa versicolor virginica\n",
      "  setosa          33          0         0\n",
      "  versicolor       2         21        10\n",
      "  virginica        0          1        32\n",
      "\n",
      "Accuracy =  0.869 \n",
      " \n",
      "          setosa versicolor virginica\n",
      "Precision  1.000      0.636     0.970\n",
      "Recall     0.943      0.955     0.762\n",
      "F1         0.971      0.764     0.853\n"
     ]
    }
   ],
   "source": [
    "set.seed(8899)\n",
    "gbm_mod = gbm(Species ~ ., data = training, distribution = \"multinomial\", n.trees = 100)\n",
    "test$probs = predict(gbm_mod, newdata = test, n.trees = 100)\n",
    "test$scores = apply(test$probs, 1, which.max) # Find the class with the maximum probability\n",
    "test$scores = ifelse(test$scores == 1, 'sentosa', ifelse(test$scores == 2, 'versicolor', 'virginica'))\n",
    "print_metrics(test, 'Species')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These metrics show a small improvement. Using more trees in the ensemble has resulted in a better model. \n",
    "\n",
    "How important are each of the features for this model? The R Caret package provides the capability to find out. As a first step, `gbm` models must be trained using the the Caret `train` function. The code in the cell below does this, using the default model arguments. The default arguments for the model are specified with the `tuneGrid` argument of `train`. Execute the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gbm_mod_train = train(Species ~ ., data = training, method = \"gbm\", verbose = FALSE,\n",
    "            distribution = \"multinomial\", trControl=trainControl(number=100),\n",
    "            tuneGrid=expand.grid(n.trees = 100,\n",
    "                                 interaction.depth = 1, \n",
    "                                 n.minobsinnode = 10, \n",
    "                                 shrinkage = 0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Caret model object trained, the feature importance can be computed and displayed. Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAFoCAMAAAC46dgSAAAANlBMVEUAAAAzMzNNTU1oaGh8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\nQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "options(repr.plot.width=4, repr.plot.height=3)\n",
    "imp = varImp(gbm_mod_train, scale = FALSE)$importance\n",
    "imp[,'Feature'] = row.names(imp)\n",
    "ggplot(imp, aes(x = Feature, y = Overall)) + geom_point(size = 4) +\n",
    "       ggtitle('Variable importance for iris features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the following:\n",
    "1. Petal length is the most importance feature.\n",
    "2. Sepal width is the least important feature. \n",
    "\n",
    "The question now is, if a model with a reduced feature set will be an improvement. Execute the code in the cell below which creates a model using three of the four features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Predicted\n",
      "Actual       sentosa versicolor virginica\n",
      "  setosa          33          0         0\n",
      "  versicolor       2         24         7\n",
      "  virginica        0          1        32\n",
      "\n",
      "Accuracy =  0.899 \n",
      " \n",
      "          setosa versicolor virginica\n",
      "Precision  1.000      0.727     0.970\n",
      "Recall     0.943      0.960     0.821\n",
      "F1         0.971      0.828     0.889\n"
     ]
    }
   ],
   "source": [
    "set.seed(8899)\n",
    "gbm_mod = gbm(Species ~ Petal.Length + Petal.Width + Sepal.Length, \n",
    "              data = training, distribution = \"multinomial\", n.trees = 100)\n",
    "test$probs = predict(gbm_mod, newdata = test, n.trees = 100)\n",
    "test$scores = apply(test$probs, 1, which.max) # Find the class with the maximum probability\n",
    "test$scores = ifelse(test$scores == 1, 'sentosa', ifelse(test$scores == 2, 'versicolor', 'virginica'))\n",
    "print_metrics(test, 'Species')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with reduced feature set has slightly improved metrics. Overall, this model is preferred since it is more likely to generalize. \n",
    "\n",
    "The code in the cell below plots the classes of the iris flower along with the classification errors shown by shape. Execute this code and examine the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA8AAAAJYCAMAAACaSn8zAAAAqFBMVEUAAAAfwE8zMzMzxF5N\nTU1Vy3hXzntauK5c039evbJoaGhrqetxsd51tuJ7v8h8fHx/rfuDsf+Dx8+MjIyN16ORkZGR\n3KiTuPiVu/qZ46+ampqav/+np6eysrK0y/O40Pe9vb3A1//Hx8fQ0NDZ2dnh4eHp6enr6+vw\nvLnw8PDy8vLzoJv0wL31j4n2hX72op33d273eG/3e3P3f3f5k436p6L8yMX////OANDsAAAA\nCXBIWXMAABJ0AAASdAHeZh94AAAgAElEQVR4nO2dDXvrqNWu1akyM8fdJ+5bezcT13Gz8+HI\n4+6Znvad6P//syNASAgQAgSCJa/n2jtx8DK21uNbEh9CRY1CocCqSP0BUCiUvxBgFAqwEGAU\nCrAQYBQKsBBgFAqwEGAUCrAQYBQKsBBgFAqwvAH+0Mqt2DF8ycpD5hik0lsQtfLU6Q0mBFhf\nHDLHIJXegqiVp05vMCHA+uKQOQap9BZErTx1eoMJAdYXh8wxSKW3IGrlqdMbTAiwvjhkjkEq\nvQVRK0+d3mBCgPXFIXMMUuktiFp56vQGU34AV2hwDkpvQdTKU6c3mLIDuKqqNRr8XIauMbLS\nWxCo8tPppCkOna5k/iLA+uKQOSYqoM2YSW9BmMpPJ0bwWv3NDeCKKINvT8gcEyHAoYvtwk+n\nluC1+pslwJ+RKncontj6864oj/TBoSgOZ5LI4r3ctj/rCym9CIFFAY3g9BYEqdwTYDj+ZgZw\nBQPgS0kc2/EH5YVYuC0O7c+alm6EQAQ4ePF0OAHXD2BA/uYJcBWlcpdi88YfGxffiGXHotkf\nbwu6DyZ7bPbzifw8Fs9CIDR+VwAwI9erDQzI38wADlgcE+BNceEPmtOrM9kZFwU70TrTUpra\nnRCIAIcungrnx16fXmhA/iLA+uKJrBWDB/0umP9kUgIBKb0FMyvvTp610eaNB+QvAqwvnsga\nHIN9ld6CmZUjwGYt4UHCys0brz3FouksWKkSiACHLrZsA49EmzcekL8IsL7YvPHH4li/y50c\nNJ0Ff7p+IU91gQhw6GK7XuixaPPGA/IXAdYXmzf+zMcRxGEGmk76k5UW70JgE+Sb6TRKb0HU\nys0bD8hfBFhfPLH179t2fF8Y6K+7n7R0+yYGPkMDeI369u2bZSQcfxFgfXHIHINUeguCVy42\nilOnN5gQYH1xyByDVHoLQlc+6JZOnd5gQoD1xSFzDFLpLQhdOQKMuiElYSxq5QjwQGE9qPTF\nYSr3KQ6ZY5BKb0HwyrENLCqoBxUCnJvSWxC+cnFi1lqUC8CVrjhM5V7FIXMMUuktiFp56vQG\nUxYAV9AA/nVcvvnMTektiFq5eeMB+ZsNwJVaHKZyv2LzxgMy2FfpLYhauXnjAfmbA8AVApyf\n0lsQtXLzxgPyNx+Aq8lwNHhBpbfAr/LTt5NU4HFBPyB/cwA4TjECPEvpLfCq/NQAfBoWeCyp\nA8hfBFhfbN54QAb7Kr0FPpWfKMCnQYHHonaA/E0DcDVSHrIYAZ6l9Bb4VI4AW2uOB12DFwHO\nVuksOA3PgW0q716CAFtrjmMIcP5KZsFwJRyryoWXYBvYVjMc6/ucEeBslcoCaS06m8oHL8Fe\naEvNcKwfNUKAs1UqC8wAK8vUDe6+4PCe5o0H5G8CgKt1Afz169cZBmd7OWcqC4wAKwvFngQ5\nvad54wH5mw7gyjrZfsULAfz169DhpfIfW8ksMLSBFVI1/N6avzgOrC82b7zkr+hwm9aiXwKc\nZrn5VbQLhIvP5HvTs3QWjPdCjwGsnllPvqd54wH5iwDri80bP2Vw0f7oflOXC90zvvmPrfQW\nfLQsZwhwRv7mCrDu/oTAAK5VO1VjEWBDMaNzsg2sFk9Xbt54QP5mCrB0gXDYym2KzRs/1UYy\nGMxvqjM3/7EVIqczAW6Pr2l7oXP3FwHWF5s3/tehw786GCzlHQEeL9YArA1fvhc6I3/zBFi+\nQDho5VbF5o3/dVyDrGZgsK9C5DQowMLBV5pbtfw4cEb+IsD6YvPGWxksd2WobSTsxDIXi21g\ncbLVSde55Va5eeMB+ZslwJ/KGh0BK1/CYHWYoUu1/AwOIxmKe1CFoyx/WEtxTpWbNx6Qv1kC\nnEHl5o2fNBi+0lswKDYD7F65eeMB+YsA64vNGw/IYBeJN9hLZwHjVGrUnk4PDw8IsCoEWF9s\n3nhABjuozAJgZWiXFT8QfQgR6C8VAqwvNm88IIPtVWZxBGaH1wehY1k+Aiu90D7vaU4FIH8R\nYH2xeeMBGeyirAHmmlO58KdRgPxFgPXF5o0HZLCLOMA/NEr1Gb5R/YP90hYv8CEA+YsA64vN\nGw/IYBflcAQeawPLx2X0lwkB1heHzDEYZQGwvhdacxKN/hI5AFyWYjflnOx5hucDMKA9tIvy\nAHi8mAOMnVi97AEuux9UIaxBgPNS7gAPT6/RXyIEWF9szgUgg12UPcCDqwfRXyLHNjACTAXI\nYBflDzARAizKC+AZwwz/Oxnx6VnzkgJksK9CoDeHsfHOKgRYlBvAszuxvn//PhFOrkJK9e0R\nL0ozCpDBvkpkAZcyjKQ+hwATLQ/wd115L3IdYaJvz2CMwihABvsqjQVc4oCRGo290L2cAJ7d\nRvo+CXCVDuDBKKM5EYAM9lUSCzqZAZ5ZOf3TKED+ugAs8jsD4O9qeS8K8OccazzDEeChkljQ\nSZy1UXdFgSqnfxoFyF+XiRyDv3yy9/27RLASzpbi+NQtKpstwF9VgzNdZcNFIXI6tw0sdlYN\nL2SYV/nHivx1GAcuB1Ox5mQvjAfBK/drA3/VGOyljL4VdXp/e4Jr4c9Ala/IX5wLLRR79ULP\nu/mVoFsCWJ7vLEZ3pe01wCrAi3ZiZe4vAqwvNm/80F/tusFFPVzdLIPb6BAdy4KvPW5WVAsG\nUyKl6L70RFfhUAFedBgpd39TAhxowDc9wLqV+/lqhco6o2ytwlrzjK8VDjoWRQYAf6MIDi8O\n5NECqSLAHxK/i03kyN1fudYF99ChxouSAizfOkfYQ9cD2yaWENZYEUNl8WQZGdMCW4CFU+jm\nr3txWeilAM7eX6nWBffQwQZ8MwBYc/MrC4MT3CbJ/iwupgUDgIWOQ/JDXX6yrWS/b8MSAJyx\nv1KtC+6hgw34pj2FlsWzarmH1vyOqWNxsYyMaoHQBhaH7uhPZfnJDuABweivttbl9tDhZmwg\nwA7abc92gXEt6HqhRYLlI/Dg/sD7HuD0Uykz8lc5hV5oDx1wxgYAgDW/E3RiFaKmgheyQANw\n09hVO6f3e5HgvftnWau/cq2Z7KGTV27eeAeDc7qNTmKApXNlXsZPpznAX758mQCYPUJ/+7eq\nuzfMbQ+dqnLzxk8ZvAKFt0A61A5Khd6qhwbgB7mS/V4geJ8c4IwUA2DlvHistxkBzlfBLRBO\nlrvi/Ye4Ss43GnbfAHyvzu8QXtGijP4SeR/Vx9Ok3Bh0tLMKAV5afMdclua4ZQCWRoZagInG\nACbk7hFgQQiwvti88YAMFlQmbSJpAd6Lz5Hh4Y97ppFK9r3QX6qRYaQZe2jl3tzjvc0I8KJ6\nFvh9ngoOb4HSBha6pTqA1RnSYiWDrmj0l0gEOMweupIJpgBHHC9CgO2VdCaW1As96FjuAD71\ncyalSvbSS9BfItHQIHvoSga4QoAhKr4F8sgQB7g/xg4q6Vq/7dP7RynO6SOaNx6Qv8FnYlUy\nwS3AWoIR4IXV76HLg3nAP7oFysjQfQfwXo0eNn/pn48y6S4f0ZwmQP5G6MSyLwYLMFgNRgqN\nBC9qAePyb5xfHcBS8Z4CrCP4xvzFiRz64ogpT6nXgsy0O2+Ll/pYHEyRi1rAAH6kjeS9TLBw\nvB68AgEmWhDgK/nBzqQrObxSw32tCVNLxJSn1Kad615spppLi1rQAtw/HESLg7/CKxBgIu1c\n6PN2N/lCZ8euBGDWFu5axDy8QoCXEYeWrfGS9rOIeiTqH7WP++d+VorlqFuVZOKO76EnCXam\n40oIHgW4ksO90QtTS5Rkp9eWn0Jv6zdyFB7XwhaMz8ygR1rNsTZmLzQg6XuhL+FPoa8EYNY/\n3fdSt+Hy1A8EOJbO7Vh/ea4nhgpdkneKYkGrMYDRXyoJ1G4PHfoIfKWqBAnh8tQPBDiaLk+b\notgcm/Os4mgMdEheO4tZG66gpxxq7+7ueLQYLI73IsDjkgAW9tATcswe5fc7JfUqA6zM3UKA\n08sheWQO1Ui4wh4tEGu5u+sIfhxO7OCPH/X8or9U8qny5Uj20E/T63K4ZY8dgL9Tgq89wSLA\n1XQtTsVo8CzZJ+80DvCjpvOYdzcz3d11BIuHWnGSR63nF/2lwokc+uKQOc5JEZYNZrOYteFh\nANZ/FPSXCgHWF4fMcUaKsGxwdyGgRhzgvkFrAXA/7DsAWDkOo79Ew4kcdZCJHGJz9toXX/tS\nYSKHbo40AhxL5fRVKq3CWNAhOSDY2AZWXlL3L5zxWdbqbwSAxebstQf42gMsjANrr3IYVN77\nhgDP1uKXE8qH08leaB4s2T6ow+ezrNXfCLNxPj8/u8d//PGH5qEQIQZrhTNuQmqXYmF3BT5T\nLQiwm0SAN4eXd+sXjqZJ7FGmszdYMX/4IUbUyvIdcrItejPwCGyvc5lg2WAfgOVTaAR4RMrF\nDLunV6ud9GiahDEhNnhEi7uHH2JErQwBy8lGgIMqzdVmMnvGWhSCsQ1skmjk5fVp217s/Tx5\nKB5LUzUJsBDxiQAvq0SXizodO7EX2kWKke/Ph3KOwcJMyeuVY1v3Dz/EiE9lDoeSbKU70tYx\nNHiWQuRUV076rKZrsdhvo79UWlDfDxld0I+90EkUIqeacjpqZFHL9H4b/aUKfgR2KQ4z4IsG\nu+h513i7ne6sjGMBm7bxs0X05H4b/aUSQT2/HLfWHVnu2au6HzM9sGsLocEaXTZ051wUb1OR\nIXI6B2CPyl2Kl0j2IpJ7oa2HkpyzN7yUf4YH9/p1g5XZfGiwqkNxJJM5XortVOSc5I2X3939\n+c9/RoADagiwxVVIXM7Zo+wGAPgk3TqHF8sL+qPBGvGldJI1kf5MhE2kcFrsCCytxeHtAb35\nlXr/WPFG0f6VC3+uU6kB3u8bfsWLGUJW7lK8RLIXkdQGpk2kYvv0GvqCfnktDm8PRu5ehwBb\nqT2FnlhSlmhO8sbL2egQAhxOml7oXYxeaHktDm8P7hHgGbrEWnHFXD68GKkBWJiVgcOEs6Qf\nB96FBpiRe5UJ9vBgTwFWJ+ZhG9hO3ZpYE5qTPLl8eDHSfl/LBXMq9yteINPLCOA48Aio2Asd\nVHOSJ5UPLkQgD4SFOnCq7Exp50Lb9GSFSOo8D6JWHjHlMDQnedIxVbmS6HF/d3eHAAeR5mqk\nt3lXI3Epvc1TV+5riudcfoIGGxT1DEt7IdEAYDqdAwEOofDXA7dShosml95Qi03XjzUN4fa0\nWSxm59HsJlnCyv3i6fWNGaxXTIAFJtU2MJUAMLaBZyraonZKb/PElfu6YsNVaKwvmnZcCcXt\nLd6JxPvHDjq4bsxgvZYFWDZSOIXGXuiZGjFytsHKcNHU0huaYvkSbiGa8vvlCyWzL2ZDSQ/k\nx/19f/e64RDTjRmsV2qA1cvzrZ3xDF+rvzEBFoeLJq/cV4sVlxHgYNL5Wzbq/7LKkr5Ydy3+\n0EgtvwiwjyKdQivjvQhwVtIAXHY/qKyyNFI81QutuWWSfeWe4Wv1NyrAHbC1OotyMtl7je+9\nsA3sLvMN3AMCLJUbjZxbuWfxclmPLLB3ZsBeaGelA1jos3KvBQE2SRkHTrLoWYxiNNhNAsA/\nNApZ9c9EIStEdYoBsHiizJbhWARg8R5asyuPmPJMhUdgkIpwCi02ddtlOD5jeCAV//TTPS/5\n8uVhbuUhczymCHfFsH5rbAMvkOSs32Q8TUOAq4UAvv+pI/jhCyU4osHnbVFsNOXPpaaw0Su/\n/JaAw178tmtOcw5k5tuFXMC5nb7pmP6s6PA6+UK7uhDgXoD8lV90nH0KLfY282U4PnX3IJzr\nwaCY8PvTPSWY8Pvl4SGmwWPXa42k7a0UA+iLX9s0v9Xv/BLdqSnoI5WXk+vT2SkewG53ZnCt\n3K94Ihdw/JVeFOD+scJ4UbcGx6duIvRcDwbFP1FRgL9QRQV4LD1jHjyLATSoLF5qsrjcpt4U\nhwvdbR+nUq7X2FHBVbMBNozsGubEehejv+wdpY9SvG+L82Xrv+xoJQN8XR/A7R6OnCWV1Jbz\njj5od3znQ3PydKZx7+W22StSCxoLd90dXJnTbWX016X1flew249dSCUXoXIW2Bc/lcWGfnFK\nz2+GrJkzsSynZuzl24u6OuMZvlZ/JYCbSp6K1+bdvJcdFaZs9MtwfOomQs/1QCze7ynAe/qQ\nAjzza2JhcHuWdOQL1exag9lf5IypKLbFoUkmcYCW7jqDGx/fWO53xatQ8YGfbNFKNnVfOd+z\nt8XsXOmZPrJcSzTuMKHd5Eh6Ng0DYBj+qgC/klcBHAe+v+96oR8eIvdC0/RsmrOk+p08JGvE\nvXVnT0ey96NnTNR9ukdkpc2ZTbciJOnjeCLnOedmT3t8Obf1bi/spU/kx5FYMai8Ly6Kc1NM\n9v2vxdOUWfxTRwTYbnYz687KflE7QP5KRu6KlzPpQos62T2QB1ErtzC48YYuYUKsvgyMb9wi\nWaQmkJye+9J+SVdyAyp6onMhy1QVmzf6+vf2pRsaRHbMg8r74rLg/ZNn8mdwOSbP8voiSADD\n8FcClZBLl9VRlx0d7eSYXuiZTeTwWJHDqVj++kQ3eMuPZd3ujj5gf/Xl8s8++qloezbej4ct\n2d+LQdrK++LX5mRrc+7fNbQck7dGgEH4Kz/3uiGLB+t6zMYAnl4nlo0D668HVlwXOzmEKczS\nVQut6LmyeAqtTHoWJkcrH3HwnLvBh2Lz/HqeYzDbY/O/SieDm2/FpmBDDC4Azx8m1BarV56M\nhINoA0Py1957A8AT68TScWD9Bf2K66LBwkVE0nWDrR6I2vfsrk4aXHYkXJ6kfMThc+4Gs15D\n8ylWn3/pFKtsXsIeF+2L2VM0aMvPpdgLlVMsrmfxDawUYJjQoRhqLzQkf70AFie7fxIZX8gi\ntHGPTPqCb0zSQ0H/YGLBf2v0f/8hhbGX/UP7atNzU2oNfmu7LY7NGcu7tpODBLM20hPrv+BB\nh2J3KS6HpqnShDf72cuRdUTSnpAnViW7B9mg8r64JHMEaCeHSxt4/jDhcoxFrXw1/soAX8jd\nVba6bq+RI7B04a8mdXQc+Kq9oF9ZW0VsIwnX4bNH36RXPzDR92RX+D9IV+4LV/grF/RLz7kb\nfOyOZWfe+V+QjIvDDHXNeym7YYZ2z87m55A7JLDb2dCHZFyC1sGffx9ULhazdydGWfdCsw8+\nc5jQpRg2wDD8lQA+j996Qw+wdKW+Jnt0Jtb1qiU4GMB7cYmO/rNEBZj0FWxZf31zWKND+89s\nl9kP9JMwNk5Ixut3QhvpfCzbqPp5SyYMXOgLztu28Exrr4XK6ev64ub1JXXWehyYfSKow4Rh\na1mNv5KRWzpPhLyLEhllrmy4NvCpk/imkdrAbjqO0KIWu3RH9S9ymIllGiYcKoy/EYtjAuym\npP5KNbbvcNG8U5zJ7uF6oWnwQr3Qjurnyg6TrZZ4GOw0F9owTCjJycjbBjipv1KNO95lpraR\nIl6tEqU4I4PfbBnzMdjtaqTxYUJJ6S2IWrl7ng1K6a9c42FL2tXbreb2kzgTy1uvkwc8Jg+D\n/a4HnlZ6C6JWHjZZCf1VTqFFmV4YywNlnLA/yRbPlcVi7RqlWRkMUHOSF8aCqJWnTm8w5Qaw\nMlNH6OYSeqvE4kftPHo0WCfuaTl50jcneWEsiFp57EQvJp9uMao4HihzZcUperxXqtF9X7x/\n1E7DRYMVlbY7Z6I5yQtjQdTKl8j3IkKA9cUhc5yLngV+9f2mguYkL4wFUStfIt+LSAH4mcwm\n2U7fZjSOB0AA/nVc3k4sIvteFM/kzbmjcz4AA/JXMvRC533RaaATiuQBjDYwIIN95Zc8yQkE\nOLokgA/FkeylX9LNlQXRCw3IYFlxz7AQ4KWlmYk1uEhxVOk9iFq5eeMBGTxU5DMsuTGD/kbX\n2gAOtXy4eeMBGTxU5DMs+doUBDi69KfQxyRzZem58d3dXVMsTl6momfW4uRl9fIiHmZ+09M3\nqWafudCADB4q7g5aWZYDAY4uuRNr/HJCSeE9oL1Td0T14PIhIvrF+CZcPiREyF1exjc9NQCf\nhgUeVyMBMnioRQCe2ociwAGlGEmW0Nscp68wDe4BJemnnwjA/+ckEUy/Fvd/6y/gFS/xraU4\n05ueKMCnQYHH9cCADB4q6RlW2GIEmCqfiRzBAJY6rQe6dYBTnmEFLkaAqVYF8PAUDgHWKdkZ\nVuhiBJhqAPCFri3wUha7yR10pm3gyTcN3wb+/fffJw323k2ikgiQv4O3LknfxhtfssssJ1IX\n64WeftPQvdC//+tf//o9Y4N95ZLTNR+Bc/dXfOtnsjJmvWmXupxQeg+iVm7e+KG/A4fbtBZ8\nSV++XLdc1v9eTnz5NBultyBq5eaNB+Sv+BZbtuz0gayJhdeLmjRlcNH+EH/rypbddb8XfM1S\nC6W3IGrl5o0H5K/4DnSH8VJ09yo1ak72jP1MtNh0eZF8ep2rwbXBYF36o4uMIdWH6REkqhA5\nXTfAdRb+iu9Qkj+OdA8dFWDpAkFNuP4CQRatdHDl2EZq88evnRcNFm+Gs+w5dHuvELsV2ELk\ndAUAZ++v+A50ScoNubvwW8yrkYSxnpFwZb13oZZu3GfqPZcaZhjppSw6U2vJYG6xGLCInO6k\nFCKnawA4d3+HnVgHdh+Hyzbmig0rA1jSILUjBidqAyPAwp9GAfJXfAc6TYfd82Uz+UL/7N0A\nwLadHAlOoRFg9qdRgPwdvMP7hk3hsLlZx4zsraoNrN9DS0MJY8MMeAROVbl54wH56/0Oc7K3\nol5owylWdrJeMpgoRE7XC3A+SgJwGA+iVm7eeEAGC0KAhT+NAuQvAqwvNm88IIN9ld6CqJWb\nNx6Qv3kALJwxs0XtAqwTiwbPUoicIsDxlQXAQp8VW1bW1InlWrlfsXnjARnsqxA5RYDjKweA\nhXEl9uhnwzCSa+WexeaNB2Swr0LkFAGOr0XHMUb0yNQ//LkvyFOADPZVesaiVm7eeED+4hFY\nX2zeeEAG+ypEThHg+MoBYHht4BtQiJyCBRiQsgAYe6HzU4icggUYkL+LAHzqi0PcvW6vL3as\nxVxs3nhABvsqRE4R4PhaAuDT6Rt/KB1ZNUfgaQ/u73EqZXyFyCkCHF9ZACw3eY2Vn+4lgtHg\nGAqRUwQ4vhYAmK7FzB7qSd3Lnc6mygm/97lfTrgChcgpAhxfSwHMiNOT6g7wPQIcWyFyigDH\nV3yATz3AI6Q6AXyPAC+iEDlFgONrMYAJcnsZVY828J4CbBGNBs9SiJwiwPGV3TjwdC14Qf8S\nCpFTBDi+8gA4RnGuBmsynsOEdFnpLYhauXnjAfmLAOuLzRsvOvrjjz/O3UMjwItXbt54QP7m\nATC5o1koa8LUYt540d+///3vP2ZssIvEJd/TWxC1cnMiAPmbBcD0nqKhrAlTi3njh/4OHO6T\n2i9UWNSF+Ie6qGEthaZR6QFwJRVX4SyYKEZ/+afwU0AP7u4kgtdicLdEsLRusGYJ4UFoGpUe\nR+BKAriqKlN4yGL0t+4/i4cCerA6gPlC37KRtfh3Pe55KvkAXA2KbwPgnPxFgPXF5o2faiNx\ng4V7X3U7XwAA/9DI6gWfjeS/P8eCAQmQvzkADLkNPNJLWXCPB0nmp1q5A0xkk6WKSijuCkJY\nMFGM/g7e21lBPQDcCy2Lp1Vn8MDNen0AV2sEOHN/8wA4RnFSg4VmUaH+Yerk8PVjhsqy7X92\nBLjqgK3lghAWTBSjv/0H8VF6D6JWbt54S4PFYYTadpghoXAcuBUgfwECLK+XlaXBIBUA4Olx\n4GoVAGcjeAArK1aiwaE0H+DpYSQSgQCHEziAlYuH0eAo8kvesA9rBOAKAQ4nBFhfbN54QAb7\najJLDNRrX/yd0HmdALhCgMMKAdYXmzcekMG+msoSA/XaA/ydAHy9Xs3DSBTgzxCGob9U4ADG\nNvAymsoSO1e+MoIZwN8rBrBhGIkNM31WcrmHYegvFTyAsRd6EU1kiTV2rz3ADb/fP6rpNnCw\nYvSXCiDAi1QeMscgZcgSQZSier0ygptGbYPvb/QQzMD+YHG1PK70AWMYCZAQYH2xeeMB7aF9\nNZ4lgmglAkwatQ2/v33/rW0Ds+MyHS+SxpVwGCm08gf4/v6eF2tveYYGR9F4ljqAv39nBDcA\nkwMwEWsDc4ArBDi6HADupsxSLeUBXUWWPXzU3nQUDY6i0SzJ7dwP1itVdVx/8ONyA7AuGIeR\nQsoe4LL7QbWQB2wdd0rw/tHptsFo8CyNZom3czs1h+JPUnalZ9X0ANwcmr+z7uaW4KoPxmGk\noEKA9cXmXAAy2FdjWarksV5yKv2/DbhN6ZU+9/3anFAzgP9oAe7iKwQ4sBzbwAgwFSCDfTWW\nJWWslwJ87VV9b0UAvvIrDCsejOPAYeUFsPWSKwH0NyL28JFoqfc1CpDBvhrFQGnWNtD+cRXb\nwH1EW4v8CmwDh5MbwAk6sbAXOo1GsySvusEAboeABQlJnZ4i7VGM/lLlD3Cays15AGSwr8ay\nVMkAX1uA2YBS1YHcJ5WddYdyJkwt5o0H5K8NwL5LrkT2IGrl5owAMthXY1mqZIIpwN+//3bl\nBF97gkWAQy+Vhf5SuRyBRX4z8CBq5eZMWBosJ7cYfyo7uSZPPk0WwzVTpHXdWOivh1wmcgz+\nCpHUVQD8yy+/5LyH9pVj8tiBdyRcBVgNtXbGM3yt/jqMA5eDqVghkroGgH/55z//+UvGBvvK\nLXmVcposhCvNZs0MS3tnPMPX6m/+c6HTVG7e+KG/A4f7pBZsGdH2vlfd8qKjaxfmJdfkyUNF\nxnBtMPrrIwRYX2zeeAeDezdr8W/N6sF5qU+FVf+xMrhkDNcGo78+QoD1xeaNnzC4Vl3UuDpI\nfb4AV1b9x04AfwIHOCd/EWB9sXnjp9pIjgbneA7dZcIKYGVwyRj+6Xa81hejv91H8VKIpK4B\nYH0vZSF4PGkwPwvLSjwRQ9TQXxaQj78LAmx/KS8ogyXxtNobnHcbeHiwpMXWJ7/or/hsHC0H\nsMO1+PANVnswCl/2EFcAACAASURBVL3BRf9sXmrzUKkA25/8or/x/V0SYOsrAddm8GCYoRZc\n7YYZsj0CS41bBLiNyMbfxQB2uZR3BQYHzHRm+iRK/SEiC5C/SwGs3E9hPmMTxRkBnGcr16zR\nLOl6kG/vCDxQSn8RYH2xeeNd99A5jhNNaCxL2vGiGwc4pb8LAbxXb2k024OJ4pwMBqixLGlH\nfG8d4ITCcWB9sXnjARnsq/QWRK3cvPGA/EWA9cXmjQdksK/SWxC1cvPGA/IXAdYXmzcekMG+\nSm9B1MrNGw/IXwRYXxwyxyCV3oKoladObzAhwPrikDkGqfQWRK08dXqDCQHWF4fMMUiltyBq\n5anTG0wIsL44ZI5BKr0FUStPnd5gQoD1xSFzDFLpLYhaeer0BtMyE0gcb8TiFh61cpSN0N9k\nQoBR84X+JhMCjJov9DeZEGDUfKG/yQTtIhkUCiUIAUahAAsBRqEACwFGoQALAUahAAsBRqEA\naymAy+mQLlS8i2nYqt0rR9kJ/U2khQB2yGrZ/QhetXvlKDuhv6m0DMClQ1IdPXCp+hYNXkTo\nbzLldwrt7IGzXzdl8EJCfxMJAUaFEPqbSDcI8E35u5TQ30RCgFEhhP4mUmSAu179fAy+LX8j\nC/1NrZs7At+Yv0sJ/U2kWwP41vxdSuhvIuUHcNSZOmV5c1N1lhH6m0g4FxqFAiwEGIUCLAQY\nhQIsBBiFAiwEGIUCLAQYhQIsBBiFAiwEGIUCLAQYhQIsBBiFAqw0AF+ed2WxfZ4OLAr5wYSe\nS4dgVCShv4spSSrey4KqvExFOhtM49DgtEJ/l1OSVGyKQ2PteVscpyLRYIhCf5dTklS0Blzo\n78uhoH6T0l2xPZNn3nbN7vtY6w0WXnDesTDyZdm8NjFkv0+Dj+0TqARCf5dTEoB3xWv/Bz3d\n2pCPUhza065XdgZ21BssvKBswy7tOVtn8K59ApVC6O9ySgLwuSw2xxe6L66fiA/H4pnYsr3U\n9LRrU7w0Dal2X9t+zu6DSi94LkpStq0v2+4F9Imn4qauC81J6O9yStQL/bQhe9m3mphJP8aO\n2PLeeE/3vfX59Wk7YrD4gnP7zIY8OgsGnwcvQS0s9HcxJUvC+/GwJTviohU3hP7cDstq0S3d\nC6RHQk2oREJ/l1HSJJCTIJ1fh2Lz/HpGg6EL/Y2vRL3Ql/Y3P2Nif9HzpG1rzcV4itWXaU+x\nhi9BLSv0dzklScKx2DbNo8uRNHSOpM/ihdlKeyqeyKM3sc+Cfs6if3H/Av7Mkfy5RYMzEfq7\nnNIkYdPO1Dl3IwTv1GBSVhO/BmdR7AEvEl9Qt892wwx1wU7b+BOoJEJ/F1OiJDxvyUg+PdE6\nHwq6wyanWNviQAcfaNGIweILav6TDPS/kEfPaHAOQn+XUkZJmG0IDgxmLfQ3htYBMGlUNSdm\nh4CfBhVa6G8MrQPgtlF1DvhpUKGF/sbQOgCunzdF27xC5Sr0N4YyAhiFQrkKAUahAAsBRqEA\nCwFGoQALAUahAAsBRqEACwFGoQALAUahAAsBRqEACwFGoQALAUahAAsBRqEACwFGoQALAUah\nAAsBRqEACwFGoQALAUahAMsP4I9e4mOtZgfEfwdNQOA0w5N/Mu1iEoekTm8wIcD6gMBphif/\nZNrFIMBhhADrAwKnGZ78k2kXgwCHEQKsDwicZnjyT6ZdDAIcRgiwPiBwmuHJP5l2MQhwGCHA\n+oDAaYYn/2TaxSDAYYQA6wMCpxme/JNpF4MAh9EtAnw6nSZrCJzm+hnafX38s62JUTO+NoCT\n+XuDAJ9OyvcpPsDgbqXnn201RpPxtQGczN/bA/h0Ur9PCLAi/2wrMbqMI8Ch3tjrVS4pXCXA\n511RHumDQ3vTnqJ4L7ftz/pCSi9CYHvrW0Dyz7YSAxBgOP4iwPoazNvP7hi/4w/KC7v//KH9\nWdPSjRCIAA8zfjp9c60lbIh54wH5e3sAh2gDk1vV0jvMH4tmf7wt6D6Y7LHZzyfy81g8C4HQ\n+I3bBm7+/KZ0a/m8UySAAfl7gwAH6IXeFBf+oDm9OpOdMbt7Lfu5oVkle/Au8KYBljN+ogBP\nEpwOYED+3iLANgETWSsGD/pdMP/JpAQCkn8yp2OyBxiQvwiwPmAia3AM9pV/MqdjEOBwWiHA\n6hny5JclyCk0TWfBSpVABFhU7m1gQP6uD2C1j4rs7c1fF/dOrGP9Lndy0HQW/On6hTzVBSLA\nQ0vy7oUG5O/qAFbHLOjpmplgV4DPfBxBHGag6aQ/WWnxLgQ2QV6ZTiZ/O+xish4HBuQvAqx/\ni4kEvG/b8X1hoL/uftLS7ZsY+AwN4Oz17du3eJXD8RcB1r9F4DTDk78ddjFzQ5jLkY7AgLQ6\ngBdpA9+A/O2wi5kZ0u6nEeD1AbxEL/QNyN8OuxgEOIxWCHCQgMBphif/ZNrFIMBhBG10A7WQ\nvLByiME2cBit8AhMjB32ZNUf3Yn18Nf4WwROMzz522EXMzuEWogArw9gQu5D8++hh7TuuraG\nvwxvETjN8ORvh11M1uPAgLQ6gCm/TB2kNW8yDWV6C/P2/zour3xmKH877GKyBhiQvwiw/i3M\n2w/IYF/522EXgwCHEQKsfwvz9gMy2Ff+dtjFIMBhtDqAF2kDAzLYV/52aGKUofnML2YA5O/6\nAF6iFxqQwb7yt0PrCKwldQD5u0KAgwSYtx+Qwb7yT6YSo7RYTqfML+gH5C8CrA8wbz8gg33l\nn0wlBgGOqFsCWGmJGWowbz8gg33ln20lxgZg/rwYhwBb6IYAVlpiphrM2w/IYF/5Z1uNmW4D\n8wil68LtjexDzBsPyN/bAVg5DhhrMG8/IIN95Z9tTcxULzT3ZugRAmwhBFhfg3n7ARnsK/9s\n28QoANNhewTYXQiwvgbz9ouOfv36dYbB2V4N5p9tixj5FJpNvckT4Mz9vR2AI7WBv34dOrxI\n/heQf7anY5ROLA5whm3g3P29IYCj9EJ//So53Ka16JcAp1lufhXtAuHiM/ne9Mw/29MxKsDt\nKXR+vdDZ+3tLALsEmLd/yuCi/dH9pi4Xume88r+A/JM5HXM6/fWv/yOPK6m713wBzshfBFgf\nYN5+K4Nr1U7V2JsE+OOvDcB/FQu0zZvMAa6z8HfdALdDE9/YL16irUGeHW3UVBvJYDC/qc6s\n/C8gfzumYx4eGoAfHmSnPN4pURs4I39XDTC/MOkf9JfuMqSuBuX6JKN+HTr8q4PBUt5vFOCH\nh38MAfZ8p0S90Bn5u2aAKb/tl4VeXXjSt7WaGpQnzNv/67gGWc3AYF/52zEdowEY1lTKjPxF\ngGkNMQCWuzLUNtKtdmJ9UE9kqwBNpczIXwSY1hAYYHWYoUu1/MwtDiMRgv8hOwVqKmU+/q4Z\n4GXawFqD4cvfDruYQQg4gPPRqgFeohc6d4NdJN5gz98OTYymhVuLBQiwt9YNsH+AefsBGeyg\nMhbAuhZuLT4BrQ2ckRBgfYB5+wEZbK8y1hFYe3ytPwYTsGD1QmekNQEsfBekBq3p1af+Fjvi\n3t8oQAa7aDmA2eWEY32K9u+EAK8I4FPfT/UgdSkbXkxfVfcP+UuMAmSwizjAPzQKWO03pu53\nW1ILBZkJkL/2AI/snrMBWBgmageNbGpgL/gmVNC+xChABrtoqTbwqe1YNF3hafdOeAS2Bnis\nh+MmAV6pluqF7gA2XOFp906RAAYkW4BHezhuEmBAe2gXRQNYk/SsAQbk73pOobENPFtLAdwt\nqYOn0LPlDPAPgXs4AqrtD2n7SxxfJT00C5DBLloMYOyFDqYVHYGDBphzAchgFy0HsDIOPKMW\nrxBzIgD5iwDrA8y5AGSwr/yTaRczMpHDoxavEPPGA/J3bQBrd+hqDVIYAqzI3w67mLq1YTiV\n0qsWnxDzxgPyd2UA60/JlBrkMARYkb8ddjFdv2HnR5aL2mXv77oAHmlUaYYlp7qpzbkAZLCv\n/O2wixmEIMDeQoD1b2HOBSCDfeVvh03MyAX9zu+EAK9oLvRHIoC/qgZnusqGi/ztsIgZXVLH\n9Z2WADhvf9cFcIo28FeNwV7K6FtRJ1vUzvGdFgA4c39XBnCCXuh5N78StGKAH4a09utCdz7k\nDHDW/q4N4FAB5u0f+qtdN7ioh6ubZXAbHaJjWfC1x83yT6YaQ1cWFP+md2Z4ADGMlLu/qwF4\nH/YjmLdfMli3cj9frVBZZ5StVVhrnvGywk3Holge4IcHieATBVi4wj/jTqzc/V0LwPf39/xE\nTf0mfBuW2HxVzNsv+as1WPxdq3aORERWWTxZRvrbocSoAJ/++pf/OYEAOHt/VwLwqQH4L+xb\non4VyKVr8q3wpt7CvP2ywZqbX1kYnOA2SfZncePZHp+93D0zCfD9/TdIAGfsr1yrXRvJJYVL\nAEz5/ctfhfXbhafptad9ie7LEnoc2H4PrfkdU8fiYhk5mu1vGtSEzOryKbeBiV9/G1xOmG8b\nOHd/pVot20guKVwO4L8gwJPabc92gWPJZsnUESwkdqIX+mPfAMxf8zF8YHQlXIh54wH5K9Vq\n2UZySeESAN8zgL/kCrDmd4JOrELUVPBYso0A348ALGu/3z+yXsdZfY/ZAJzQX6lWyzaSSwqX\naQM3+muKNvCkwTndRic2wM1udAgwD5ResCdH4D17ZCAYBMCp/VVOoa3aSC4pXKYXmnxFUvRC\n6w1egUazPd4GJudBgzYwR11GvuH3/m/3BN29ieB8Ac5I8k7Bro3kksKoAKsnbLrvQ8xx4NwN\n9tV4rkZ7oQmXeyGfvLEinFt/dE/Qneq+B1jjGwJsIRFg+1MslxTGBFhtcWl36AgwF/d1sEiw\nVu527HscZYDvTQDvx3xDgC0EHeDThwywzzfhNgAurf2tPQDe73scZYC/fPmiA1h4ic43BNhC\nfu1qlxRGBPikAKxvUyHAVM8Cv89TwR52KDG87dvw+2U4DtwuK9tJ6xsCbCHIAPPxoD5AOIub\n+RHM2w/I4KFCzMSaSqYQ0/Zh3TcA30v9iN/Yg3YwqfdNcA8BttDIMNJEG8klhQgwSLnbMRpz\nf//TTz/da0fuuhbw3d2dMq6EAFtIBNi+jeSSwmgAn3gbqwsQm2EzP4I5a4AMHqr3tzyYhxuc\n7RiPIfw2BOtCeAP47q4lGAF2lAiqfRvJJYXLjAOHDoib9WQa9FMaCfZPphJDhpf4ANMwROjC\nYo8G+990AAPSSmZiBQ+IkOoc9FqQcf7ztnipj8XBFOmfTCVmTweIRwHeywDvtbX4fpjbAthS\nLilcAmDTjFq/jxA4zblo0860KzZTO2v/ZCox2sGi+kPhdi9EWr4TAgx7HJiZXZsm1Ao1jE4h\nuh2Aua9sjZel3vWRSPg9KKdF4q9hDMos0AC3u+vaOCW+q2FsEr72LeJmPZm2/BR6W7+Ro/C4\nPOwYj5FaupqQvMaBAUk7F/q83Zlf5ZLCeADzL8SjFcC6ywjH3yJawtPq3I40lOd6oqPS3Y6P\ndmr6aIwJYNd3mhuyVMKjSwJ4x9tIZoJdUhgd4P2j8ZqWDwRY0OVpUxSbY+NycTQGjidzPNld\neseOwGSsqB3ufdRFDAsQYAvpe6EvIE6hEeCIGk3meLZPYwDzAy8d7W0LHve6iEGBxLidcZYh\nqdMbTBKoXRtp4hw6Dz2Kmoz+RrTAp1qHxnhgu0stwf0OUt8L3R2B272uGjEseJzomxyl0yIk\ndXqDSQJYaCOZ5JLCBXqhLd8Be6EDLOw+DvBpGmBh1AgBDiPZyMuRtJGeJtblcEkhTuTISPMX\ndhcAHnJ86tsouvNjPcB9TQiwn1Y1keMu3EcInOZcVE5fR9hqNFePe5E6XYR0eJVmTLYFj3IB\ntoG9BBvgP/3pTx/9RA7SQaLtqUKAuUJcTjh21ORquxWlosHvthdaqAJ7oT01nMhRw5rI8ac/\nUYL5V+ru7q6Bl64sO/sjxM16Mu3mL+zOn3EBWHhKrGa0ijFXwoVEzfKSggzwn5j4RI6G358I\nvyrBCDDXuZy7sPscgMUXIMBhBPkUugWYjwMTgH9CgI0KuSaWfRtYfMFgJpaRXwTYRqKRm8PL\nu92rXFK4FMB3CPC0gi5qNwrfSP+xCnCuC7sDknIxw+7pdbqZ5JLC2G3gdiYWW9SBEoxt4ADy\nsGMqxjSV0vOdEGAR4Mvr07ZdbuXZfCh2SeECvdBCCfZCB5KPHVMx41Mpfd8JAVZOpd6fDyWQ\nTqyYAdESnlrPu8bb7XRTyT+ZozHjUym93wkB1oL6foAH8Kzb3N0MwJcN3TkXxdtUpH8yR2M0\nM7E8agkTskSyFxGUI7Da2yGMSJhHIxDgXofiSEYLX4rtVKR/MjUxwkQsBDisRFDPL8etXUeW\nSwqDAKwSKpSQTqw7BNhGfCmdCLdWMcSMTqV0dSVcyBLJXkRyL7TdUJJLCkMArI74i7PwHoXu\nzVAfIVrC0yoJwL1X0lRKZ1fChSyR7EU0BHjqKiQulxQuB7D/lJ6bAbg9hZ5YUpbIIVdTMdoJ\nVzHptAhZItmLaB1H4DsE2E4Xu+u9a78ldfgzI5cTunmCAFtJagPTTspi+/Sa2QX9E21gxu/o\nFwsB7tWtiTWh0VyNL6nTnyiPXE7o5AkCbCVNL/QOYi906I8QLeFQNJaq8RU5+IF2P345oYsn\nCLCV9OPAu+wAXjogSrIhaSxVTgC3E6+8PEGArQRlHHjpgGgJz0Mx1sRSAe6mTvp4ggBbSTsX\nerInyyWF4QDuvyDtr/YhAuysGcNI9m3guzsDwQhwGGmuRnrL9GokMl3jjk7ZaH/xh5NDigiw\nrDnjwNa90AjwAoJzPfC+vWKwE/9zelIPAiwr6ESOsS5mBHgBwVmRAwEOKB3AZaP+L/tcjQ/y\nYhs4vkYAzrATCwEOKI2/ZfeDyjpXumkaPAZ7oaMLDsDYBp4r8+1jYwA8xxME2EpwTqGxF3qu\nYgGsXkmCAC8mSAAvGRA4zQAkAPxDI/sX/kwU5SOhpoUA6wMCpxmA8AgMUso48Mgp1mgX5UIA\n85Pmjw/p14dxZNL7Iyyd+4WFbeAlkrz0mxgAHnd3GYD3rMuKr8eyHwxe2N3g2y3AnLXztig2\nmvLnUlPY6JVffkvyyl78tmuSfCDj7hdy+ch2+qZj+o7Fw+vkC+3qQoB7AfLXci+RGOD9ft9d\nMjgQed5wy2n/jzCRjpHZ4iOd92+lGEBf/NruJ9/qd36J7tQEuJHKy8n16ezkC7BhHNgt5YuG\nTOQCjr8IsD7AK9ejHjyLATSoLF5qsrjcpt4UhwvdbR/trJA1dlRw1ajFk3COzcRyTPmiIeZc\nAPJX/kTdHaClj9j9+MGtizKMHhv9/PPPj4r4k+3DhdQmiJwlldSW844+aPN2PjQnT2ca915u\nm5zSxDUW7rr7xzGn28ror0vr/a5gtx+7kEouQuUssC9+KosN/eKUnt8MWSPdHB6nx6lXrLMI\nWY2/Eqhjd3DHNrBqcHuWdOQL1exag9lf5IypKLbFobGOOEBLd53BjY9vLMe74lWo+MBPtmgl\nm7qvnO/Z22Lm1DN9ZLmSmXkcWFKXCXeAk68ZaxGyGn9lUIv3bXG+bOWFv1MDnFkvNM31pjlL\nqt/JQ7JG3Ft39nQkiy7TMybqfv1EHSSlTV67FSFJH8cTyfK52dMeX85tvdsLe+kT+XEkBg4q\n74uL4twUE0teiyfzp+0/tQfASvNkMpn78fsDu1ST/BQahr+SkU0l5PNc5IW/kwO8dICFwY03\n9AJqYvVlYHzj1pnsRokJxMpzX9ov6Upuf0HTfCHLVBWbN/r69/alGxpEdsyDyvvisuD9k2fy\nZ3DxRCh9zDcDMAx/VYBfGf/D8ugAh12WfX6AjcFbfizrskUfsL/6cvlnH/1UtD0b78fDluzv\nxSBt5X3xa3OytTn37xpabR7UQaLbARiEv9Jzu+KF7CDelgbYZHiMM+TJAAuDD8Xm+fU8x2C2\nx+Z/lU4GN9+KTcGGGFwAHumkVNXmYa8QfANtYEj+Ss8RcumOR174O/JMLFM3VIw+qskAC4NZ\nr6H5FKvPv3SKVTYvYY+L9sXsKRq05edS7IXKKRbXs/gGVhrrpFTln0z4vdCQ/JWfe92QXc/U\nmJVLCm0CTAO5UYZ5JwPM298a/NZ2WxybfL1rOzlIMGsjPbH+Cx50KHaX4nJodpRNeLOfvRxZ\nRyTtCXliVbJ7kA0q74tLMkeAdnK4tIHHOimNFjsm0y4GAsAQ/PVrPrmk0BrgEUazBfjYHcvO\nvPO/IBkXhxnqmvdSdsMM7Z6dzc8hd0hgi+nTh2RcgtbBn38fVC4Ws3cn/ZPWvdDsg2s7KY0W\nOybTLiZ/gGH4mwXAakNLULYAkzOVLestaA5rdGj/me0y+4F+EsbGCcl4/U5oI52PZRtVP2/J\nhIELfcF52xaeae21UDl9XV/cvL6kzlqPA7NPpO2kNFrsmEy7mPwBhuGvZCQ3Vmzwmt2N30ec\nYRvYTccRWtRil+6o/kUOM7HGOilV+SfTLiZrgN2U1F+xxrIQZKzTJYXzx4Hz64V2VD9XdqAw\nBjvNhR7tpFTkn0y7mBUBnNRfscZngV/9R+JySeEaJ3I46s2WMR+D3a5GsuukrBFgF6X0d+QU\nekIuKfQN2E88H/cjuCfapNfJAx6Th8F+1wNPyz+ZdjFrAjilv1l0YunEVyQVFp0cO5XOH2CA\n8k+mXcyqAE4oBeBn0hW+nbhDg0sK/QL27ZrgdF5eWzTWmYUAW8uyk7JGgKFIAvhCB63oGLZJ\nLin0CmCLtrN1Yx+7q49GCEaA7WTfSVkjwFAkGXkojmQv/TIx0O+SwhkA3yHAIWXfSVkjwFCk\n6cQazLDWyyWFXgH8rimZAvzruGb6EVn2vSheydxbxOQRYt54QP5mCrBwr9kc28CADPaVTzL3\nCPDi0p9CHycG+l1S6BnAYc2yFxqQwbKsOilrX4D3lglPHmLeeED+yp1YfA3Ms/FVLilc40QO\nQAYPZdlJWXsBvEeAl5dyqkzW/9gcJ6bHu6QQAc5Ilp2UtQ/AQh8FAryYsp3IseQ73A7Aln0c\nNQIMxV8EWB9g3n5ABg8VEWDxmlAEeDENjORXJk7KJYUIcEay7KSsb3wcGJC/IsDvBV8OYEou\nKUSAM5JlJ2WNAEPxVwSY7J7rw/TOGQEGZLAsq07KGgGG4q8IcLsMn8XFjS4pXDvAv//++6TB\nfj0NqFQC5K8CsNVsu0Xxyhvg3//1r3/9nrHBvvJPpl0MlCNw7v4iwPoA8/YP/R04zFPJk8mX\n65bL+t/LybqTskaAofiLAOsDzNs/ZXDR/hB/68qW3XXbd1LWCDAUfxFgfYB5+60Mrg0G69If\nXfadlDUCDMXfIcB5rkoZ/R3Ct4Hb/PFUigaLN8NZ9hzavpOyRoCh+OsH8K3rV9FhfS9l0Zla\nSwZ3pzr1EntoQfZnWDUCDMVfnEqpDzBv/6/jGqR2xOBEbWAEWCg1CpC/CLA+wLz9UwbbdnIk\nOIVGgFmpUYD8RYD1Aebtn9xDS0MJY8MMeAROFWLeeED+IsD6APP2W51iZSenPg7/ZNrFwAY4\nHyHA+gDz9gMyWBACLJQaBchfBFgfYN5+QAb7yj+ZdjEIcBghwPoA8/YDMthX/sm0i0GAwwgB\n1geYtx+Qwb7yT6ZdDAIcRgiwPsC8/YAM9pV/Mu1iEOAwQoD1AebtB2Swr/yTaReDAIcRAqwP\nMG8/IIN95Z9MuxgEOIwQYH1A4DTDk38y7WKyBhiQEGB9gHn7Ae2hfeWfTLuYrAEG5G8cgCuX\nHCPAOco/mXYxCHAYRQG4QoDByz+ZdjEIcBhFArgyBziZgACnkH8y7WIQ4DCKAXCFAMOXfzLt\nYhDgMIoFcGUKcDMBAU4h/2TaxSDAYRQB4AoBXoH8k2kXgwCHUTSAq/EARxMQ4BTyT6ZdDAIc\nRjgOrA8wbz8gg33ln0y7GAQ4jBBgfYB5++cYrMl4jkuA+ifTLma1AC/sbzyAq6kAWxMyB/jH\nH3+cu4dGgBcPMW88IH+jAVzdCMA//v3vf/8xY4NdJC757p9MuxgoAOfub0SAK8scgwaY+Dtw\nuE9qv1BhURfiH+qihrUUmkZlVgBXUyHVnDcyZwKQv7EArhDgol8iWFo3WLOE8CA0jcqsjsDV\nFMD0C5YW4Bz8jQSwMJJ0gwDzhb5lI2vx73rc81TKC+AqW4Bz8hcB1geYt3+qjcQNFu591e18\nAQD8Q6OUn6PRZ6PJAHPEDAHyNw7AlTCZY+UA63spC+7xIMn8VCt3gIn8k2kXM3l4rarPyYiJ\ng7ThjcyJAOSvfYUj7uI48FA8rTqDB27WCPC4JgGuFgM4c3+tKxzrokSAdQYLzaJC/cPUyWHr\nR0CVZWtuNgAzOj+rqQjzQdr0RuaMAPLXtsLRLkolP9ePwYoc0zvJ9QIsDiPUtsMMCZUNwPFD\nzIkA5G/wU+jr9SquyGFxlrNKgEEqB4DtryRfZBw4e3+dAf5hoovyjz/+qIX+wYhdhQkFyGAX\nZQCw/YWoywwjZe9v6CNwcwC+fu9dsGmm4BE4R/kn0y5mcgKQTT8XAhwH4I7gqa4IQ4btn0eA\nY8g/mVYxvGXVfTn6L0xbdG1rGfn6VNUVAa4tAJ7qohzmh/GLAMOXfzJtYnjXSIer8IVhD68t\nwNUYwM03Ld0wUj4KfARuAW4JbgcDJghGgHOUfzJtYnjLSgC4+8Kwh80XqR5EyBVQgJMNI2Uj\nnMihDzDnApDBvvJPpkUM7xrpjreVzOm1BVh5QqgCT6FrBHgsIHymgck/mRYxvGVVVexcTVoH\nkfL72/WPD97UHb64reI6OhNr8IJa+0Tq9AZTwLnQFftfSclaI8CA9tC+8k/mdAzvGmn4Ja0t\nLcC/UYDJmbIEMD9kj7eBqzGAK1uAAfkbDmBmRNdmuZUVOXI32Ff+yZyO4V0jLcCdugByAP7t\ntz+unFPpxR/mNvBwX1BrnzBvPCB/QwJciXvSG7mgP3uDfeWfTLsYGkJaulft06NtYLHVrG8D\nV2MAVwgwgz0+DQAACDVJREFUlyY/7U70WvX9iZZmIsA5yj+ZVjFVD/CQYPat0bWBhyPFo1cj\niQeR4WcRj/LmjQfkb0iAyTnNlREsZBEBBin/ZNrEMPSuKsDdKLDSBhYOvF2r2Qww+1WrT3ys\nyN9QAFcM4CsCvBL5J9MmhrVeryrB3SCw0gYejBSPX04otKe1APPzQqMA+RsQYG4HP5GubmNF\njswN9pV/Mi1itOi1T/BDsNQGVhrDk2/U1oXjwBo5eIUAw5R/Mi1iqrEZtny/T48ErA38XTjy\nOr1RNd7PtSJ/EWB9gHn7ARnsK/9kTsdUYwBXMsBsoOnDB+AKAR6XkCb2mC/D8V/6j/4fBhiE\nAOco/2ROxzCANQTzBixri9E2MJvqIbRsbd+IvwQB1kjIEm3L8GU4/vvf5h/5LwTcMMBycovx\np7KTfzLtYuSQkQbu+EixWot+0nREgHPwNyTAFaH3vy3EwwCTgAP8yy+/5LyH9pV/Mu1ipJCx\nHioHgLW9XPMBztzfeQC3bRm+DEcD7r/b/0LAePpNGbZ/Pi3Av/zzn//8JWODfeWfTLsYBWBt\nA1czUjxaSxyAc/c3AMAVX4bjP/9p4P03+/9f/vzKL+gn/g4c7pNasGVE2/tedcuLjq5dmJf8\nk2kXMwxx7mJWQ/S9XHMBzt7fWQC3nRF8GY4G4H838P4bAW5TS62tuZu1+Ldm9eC85J9Mq5hh\n14hzD5UmZKSfOjbAqf0NAfB3BvD/+89//vPvVozgarS70cUneADXqosaVwepvzGAq+AAj3RT\nRwE4J39xHFgfYN7+qTaSo8E5nkP7J9MmZtg1MjJI5AOw7Ym4eeMB+YsA6wPM2/+r6LCul7IQ\nPJ40mJ+FZSX/ZFrE2PRteg1GWYeYNx6QvyEAFm+koo7GrR1gSTyt9gbfYBvYpmtE7aj2+zBR\nxoHz8TcAwIMbqaid+TcJsNqDUegNLvpn85J/MqdjrPo21Y5qvw8TZyJHNv4GAbhfnVvTDEGA\n68EwQy242g0z3NYR2KpvUzPS5PVhFgA4pb/zARaQrfoVORxyvEqAQ2Ua1epzybtsAfJ3NsCf\n/UGXTUOXCUaAM23lmuWfTLsYxxCPywmNIeaNB+RvYICvCLA2yxmOE03IP5l2MWsCOKW/cwGu\nPrvU8uvApEwjwCDln0y7GLcQj8sJzSHmjQfkL44D6wPM2w/IYF/5J9MuJnGIeeMB+YsA6wPM\n2w/IYF/5J9MuBgEOIwRYH2DefkAG+8o/mXYxCHAYQetbQS2kOcykptMiJHV6gwmPwPqAwGmG\nJ/9k2sUgwGGEAOsDAqcZnvyTaReDAIcRAqwPCJxmePJPpl0MAhxGCLA+IHCa4ck/mXYxCHAY\nxe/E+iF5BQFqQHUKlMww1aCzCDDKTQhwVkKAUW5CgLMSAoxyEwKclXAiBwoFWAgwCgVYCDAK\nBVgIMAoFWAgwCgVYCDAKBVhLAFzOenGjpB8ANVQIQ8JUEqgW2FoA4FlJLrsfiT4AaqgQhoSp\nJFAtwBUf4HJWjgOYNO8DoIZCgPNS7qfQ4b4vqHDKA+C+qhsWAoxyFwKcjRBglLMyOvm9eWsR\nYJSzEOB8FBHgrpMfAV6FgvhZ9zWEcAWdxSMwylEhBnBD1nPbQoBRbgp24Ax0IL9x5Q4wzsTK\nTGUZxJFQ07lufpIOzoVGoQALAUahAAsBRqEACwFGoQALAUahAAsBRqEACwFGoQALAUahAAsB\nRqEACwFGoQArCcAF0+F9UPpcSkHygwnR19sGo2Lr8rwri+3zdCC3bFNcmp/n5otxJi8vNoKZ\n5BH6q1NKgIvifVhaa/+09YzGocGZ6L1kFpeXqUhu2aF4bX6+NK95aX6/FgcJYPRXp0QA01/H\nYqspVf5EgCFqUxwadM/b4jgVyS17KZ5q8qV4oi95ohiLMeivTikBZr8vh4J6TXbXzZ9vu2av\nfaz1APPYpuy8Y2HkO7J5bWLY65v/x/YJVEq1rl0GHpPSXbElp8iq0e90f96cSNPf2+b0jD7T\n+Lur0d8xpQeYnmptOMCv7MTrqAeYxzZlZRt2aU/VOoN37ROopNrRE+JWgm+H9rR6zOgmbMMP\nteQ/9XeH/o4pIcDNbvlATpWO5LzpuS3dkDOn9+6MqY8mEmO3l/q5KEnZtr5shVOs5omn4tav\nEk2vc1lsji/0WCv7Rk+rNUbvmoPue/OdONDfO/bMEf01Km0n1rlmu9ua29Xo/Pq0HQFYjD23\nz2zIo7Ng8HnwElQqXZ425LD7Vg99eydukYOxavRTg/hzw/UL/f2E/tooIcAlHUbiLHNXtsM/\na9EtOVbzqP8TlVzvx8OWHGh1vmmMfmuOvv1R+BX9tVHSNjB7PDDyUGyeX88I8GpEznZ1vmmM\nvjSxZcFeUxYX9NdGyQHeDMb62o5p4yl0X6Y9xZLfAJVERXFpfw99o25ttUY3cawnujkMF2WN\n/tooOcBH0qPx0llaFG9in8UwWortOjnqLRqcmxpfmubv5UhavqJvtEvqSWt0c1Te0bHgp4L2\nb9Jnnli3F/o7ouQAt8NA76S0JF4PzrbYA14kxvJ6umEk9no0OBdt2p6O89A30vQlR1eN0XQW\nFhl8IkNMz7Xg7w79HVNygOvzoaA7azYqVNO/RgDuYwUfyUSOF/LoGQ3OSs9bMlODnkgLvjV2\nHejgkmo0GVVq50GzWbbM3x2byIH+arWSTODAIAwheaEFPqGkLdWcjx1Sfw6UjRDg0AKf0GM3\nJQSVvxDg0IKf0OdN0baqUNkLAQ4tTCgKBVgIMAoFWAgwCgVYCDAKBVgIMAoFWAgwCgVYCDAK\nBVgIMAoFWP8fdz27rnyX2IoAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Create column of correct-incorrect classification\n",
    "test$correct = ifelse(test$Species == test$scores, 'correct', 'incorrect')\n",
    "\n",
    "single_plot_classes = function(df, colx, coly){\n",
    "    ggplot(df, aes_string(colx,coly)) +\n",
    "          geom_point(aes(color = factor(df$Species), shape = correct), alpha = 0.4)\n",
    "}\n",
    "\n",
    "plot_iris_classes = function(df){\n",
    "    options(repr.plot.width=8, repr.plot.height=5)\n",
    "    grid.arrange(\n",
    "        single_plot_classes(df, 'Sepal.Length', 'Sepal.Width'),\n",
    "        single_plot_classes(df, 'Sepal.Length', 'Petal.Length'),\n",
    "        single_plot_classes(df, 'Petal.Length', 'Petal.Width'),\n",
    "        single_plot_classes(df, 'Sepal.Width', 'Petal.Length'),\n",
    "        nrow = 2)\n",
    "}\n",
    "\n",
    "plot_iris_classes(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine these plots. You can see how the classifier has divided the feature space between the classes. Notice that most of the errors occur in the overlap region between Virginica and Versicolor. This behavior is to be expected.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another example\n",
    "\n",
    "Now, you will try a more complex example using the credit scoring data. You will use the prepared data which has been prepared by removing duplicate cases. Some columns which are know not to be predictive are removed. Execute the code in the cell below to load the dataset for the example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 999  16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'checking_account_status'</li>\n",
       "\t<li>'loan_duration_mo'</li>\n",
       "\t<li>'credit_history'</li>\n",
       "\t<li>'loan_amount'</li>\n",
       "\t<li>'savings_account_balance'</li>\n",
       "\t<li>'time_employed_yrs'</li>\n",
       "\t<li>'payment_pcnt_income'</li>\n",
       "\t<li>'time_in_residence'</li>\n",
       "\t<li>'property'</li>\n",
       "\t<li>'age_yrs'</li>\n",
       "\t<li>'other_credit_outstanding'</li>\n",
       "\t<li>'number_loans'</li>\n",
       "\t<li>'job_category'</li>\n",
       "\t<li>'dependents'</li>\n",
       "\t<li>'telephone'</li>\n",
       "\t<li>'bad_credit'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'checking\\_account\\_status'\n",
       "\\item 'loan\\_duration\\_mo'\n",
       "\\item 'credit\\_history'\n",
       "\\item 'loan\\_amount'\n",
       "\\item 'savings\\_account\\_balance'\n",
       "\\item 'time\\_employed\\_yrs'\n",
       "\\item 'payment\\_pcnt\\_income'\n",
       "\\item 'time\\_in\\_residence'\n",
       "\\item 'property'\n",
       "\\item 'age\\_yrs'\n",
       "\\item 'other\\_credit\\_outstanding'\n",
       "\\item 'number\\_loans'\n",
       "\\item 'job\\_category'\n",
       "\\item 'dependents'\n",
       "\\item 'telephone'\n",
       "\\item 'bad\\_credit'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'checking_account_status'\n",
       "2. 'loan_duration_mo'\n",
       "3. 'credit_history'\n",
       "4. 'loan_amount'\n",
       "5. 'savings_account_balance'\n",
       "6. 'time_employed_yrs'\n",
       "7. 'payment_pcnt_income'\n",
       "8. 'time_in_residence'\n",
       "9. 'property'\n",
       "10. 'age_yrs'\n",
       "11. 'other_credit_outstanding'\n",
       "12. 'number_loans'\n",
       "13. 'job_category'\n",
       "14. 'dependents'\n",
       "15. 'telephone'\n",
       "16. 'bad_credit'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"checking_account_status\"  \"loan_duration_mo\"        \n",
       " [3] \"credit_history\"           \"loan_amount\"             \n",
       " [5] \"savings_account_balance\"  \"time_employed_yrs\"       \n",
       " [7] \"payment_pcnt_income\"      \"time_in_residence\"       \n",
       " [9] \"property\"                 \"age_yrs\"                 \n",
       "[11] \"other_credit_outstanding\" \"number_loans\"            \n",
       "[13] \"job_category\"             \"dependents\"              \n",
       "[15] \"telephone\"                \"bad_credit\"              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "credit = read.csv('German_Credit_Preped.csv', header = TRUE)\n",
    "## Subset the data frame\n",
    "credit = credit[,c('checking_account_status', 'loan_duration_mo', 'credit_history', 'loan_amount', 'savings_account_balance',\n",
    "                   'time_employed_yrs', 'payment_pcnt_income', 'time_in_residence', 'property', 'age_yrs',\n",
    "                   'other_credit_outstanding', 'number_loans', 'job_category', 'dependents', 'telephone', 'bad_credit' )]\n",
    "print(dim(credit))\n",
    "names(credit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation will be used to train the model. Since folds will be selected from the entire dataset the numeric features are scaled in batch. Execute the code in the cell below to accomplish this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>loan_duration_mo</th><th scope=col>loan_amount</th><th scope=col>payment_pcnt_income</th><th scope=col>time_in_residence</th><th scope=col>age_yrs</th><th scope=col>number_loans</th><th scope=col>dependents</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td> 2.2464282 </td><td> 0.9483849 </td><td>-0.86876113</td><td>-0.7645835 </td><td>-1.19202026</td><td>-0.7035652 </td><td>-0.4283287 </td></tr>\n",
       "\t<tr><td>-0.7397312 </td><td>-0.4170067 </td><td>-0.86876113</td><td> 0.1414888 </td><td> 1.18945982</td><td>-0.7035652 </td><td> 2.3323187 </td></tr>\n",
       "\t<tr><td> 1.7487350 </td><td> 1.6323204 </td><td>-0.86876113</td><td> 1.0475610 </td><td> 0.83664795</td><td>-0.7035652 </td><td> 2.3323187 </td></tr>\n",
       "\t<tr><td> 0.2556552 </td><td> 0.5655086 </td><td> 0.02505181</td><td> 1.0475610 </td><td> 1.54227168</td><td> 1.0276211 </td><td> 2.3323187 </td></tr>\n",
       "\t<tr><td> 1.2510417 </td><td> 2.0477820 </td><td>-0.86876113</td><td> 1.0475610 </td><td>-0.04538171</td><td>-0.7035652 </td><td> 2.3323187 </td></tr>\n",
       "\t<tr><td> 0.2556552 </td><td>-0.1552623 </td><td> 0.02505181</td><td> 1.0475610 </td><td> 1.54227168</td><td>-0.7035652 </td><td>-0.4283287 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllll}\n",
       " loan\\_duration\\_mo & loan\\_amount & payment\\_pcnt\\_income & time\\_in\\_residence & age\\_yrs & number\\_loans & dependents\\\\\n",
       "\\hline\n",
       "\t  2.2464282  &  0.9483849  & -0.86876113 & -0.7645835  & -1.19202026 & -0.7035652  & -0.4283287 \\\\\n",
       "\t -0.7397312  & -0.4170067  & -0.86876113 &  0.1414888  &  1.18945982 & -0.7035652  &  2.3323187 \\\\\n",
       "\t  1.7487350  &  1.6323204  & -0.86876113 &  1.0475610  &  0.83664795 & -0.7035652  &  2.3323187 \\\\\n",
       "\t  0.2556552  &  0.5655086  &  0.02505181 &  1.0475610  &  1.54227168 &  1.0276211  &  2.3323187 \\\\\n",
       "\t  1.2510417  &  2.0477820  & -0.86876113 &  1.0475610  & -0.04538171 & -0.7035652  &  2.3323187 \\\\\n",
       "\t  0.2556552  & -0.1552623  &  0.02505181 &  1.0475610  &  1.54227168 & -0.7035652  & -0.4283287 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "loan_duration_mo | loan_amount | payment_pcnt_income | time_in_residence | age_yrs | number_loans | dependents | \n",
       "|---|---|---|---|---|---|\n",
       "|  2.2464282  |  0.9483849  | -0.86876113 | -0.7645835  | -1.19202026 | -0.7035652  | -0.4283287  | \n",
       "| -0.7397312  | -0.4170067  | -0.86876113 |  0.1414888  |  1.18945982 | -0.7035652  |  2.3323187  | \n",
       "|  1.7487350  |  1.6323204  | -0.86876113 |  1.0475610  |  0.83664795 | -0.7035652  |  2.3323187  | \n",
       "|  0.2556552  |  0.5655086  |  0.02505181 |  1.0475610  |  1.54227168 |  1.0276211  |  2.3323187  | \n",
       "|  1.2510417  |  2.0477820  | -0.86876113 |  1.0475610  | -0.04538171 | -0.7035652  |  2.3323187  | \n",
       "|  0.2556552  | -0.1552623  |  0.02505181 |  1.0475610  |  1.54227168 | -0.7035652  | -0.4283287  | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  loan_duration_mo loan_amount payment_pcnt_income time_in_residence\n",
       "1  2.2464282        0.9483849  -0.86876113         -0.7645835       \n",
       "2 -0.7397312       -0.4170067  -0.86876113          0.1414888       \n",
       "3  1.7487350        1.6323204  -0.86876113          1.0475610       \n",
       "4  0.2556552        0.5655086   0.02505181          1.0475610       \n",
       "5  1.2510417        2.0477820  -0.86876113          1.0475610       \n",
       "6  0.2556552       -0.1552623   0.02505181          1.0475610       \n",
       "  age_yrs     number_loans dependents\n",
       "1 -1.19202026 -0.7035652   -0.4283287\n",
       "2  1.18945982 -0.7035652    2.3323187\n",
       "3  0.83664795 -0.7035652    2.3323187\n",
       "4  1.54227168  1.0276211    2.3323187\n",
       "5 -0.04538171 -0.7035652    2.3323187\n",
       "6  1.54227168 -0.7035652   -0.4283287"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_cols = c('loan_duration_mo', 'loan_amount', 'payment_pcnt_income',\n",
    "             'time_in_residence', 'age_yrs', 'number_loans', 'dependents')\n",
    "\n",
    "preProcValues <- preProcess(credit[,num_cols], method = c(\"center\", \"scale\"))\n",
    "credit[,num_cols] = predict(preProcValues, credit[,num_cols])\n",
    "head(credit[,num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below uses the capability of the R Caret package to estimate the best hyperparameters using 10 fold cross validation with 5 repeats. There are a few points to note here:\n",
    "1. Since there is significant class imbalance, recall is used as the metric to optimize. A function, `recallSummary` is used to define this metric. This function is specified in the `trainControl` object.\n",
    "2. The model is trained using all features as can be seen from the model formula in the Caret `train` function. \n",
    "3. The recall metric is specified as a `metric` in the call to `train`. \n",
    "4. The `train` function automatically generates and searches a hyperparameter grid and prints the metrics. \n",
    "\n",
    "Execute this code, examine the result, and answer **Question 3** on the course page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stochastic Gradient Boosting \n",
       "\n",
       "999 samples\n",
       " 15 predictor\n",
       "  2 classes: '0', '1' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold, repeated 5 times) \n",
       "Summary of sample sizes: 899, 899, 899, 899, 899, 899, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  interaction.depth  n.trees  recall   \n",
       "  1                   50      0.7308754\n",
       "  1                  100      0.7557016\n",
       "  1                  150      0.7692324\n",
       "  2                   50      0.7552704\n",
       "  2                  100      0.7753978\n",
       "  2                  150      0.7843348\n",
       "  3                   50      0.7703260\n",
       "  3                  100      0.7850413\n",
       "  3                  150      0.7886973\n",
       "\n",
       "Tuning parameter 'shrinkage' was held constant at a value of 0.1\n",
       "\n",
       "Tuning parameter 'n.minobsinnode' was held constant at a value of 10\n",
       "recall was used to select the optimal model using the largest value.\n",
       "The final values used for the model were n.trees = 150, interaction.depth =\n",
       " 3, shrinkage = 0.1 and n.minobsinnode = 10."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "recallSummary = function (data, lev = NULL, model = NULL) {\n",
    "                    out = recall(data$obs, data$pred)  \n",
    "                    names(out) <- \"recall\"\n",
    "                    out\n",
    "}\n",
    "\n",
    "fitControl = trainControl(method = \"repeatedcv\",\n",
    "                           number = 10,\n",
    "                           repeats = 5,\n",
    "                           summaryFunction = recallSummary)\n",
    "\n",
    "set.seed(3344)\n",
    "gbm_fit <- train(factor(bad_credit) ~ .,  # make label a factor since classification model\n",
    "                 data = credit, \n",
    "                 metric = \"recall\", \n",
    "                 method = \"gbm\", # Gradient boosted tree model\n",
    "                 trControl = fitControl,\n",
    "                 verbose = FALSE)\n",
    "gbm_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grid of hyperpameters searched by the Caret package are `interaction.depth` and `n.trees`. The grid along with the recall metric is shown in the printed table. \n",
    "\n",
    "Given the optimal hyperparameters, which features are the most important? The code in the cell below computes and displays feature importance using the Caret `varImp` function. Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbm variable importance\n",
      "\n",
      "  only 20 most important variables shown (out of 31)\n",
      "\n",
      "                                                      Overall\n",
      "loan_amount                                           100.000\n",
      "checking_account_statusnone                            70.813\n",
      "loan_duration_mo                                       68.453\n",
      "age_yrs                                                63.839\n",
      "credit_historycritical account - other non-bank loans  20.781\n",
      "time_in_residence                                      18.689\n",
      "other_credit_outstandingnone                           18.609\n",
      "savings_account_balanceunknown/none                    15.446\n",
      "payment_pcnt_income                                    15.159\n",
      "checking_account_status> 200 DM or salary assignment   12.450\n",
      "propertyreal estate                                    11.354\n",
      "propertyunknown-none                                   11.189\n",
      "checking_account_status0 - 200 DM                      10.964\n",
      "telephoneyes                                           10.310\n",
      "credit_historyno credit - paid                          8.907\n",
      "time_employed_yrs4 - 7 years                            7.561\n",
      "dependents                                              7.313\n",
      "time_employed_yrs>= 7 years                             5.586\n",
      "time_employed_yrsunemployed                             5.117\n",
      "credit_historycurrent loans paid                        5.043\n"
     ]
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA8AAAALQCAMAAABoqemGAAAAM1BMVEUAAAAAgP9NTU1oaGh8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\noGEQGKBhEBigYRAYoGH+H3yXUrzawy3KAAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "options(repr.plot.width=8, repr.plot.height=6)\n",
    "var_imp = varImp(gbm_fit)\n",
    "print(var_imp)\n",
    "plot(var_imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In is clear that some of the features are not important to model performance. Execute the code in the cell below to prune the feature set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_reduced = credit[,c('loan_amount', 'age_yrs', 'loan_duration_mo', 'checking_account_status', 'time_in_residence',\n",
    "                          'payment_pcnt_income', 'other_credit_outstanding', \n",
    "                          'credit_history', 'time_employed_yrs', 'property', 'number_loans', 'telephone',\n",
    "                          'bad_credit')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the code in the cell below to perform the cross validation grid search using the reduced feature set: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stochastic Gradient Boosting \n",
       "\n",
       "999 samples\n",
       " 12 predictor\n",
       "  2 classes: '0', '1' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold, repeated 5 times) \n",
       "Summary of sample sizes: 899, 899, 899, 899, 899, 899, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  interaction.depth  n.trees  recall   \n",
       "  1                   50      0.7305414\n",
       "  1                  100      0.7529063\n",
       "  1                  150      0.7653075\n",
       "  2                   50      0.7551534\n",
       "  2                  100      0.7763280\n",
       "  2                  150      0.7851185\n",
       "  3                   50      0.7705637\n",
       "  3                  100      0.7827386\n",
       "  3                  150      0.7887009\n",
       "\n",
       "Tuning parameter 'shrinkage' was held constant at a value of 0.1\n",
       "\n",
       "Tuning parameter 'n.minobsinnode' was held constant at a value of 10\n",
       "recall was used to select the optimal model using the largest value.\n",
       "The final values used for the model were n.trees = 150, interaction.depth =\n",
       " 3, shrinkage = 0.1 and n.minobsinnode = 10."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set.seed(3344)\n",
    "gbm_fit <- train(factor(bad_credit) ~ .,  # make label a factor since classification model\n",
    "                 data = credit_reduced, \n",
    "                 metric = \"recall\", \n",
    "                 method = \"gbm\", # Gradient boosted tree model\n",
    "                 trControl = fitControl,\n",
    "                 verbose = FALSE)\n",
    "gbm_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the cross validation grid search with the reduced feature set are nearly the same as the first result. Evidentially, pruning these features was the correct step. This process can be continued, but will not be in this lab in the interest of reducing length. \n",
    "\n",
    "As a test, execute the code in the cell below to display the variable importance for the reduced feature set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbm variable importance\n",
      "\n",
      "  only 20 most important variables shown (out of 23)\n",
      "\n",
      "                                                      Overall\n",
      "loan_amount                                           100.000\n",
      "checking_account_statusnone                            70.042\n",
      "age_yrs                                                67.320\n",
      "loan_duration_mo                                       63.117\n",
      "credit_historycritical account - other non-bank loans  20.501\n",
      "other_credit_outstandingnone                           18.773\n",
      "time_in_residence                                      14.403\n",
      "payment_pcnt_income                                    13.440\n",
      "checking_account_status0 - 200 DM                      13.269\n",
      "propertyreal estate                                    11.596\n",
      "checking_account_status> 200 DM or salary assignment   10.786\n",
      "propertyunknown-none                                    9.680\n",
      "credit_historyno credit - paid                          9.268\n",
      "telephoneyes                                            8.203\n",
      "time_employed_yrs4 - 7 years                            6.805\n",
      "time_employed_yrs>= 7 years                             5.660\n",
      "credit_historypast payment delays                       5.327\n",
      "time_employed_yrsunemployed                             5.199\n",
      "credit_historycurrent loans paid                        4.121\n",
      "number_loans                                            3.337\n"
     ]
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA8AAAALQCAMAAABoqemGAAAAM1BMVEUAAAAAgP9NTU1oaGh8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\nl8fTjkUAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "var_imp = varImp(gbm_fit)\n",
    "print(var_imp)\n",
    "plot(var_imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you will run the code in the cell below to perform the outer cross validation of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbm(formula = factor(bad_credit) ~ ., distribution = \"bernoulli\", \n",
      "    data = training, n.trees = 150, interaction.depth = 3)\n",
      "A gradient boosted model with bernoulli loss function.\n",
      "150 iterations were performed.\n",
      "There were 12 predictors of which 0 had non-zero influence.\n",
      "  [1] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      " [19] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      " [37] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      " [55] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      " [73] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      " [91] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "  [1] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      " [19] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      " [37] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      " [55] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      " [73] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      " [91] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "  [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      " [26] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      " [51] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      " [76] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[101] NA NA NA NA NA NA NA NA\n",
      "  [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      " [26] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      " [51] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      " [76] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[101] NA NA NA NA NA NA NA NA\n",
      "[1] 108\n",
      "[1] 108\n",
      "[1] 108\n",
      "[1] 108\n",
      "[1] NaN\n",
      "gbm(formula = factor(bad_credit) ~ ., distribution = \"bernoulli\", \n",
      "    data = training, n.trees = 150, interaction.depth = 3)\n",
      "A gradient boosted model with bernoulli loss function.\n",
      "150 iterations were performed.\n",
      "There were 12 predictors of which 0 had non-zero influence.\n",
      " [1] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[20] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[39] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[58] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[77] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[96] NaN NaN NaN NaN\n",
      " [1] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[20] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[39] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[58] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[77] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[96] NaN NaN NaN NaN\n",
      " [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[26] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[51] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[76] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      " [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[26] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[51] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[76] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[1] 99\n",
      "[1] 99\n",
      "[1] 99\n",
      "[1] 99\n",
      "[1] NaN\n",
      "gbm(formula = factor(bad_credit) ~ ., distribution = \"bernoulli\", \n",
      "    data = training, n.trees = 150, interaction.depth = 3)\n",
      "A gradient boosted model with bernoulli loss function.\n",
      "150 iterations were performed.\n",
      "There were 12 predictors of which 0 had non-zero influence.\n",
      " [1] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[20] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[39] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[58] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[77] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[96] NaN NaN NaN NaN\n",
      " [1] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[20] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[39] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[58] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[77] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[96] NaN NaN NaN NaN\n",
      " [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[26] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[51] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[76] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      " [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[26] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[51] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[76] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[1] 99\n",
      "[1] 99\n",
      "[1] 99\n",
      "[1] 99\n",
      "[1] NaN\n",
      "gbm(formula = factor(bad_credit) ~ ., distribution = \"bernoulli\", \n",
      "    data = training, n.trees = 150, interaction.depth = 3)\n",
      "A gradient boosted model with bernoulli loss function.\n",
      "150 iterations were performed.\n",
      "There were 12 predictors of which 0 had non-zero influence.\n",
      " [1] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[20] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[39] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[58] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[77] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[96] NaN NaN NaN NaN\n",
      " [1] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[20] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[39] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[58] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[77] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[96] NaN NaN NaN NaN\n",
      " [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[26] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[51] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[76] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      " [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[26] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[51] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[76] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[1] 99\n",
      "[1] 99\n",
      "[1] 99\n",
      "[1] 99\n",
      "[1] NaN\n",
      "gbm(formula = factor(bad_credit) ~ ., distribution = \"bernoulli\", \n",
      "    data = training, n.trees = 150, interaction.depth = 3)\n",
      "A gradient boosted model with bernoulli loss function.\n",
      "150 iterations were performed.\n",
      "There were 12 predictors of which 0 had non-zero influence.\n",
      " [1] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[20] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[39] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[58] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[77] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[96] NaN NaN NaN NaN\n",
      " [1] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[20] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[39] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[58] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[77] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[96] NaN NaN NaN NaN\n",
      " [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[26] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[51] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[76] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      " [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[26] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[51] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[76] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[1] 99\n",
      "[1] 99\n",
      "[1] 99\n",
      "[1] 99\n",
      "[1] NaN\n",
      "gbm(formula = factor(bad_credit) ~ ., distribution = \"bernoulli\", \n",
      "    data = training, n.trees = 150, interaction.depth = 3)\n",
      "A gradient boosted model with bernoulli loss function.\n",
      "150 iterations were performed.\n",
      "There were 12 predictors of which 0 had non-zero influence.\n",
      " [1] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[20] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[39] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[58] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[77] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[96] NaN NaN NaN NaN\n",
      " [1] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[20] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[39] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[58] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[77] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[96] NaN NaN NaN NaN\n",
      " [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[26] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[51] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[76] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      " [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[26] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[51] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[76] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[1] 99\n",
      "[1] 99\n",
      "[1] 99\n",
      "[1] 99\n",
      "[1] NaN\n",
      "gbm(formula = factor(bad_credit) ~ ., distribution = \"bernoulli\", \n",
      "    data = training, n.trees = 150, interaction.depth = 3)\n",
      "A gradient boosted model with bernoulli loss function.\n",
      "150 iterations were performed.\n",
      "There were 12 predictors of which 0 had non-zero influence.\n",
      " [1] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[20] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[39] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[58] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[77] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[96] NaN NaN NaN NaN\n",
      " [1] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[20] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[39] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[58] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[77] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[96] NaN NaN NaN NaN\n",
      " [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[26] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[51] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[76] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      " [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[26] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[51] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[76] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[1] 99\n",
      "[1] 99\n",
      "[1] 99\n",
      "[1] 99\n",
      "[1] NaN\n",
      "gbm(formula = factor(bad_credit) ~ ., distribution = \"bernoulli\", \n",
      "    data = training, n.trees = 150, interaction.depth = 3)\n",
      "A gradient boosted model with bernoulli loss function.\n",
      "150 iterations were performed.\n",
      "There were 12 predictors of which 0 had non-zero influence.\n",
      " [1] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[20] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[39] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[58] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[77] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[96] NaN NaN NaN NaN\n",
      " [1] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[20] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[39] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[58] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[77] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[96] NaN NaN NaN NaN\n",
      " [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[26] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[51] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[76] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      " [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[26] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[51] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[76] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[1] 99\n",
      "[1] 99\n",
      "[1] 99\n",
      "[1] 99\n",
      "[1] NaN\n",
      "gbm(formula = factor(bad_credit) ~ ., distribution = \"bernoulli\", \n",
      "    data = training, n.trees = 150, interaction.depth = 3)\n",
      "A gradient boosted model with bernoulli loss function.\n",
      "150 iterations were performed.\n",
      "There were 12 predictors of which 0 had non-zero influence.\n",
      " [1] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[20] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[39] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[58] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[77] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[96] NaN NaN NaN NaN\n",
      " [1] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[20] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[39] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[58] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[77] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[96] NaN NaN NaN NaN\n",
      " [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[26] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[51] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[76] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      " [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[26] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[51] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[76] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[1] 99\n",
      "[1] 99\n",
      "[1] 99\n",
      "[1] 99\n",
      "[1] NaN\n",
      "gbm(formula = factor(bad_credit) ~ ., distribution = \"bernoulli\", \n",
      "    data = training, n.trees = 150, interaction.depth = 3)\n",
      "A gradient boosted model with bernoulli loss function.\n",
      "150 iterations were performed.\n",
      "There were 12 predictors of which 0 had non-zero influence.\n",
      " [1] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[20] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[39] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[58] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[77] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[96] NaN NaN NaN NaN\n",
      " [1] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[20] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[39] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[58] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[77] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "[96] NaN NaN NaN NaN\n",
      " [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[26] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[51] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[76] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      " [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[26] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[51] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[76] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n",
      "[1] 99\n",
      "[1] 99\n",
      "[1] 99\n",
      "[1] 99\n",
      "[1] NaN\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>fold</th><th scope=col>accuracy</th><th scope=col>precision</th><th scope=col>recall</th><th scope=col>F1</th><th scope=col>AUC</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>1   </td><td>0.5 </td><td>0.5 </td><td>0.5 </td><td>1   </td><td>10  </td></tr>\n",
       "\t<tr><td>2   </td><td>0.5 </td><td>0.5 </td><td>0.5 </td><td>1   </td><td>10  </td></tr>\n",
       "\t<tr><td>3   </td><td>0.5 </td><td>0.5 </td><td>0.5 </td><td>1   </td><td>10  </td></tr>\n",
       "\t<tr><td>4   </td><td>0.5 </td><td>0.5 </td><td>0.5 </td><td>1   </td><td>10  </td></tr>\n",
       "\t<tr><td>5   </td><td>0.5 </td><td>0.5 </td><td>0.5 </td><td>1   </td><td>10  </td></tr>\n",
       "\t<tr><td>6   </td><td>0.5 </td><td>0.5 </td><td>0.5 </td><td>1   </td><td>10  </td></tr>\n",
       "\t<tr><td>7   </td><td>0.5 </td><td>0.5 </td><td>0.5 </td><td>1   </td><td>10  </td></tr>\n",
       "\t<tr><td>8   </td><td>0.5 </td><td>0.5 </td><td>0.5 </td><td>1   </td><td>10  </td></tr>\n",
       "\t<tr><td>9   </td><td>0.5 </td><td>0.5 </td><td>0.5 </td><td>1   </td><td>10  </td></tr>\n",
       "\t<tr><td>10  </td><td>0.5 </td><td>0.5 </td><td>0.5 </td><td>1   </td><td>10  </td></tr>\n",
       "\t<tr><td>Mean</td><td>0.5 </td><td>0.5 </td><td>0.5 </td><td>1   </td><td>10  </td></tr>\n",
       "\t<tr><td>std </td><td>0.0 </td><td>0.0 </td><td>0.0 </td><td>0   </td><td> 0  </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllll}\n",
       " fold & accuracy & precision & recall & F1 & AUC\\\\\n",
       "\\hline\n",
       "\t 1    & 0.5  & 0.5  & 0.5  & 1    & 10  \\\\\n",
       "\t 2    & 0.5  & 0.5  & 0.5  & 1    & 10  \\\\\n",
       "\t 3    & 0.5  & 0.5  & 0.5  & 1    & 10  \\\\\n",
       "\t 4    & 0.5  & 0.5  & 0.5  & 1    & 10  \\\\\n",
       "\t 5    & 0.5  & 0.5  & 0.5  & 1    & 10  \\\\\n",
       "\t 6    & 0.5  & 0.5  & 0.5  & 1    & 10  \\\\\n",
       "\t 7    & 0.5  & 0.5  & 0.5  & 1    & 10  \\\\\n",
       "\t 8    & 0.5  & 0.5  & 0.5  & 1    & 10  \\\\\n",
       "\t 9    & 0.5  & 0.5  & 0.5  & 1    & 10  \\\\\n",
       "\t 10   & 0.5  & 0.5  & 0.5  & 1    & 10  \\\\\n",
       "\t Mean & 0.5  & 0.5  & 0.5  & 1    & 10  \\\\\n",
       "\t std  & 0.0  & 0.0  & 0.0  & 0    &  0  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "fold | accuracy | precision | recall | F1 | AUC | \n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1    | 0.5  | 0.5  | 0.5  | 1    | 10   | \n",
       "| 2    | 0.5  | 0.5  | 0.5  | 1    | 10   | \n",
       "| 3    | 0.5  | 0.5  | 0.5  | 1    | 10   | \n",
       "| 4    | 0.5  | 0.5  | 0.5  | 1    | 10   | \n",
       "| 5    | 0.5  | 0.5  | 0.5  | 1    | 10   | \n",
       "| 6    | 0.5  | 0.5  | 0.5  | 1    | 10   | \n",
       "| 7    | 0.5  | 0.5  | 0.5  | 1    | 10   | \n",
       "| 8    | 0.5  | 0.5  | 0.5  | 1    | 10   | \n",
       "| 9    | 0.5  | 0.5  | 0.5  | 1    | 10   | \n",
       "| 10   | 0.5  | 0.5  | 0.5  | 1    | 10   | \n",
       "| Mean | 0.5  | 0.5  | 0.5  | 1    | 10   | \n",
       "| std  | 0.0  | 0.0  | 0.0  | 0    |  0   | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "   fold accuracy precision recall F1 AUC\n",
       "1  1    0.5      0.5       0.5    1  10 \n",
       "2  2    0.5      0.5       0.5    1  10 \n",
       "3  3    0.5      0.5       0.5    1  10 \n",
       "4  4    0.5      0.5       0.5    1  10 \n",
       "5  5    0.5      0.5       0.5    1  10 \n",
       "6  6    0.5      0.5       0.5    1  10 \n",
       "7  7    0.5      0.5       0.5    1  10 \n",
       "8  8    0.5      0.5       0.5    1  10 \n",
       "9  9    0.5      0.5       0.5    1  10 \n",
       "10 10   0.5      0.5       0.5    1  10 \n",
       "11 Mean 0.5      0.5       0.5    1  10 \n",
       "12 std  0.0      0.0       0.0    0   0 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "score_model = function(df, threshold){\n",
    "    print(df$probs)\n",
    "    df$score = ifelse(df$probs > threshold, 1, 0)\n",
    "    df\n",
    "}\n",
    "\n",
    "binary.eval <- function(df, fold){ \n",
    "  # First step is to find the TP, FP, TN, FN cases\n",
    "    print(df$score)\n",
    "  df$conf = ifelse(df$bad_credit == 1 & df$score == 1, 'TP',\n",
    "                    ifelse(df$bad_credit == 0 & df$score == 1, 'FP',\n",
    "                           ifelse(df$bad_credit == 0 & df$score == 0, 'TN', 'FN')))\n",
    "  print(df$conf)\n",
    "  # Elements of the confusion matrix\n",
    "  TP = length(df[df$conf == 'TP', 'conf'])\n",
    "  FP = length(df[df$conf == 'FP', 'conf'])\n",
    "  TN = length(df[df$conf == 'TN', 'conf'])\n",
    "  FN = length(df[df$conf == 'FN', 'conf'])\n",
    "  \n",
    "    print(TP)\n",
    "    print(FP)\n",
    "    print(TN)\n",
    "    print(FN)\n",
    "    \n",
    "  ## Confusion matrix as data frame\n",
    "  out = data.frame(Negative = c(TN, FN), Positive = c(FP, TP))\n",
    "  row.names(out) = c('TrueNeg', 'TruePos')\n",
    "    \n",
    "  # Compute AUC with ROCR package\n",
    "    print(unique(df$probs))\n",
    "#  pred_obj = prediction(df$score, df$bad_credit)\n",
    "#  AUC = performance(pred_obj,\"auc\")@y.values[[1]]  \n",
    "  \n",
    "  # Compute and print metrics\n",
    "  P = TP/(TP + FP)\n",
    "  R = TP/(TP + FN)  \n",
    "  F1 = 2*P/(P+R) \n",
    "  data.frame = data.frame(fold = as.character(fold),\n",
    "                          accuracy = (TP + TN)/(TP + TN + FP + FN),\n",
    "                          precision = P,\n",
    "                          recall = R,\n",
    "                          F1 = F1,\n",
    "                          AUC = 10.0)\n",
    " }\n",
    "\n",
    "\n",
    "Create_Folds = function(df, folds){\n",
    "    ## Create a vector of the fold assignments\n",
    "    nrows = nrow(df)\n",
    "    ncount = nrows/folds\n",
    "    ## Concatenate vectors of fold number\n",
    "    fold = rep(1, ncount)\n",
    "    for(i in seq(2, folds, by = 1)){\n",
    "        fold = c(fold, rep(i, ncount))\n",
    "    }\n",
    "    fold\n",
    "}\n",
    "\n",
    "Fit_Mod = function(training, test){\n",
    "    set.seed(5566)\n",
    "    gbm_mod = gbm(factor(bad_credit) ~ ., data = training, \n",
    "                   interaction.depth = 3, n.trees = 150,\n",
    "                   distribution = \"bernoulli\")\n",
    "    print(gbm_mod)\n",
    "    test$probs = predict(gbm_mod, newdata = test, #type = 'response', \n",
    "                         n.trees = 150)\n",
    "    print(test$probs)\n",
    "    test = score_model(test, 0.5)\n",
    "    test\n",
    "}\n",
    "\n",
    "Cross_Validate_Mod = function(df, folds){\n",
    "    ## Create a vector of the fold assignments\n",
    "    fold = Create_Folds(df, folds)\n",
    "    \n",
    "    ## Randomly shuffle the rows of the data frame\n",
    "    shuffle = sample(seq(1, nrow(df), by = 1))\n",
    "    df = df[shuffle,]\n",
    "    \n",
    "    ## Loop over number of folds to fit and evaluate the model\n",
    "    training = df[fold != 1,]\n",
    "    test = df[fold == 1, ]\n",
    "    test = Fit_Mod(training, test)\n",
    "    evals = binary.eval(test, 1)\n",
    "    for(i in seq(2, folds, by = 1)){\n",
    "        training = df[fold != i,]\n",
    "        test = df[fold == i, ]\n",
    "        test = Fit_Mod(training, test)\n",
    "        evals = rbind(evals, binary.eval(test, i))\n",
    "    }\n",
    "    \n",
    "    ## Compute some summary statistics and append to the data rame\n",
    "    evals = rbind(evals, data.frame(fold = 'Mean',\n",
    "                          accuracy = mean(evals[,2]),\n",
    "                          precision = mean(evals[,3]),\n",
    "                          recall = mean(evals[,4]),\n",
    "                          F1 = mean(evals[,5]),\n",
    "                          AUC = mean(evals[,6])))\n",
    "    \n",
    "    evals = rbind(evals, data.frame(fold = 'std',\n",
    "                          accuracy = sd(evals[,2]),\n",
    "                          precision = sd(evals[,3]),\n",
    "                          recall = sd(evals[,4]),\n",
    "                          F1 = sd(evals[,5]),\n",
    "                          AUC = sd(evals[,6])))\n",
    "    evals\n",
    "}\n",
    "\n",
    "Cross_Validate_Mod(credit_reduced, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine these results. Notice that the standard deviation of the mean of the AUC is more than an order of magnitude smaller than the mean. This indicates that this model is likely to generalize well. \n",
    "\n",
    "Now, you will build and test a model using the estimated optimal hyperparameters. As a first step, execute the code in the cell below to create training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Randomly sample cases to create independent training and test data\n",
    "nr.seed(1115)\n",
    "indx = range(Features.shape[0])\n",
    "indx = ms.train_test_split(indx, test_size = 300)\n",
    "x_train = Features[indx[0],:]\n",
    "y_train = np.ravel(Labels[indx[0]])\n",
    "x_test = Features[indx[1],:]\n",
    "y_test = np.ravel(Labels[indx[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below defines an AdaBoosted tree model object using the estimated optimal model hyperparameters and then fits the model to the training data. Execute this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1,\n",
       "          n_estimators=100, random_state=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nr.seed(1115)\n",
    "ab_mod = AdaBoostClassifier(n_estimators=100, learning_rate = 1) \n",
    "ab_mod.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the hyperparameters of the AdaBoosted tree model object reflect those specified. \n",
    "\n",
    "The code in the cell below scores and prints evaluation metrics for the model, using the test data subset. Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Confusion matrix\n",
      "                 Score positive    Score negative\n",
      "True positive       188             29\n",
      "True negative        46             37\n",
      "\n",
      "Accuracy        0.75\n",
      "AUC             0.66\n",
      "Macro precision 0.68\n",
      "Macro recall    0.66\n",
      " \n",
      "             Positive   Negative\n",
      "Num case    217.00       83.00\n",
      "Precision   0.80          0.56\n",
      "Recall      0.87          0.45\n",
      "F1          0.83          0.50\n"
     ]
    }
   ],
   "source": [
    "def print_metrics(labels, scores):\n",
    "    metrics = sklm.precision_recall_fscore_support(labels, scores)\n",
    "    conf = sklm.confusion_matrix(labels, scores)\n",
    "    print('                 Confusion matrix')\n",
    "    print('                 Score positive    Score negative')\n",
    "    print('True positive    %6d' % conf[0,0] + '          %5d' % conf[0,1])\n",
    "    print('True negative    %6d' % conf[1,0] + '          %5d' % conf[1,1])\n",
    "    print('')\n",
    "    print('Accuracy        %0.2f' % sklm.accuracy_score(labels, scores))\n",
    "    print('AUC             %0.2f' % sklm.roc_auc_score(labels, scores))\n",
    "    print('Macro precision %0.2f' % float((float(metrics[0][0]) + float(metrics[0][1]))/2.0))\n",
    "    print('Macro recall    %0.2f' % float((float(metrics[1][0]) + float(metrics[1][1]))/2.0))\n",
    "    print(' ')\n",
    "    print('             Positive   Negative')\n",
    "    print('Num case    %0.2f' % metrics[3][0] + '       %0.2f' % metrics[3][1])\n",
    "    print('Precision   %0.2f' % metrics[0][0] + '          %0.2f' % metrics[0][1])\n",
    "    print('Recall      %0.2f' % metrics[1][0] + '          %0.2f' % metrics[1][1])\n",
    "    print('F1          %0.2f' % metrics[2][0] + '          %0.2f' % metrics[2][1])\n",
    "\n",
    "scores = ab_mod.predict(x_test)\n",
    "print_metrics(y_test, scores)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, these performance metrics are poor. The majority of negative cases have been misclassified as positive. Adaboosted methods are sensitive to class imbalance. \n",
    "\n",
    "It is likely that the poor performance arises from the class imbalance. Notice that there is no way to reweight classes with boosting methods. Some alternatives are:\n",
    "1. **Impute** new values using a statistical algorithm. \n",
    "2. **Undersample** the majority cases. For this method a number of the cases equal to the minority case are Bernoulli sampled from the majority case. \n",
    "3. **Oversample** the minority cases. For this method the number of minority cases are resampled until they equal the number of majority cases.\n",
    "\n",
    "The code in the cell below undersamples the majority cases; good credit customers. The `choice` function from the numpy.random package is used to randomize the undersampling. The count of unique label values and the shape of the resulting arrays is printed. Execute this code to create a data set with balanced cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300 300]\n",
      "(600, 35)\n",
      "(600,)\n"
     ]
    }
   ],
   "source": [
    "temp_Labels_1 = Labels[Labels == 1]  # Save these\n",
    "temp_Features_1 = Features[Labels == 1,:] # Save these\n",
    "temp_Labels_0 = Labels[Labels == 0]  # Undersample these\n",
    "temp_Features_0 = Features[Labels == 0,:] # Undersample these\n",
    "\n",
    "indx = nr.choice(temp_Features_0.shape[0], temp_Features_1.shape[0], replace=True)\n",
    "\n",
    "temp_Features = np.concatenate((temp_Features_1, temp_Features_0[indx,:]), axis = 0)\n",
    "temp_Labels = np.concatenate((temp_Labels_1, temp_Labels_0[indx,]), axis = 0) \n",
    "\n",
    "print(np.bincount(temp_Labels))\n",
    "print(temp_Features.shape)\n",
    "print(temp_Labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are now 300 of each label case with 600 cases overall. The question is, will using these data produce better results?\n",
    "\n",
    "You will perform model selection and evaluation using nested cross validation. The code in the cell below finds the optimal learning rate parameter using cross validation. \n",
    "\n",
    "Once you have executed the code, answer **Question 3** on the course page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    }
   ],
   "source": [
    "nr.seed(1234)\n",
    "inside = ms.KFold(n_splits=10, shuffle = True)\n",
    "nr.seed(3214)\n",
    "outside = ms.KFold(n_splits=10, shuffle = True)\n",
    "\n",
    "## Define the AdaBoosted tree model\n",
    "nr.seed(3456)\n",
    "ab_clf = AdaBoostClassifier(n_estimators=100)  \n",
    "\n",
    "## Perform the grid search over the parameters\n",
    "nr.seed(4455)\n",
    "ab_clf = ms.GridSearchCV(estimator = ab_clf, param_grid = param_grid, \n",
    "                      cv = inside, # Use the inside folds\n",
    "                      scoring = sklm.make_scorer(sklm.roc_auc_score),\n",
    "                      return_train_score = True)\n",
    "ab_clf.fit(temp_Features, temp_Labels)\n",
    "print(ab_clf.best_estimator_.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the estimated optimal learning rate parameter is smaller than before.\n",
    "\n",
    "Now, run the code in the cell below to execute the outer loop of the cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean performance metric = 0.712\n",
      "SDT of the metric       = 0.041\n",
      "Outcomes by cv fold\n",
      "fold 1   0.684\n",
      "fold 2   0.700\n",
      "fold 3   0.632\n",
      "fold 4   0.714\n",
      "fold 5   0.792\n",
      "fold 6   0.716\n",
      "fold 7   0.692\n",
      "fold 8   0.733\n",
      "fold 9   0.754\n",
      "fold 10   0.703\n"
     ]
    }
   ],
   "source": [
    "nr.seed(498)\n",
    "cv_estimate = ms.cross_val_score(ab_clf, temp_Features, temp_Labels, \n",
    "                                 cv = outside) # Use the outside folds\n",
    "print('Mean performance metric = %4.3f' % np.mean(cv_estimate))\n",
    "\n",
    "print('SDT of the metric       = %4.3f' % np.std(cv_estimate))\n",
    "print('Outcomes by cv fold')\n",
    "for i, x in enumerate(cv_estimate):\n",
    "    print('fold ' + str(i+1) + '   %4.3f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average AUC is improved compared to the imbalanced training cases. However, the differences are just within 1 standard deviation. Still, there is a reasonable chance the new results represent an improvement. \n",
    "\n",
    "Finally, you will train and evaluate a model with the balanced cases and the update hyperparameter. The code in the cell below does the following processing:\n",
    "1. Creates Bernoulli sampled test and training subsets. \n",
    "2. Defines an AdaBoosted model.\n",
    "3. Trains the AdaBoosted model.\n",
    "\n",
    "Execute this code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=0.1, n_estimators=100, random_state=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Randomly sample cases to create independent training and test data\n",
    "nr.seed(1115)\n",
    "indx = range(temp_Features.shape[0])\n",
    "indx = ms.train_test_split(indx, test_size = 200)\n",
    "x_train = temp_Features[indx[0],:]\n",
    "y_train = np.ravel(temp_Labels[indx[0]])\n",
    "x_test = temp_Features[indx[1],:]\n",
    "y_test = np.ravel(temp_Labels[indx[1]])\n",
    "\n",
    "## Define and fit the model\n",
    "nr.seed(1115)\n",
    "ab_mod = AdaBoostClassifier(n_estimators=100, learning_rate = 0.1) \n",
    "ab_mod.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, execute the code in the cell below to score and evaluate the model.\n",
    "\n",
    "Once you have executed the code, answer **Question 4** on the course page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Confusion matrix\n",
      "                 Score positive    Score negative\n",
      "True positive        66             27\n",
      "True negative        20             87\n",
      "\n",
      "Accuracy        0.77\n",
      "AUC             0.76\n",
      "Macro precision 0.77\n",
      "Macro recall    0.76\n",
      " \n",
      "             Positive   Negative\n",
      "Num case    93.00       107.00\n",
      "Precision   0.77          0.76\n",
      "Recall      0.71          0.81\n",
      "F1          0.74          0.79\n"
     ]
    }
   ],
   "source": [
    "scores = ab_clf.predict(x_test)\n",
    "print_metrics(y_test, scores) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are significantly better than those obtained with the imbalanced training data. However, there are at least two reasons to be cautious:\n",
    "1. The reported AUC is nearly 1 standard deviation above the average obtained with cross validation. \n",
    "2. The test cases have balanced cases, which will not be true in production. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab you have accomplished the following:\n",
    "1. Used an AdaBoosted tree model to classify the cases of the iris data. This model produced quite good results.\n",
    "2. Applied feature importance was used for feature selection with the iris data. The model created and evaluated with the reduced feature set had essentially the same performance as the model with more features.  \n",
    "3. Used 10 fold to find estimated optimal hyperparameters for an AdaBoosted tree model to classify credit risk cases. The model did not generalize well.\n",
    "4. Applied undersampling of the majority cases to create a balanced training dataset and retrained and evaluated the model. The model created with balanced training data was significantly better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
