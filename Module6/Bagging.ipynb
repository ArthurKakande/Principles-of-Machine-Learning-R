{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging and Random Forest Models\n",
    "\n",
    "Using **ensemble methods** can greatly improve the results achieved with weak machine learning algorithms, also called **weak learners**. Ensemble methods achieve better performance by aggregating the results of many statistically independent models. This process averages out the errors and produces a final better prediction. \n",
    "\n",
    "In this lab you will work with a widely used ensemble method known as **bootstrap aggregating** or simply **bagging**. Bagging follows a simple procedure:\n",
    "1. N learners (machine learning models) are defined. \n",
    "2. N subsamples of the training data are created by **Bernoulli sampling with replacement**.\n",
    "3. The N learners are trained on the subsamples of the training data.\n",
    "4. The ensemble is scored by averaging, or taking a majority vote, of the predictions from the N learners.\n",
    "\n",
    "**Classification and regression tree models** are most typically used with bagging methods. The most common such algorithm is know as the **random forest**. The random forest method is highly scalable and generally produces good results, even for complex problems. \n",
    "\n",
    "Classification and regression trees tend to be robust to noise or outliers in the training data. This is true for the random forest algorithm as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Iris dataset\n",
    "\n",
    "As a first example you will use random forest to classify the species of iris flowers. \n",
    "\n",
    "As a first step, execute the code in the cell below to load the required packages to run the rest of this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: 'dplyr'\n",
      "\n",
      "The following object is masked from 'package:gridExtra':\n",
      "\n",
      "    combine\n",
      "\n",
      "The following objects are masked from 'package:stats':\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "The following objects are masked from 'package:base':\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "Loading required package: lattice\n",
      "randomForest 4.6-12\n",
      "Type rfNews() to see new features/changes/bug fixes.\n",
      "\n",
      "Attaching package: 'randomForest'\n",
      "\n",
      "The following object is masked from 'package:dplyr':\n",
      "\n",
      "    combine\n",
      "\n",
      "The following object is masked from 'package:gridExtra':\n",
      "\n",
      "    combine\n",
      "\n",
      "The following object is masked from 'package:ggplot2':\n",
      "\n",
      "    margin\n",
      "\n",
      "Warning message:\n",
      "\"package 'ROCR' was built under R version 3.4.4\"Loading required package: gplots\n",
      "Warning message:\n",
      "\"package 'gplots' was built under R version 3.4.4\"\n",
      "Attaching package: 'gplots'\n",
      "\n",
      "The following object is masked from 'package:stats':\n",
      "\n",
      "    lowess\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Import packages\n",
    "library(ggplot2)\n",
    "library(gridExtra)\n",
    "library(repr)\n",
    "library(dplyr)\n",
    "library(caret)\n",
    "library(randomForest)\n",
    "library(ROCR)\n",
    "\n",
    "options(repr.plot.width=4, repr.plot.height=4) # Set the initial plot area dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a feel for these data, you will now load and plot them. The code in the cell below does the following:\n",
    "\n",
    "1. Loads the iris data as a Pandas data frame. \n",
    "2. Adds column names to the data frame.\n",
    "3. Displays all 4 possible scatter plot views of the data. \n",
    "\n",
    "Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Sepal.Length</th><th scope=col>Sepal.Width</th><th scope=col>Petal.Length</th><th scope=col>Petal.Width</th><th scope=col>Species</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>5.1   </td><td>3.5   </td><td>1.4   </td><td>0.2   </td><td>setosa</td></tr>\n",
       "\t<tr><td>4.9   </td><td>3.0   </td><td>1.4   </td><td>0.2   </td><td>setosa</td></tr>\n",
       "\t<tr><td>4.7   </td><td>3.2   </td><td>1.3   </td><td>0.2   </td><td>setosa</td></tr>\n",
       "\t<tr><td>4.6   </td><td>3.1   </td><td>1.5   </td><td>0.2   </td><td>setosa</td></tr>\n",
       "\t<tr><td>5.0   </td><td>3.6   </td><td>1.4   </td><td>0.2   </td><td>setosa</td></tr>\n",
       "\t<tr><td>5.4   </td><td>3.9   </td><td>1.7   </td><td>0.4   </td><td>setosa</td></tr>\n",
       "\t<tr><td>4.6   </td><td>3.4   </td><td>1.4   </td><td>0.3   </td><td>setosa</td></tr>\n",
       "\t<tr><td>5.0   </td><td>3.4   </td><td>1.5   </td><td>0.2   </td><td>setosa</td></tr>\n",
       "\t<tr><td>4.4   </td><td>2.9   </td><td>1.4   </td><td>0.2   </td><td>setosa</td></tr>\n",
       "\t<tr><td>4.9   </td><td>3.1   </td><td>1.5   </td><td>0.1   </td><td>setosa</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllll}\n",
       " Sepal.Length & Sepal.Width & Petal.Length & Petal.Width & Species\\\\\n",
       "\\hline\n",
       "\t 5.1    & 3.5    & 1.4    & 0.2    & setosa\\\\\n",
       "\t 4.9    & 3.0    & 1.4    & 0.2    & setosa\\\\\n",
       "\t 4.7    & 3.2    & 1.3    & 0.2    & setosa\\\\\n",
       "\t 4.6    & 3.1    & 1.5    & 0.2    & setosa\\\\\n",
       "\t 5.0    & 3.6    & 1.4    & 0.2    & setosa\\\\\n",
       "\t 5.4    & 3.9    & 1.7    & 0.4    & setosa\\\\\n",
       "\t 4.6    & 3.4    & 1.4    & 0.3    & setosa\\\\\n",
       "\t 5.0    & 3.4    & 1.5    & 0.2    & setosa\\\\\n",
       "\t 4.4    & 2.9    & 1.4    & 0.2    & setosa\\\\\n",
       "\t 4.9    & 3.1    & 1.5    & 0.1    & setosa\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "Sepal.Length | Sepal.Width | Petal.Length | Petal.Width | Species | \n",
       "|---|---|---|---|---|---|---|---|---|---|\n",
       "| 5.1    | 3.5    | 1.4    | 0.2    | setosa | \n",
       "| 4.9    | 3.0    | 1.4    | 0.2    | setosa | \n",
       "| 4.7    | 3.2    | 1.3    | 0.2    | setosa | \n",
       "| 4.6    | 3.1    | 1.5    | 0.2    | setosa | \n",
       "| 5.0    | 3.6    | 1.4    | 0.2    | setosa | \n",
       "| 5.4    | 3.9    | 1.7    | 0.4    | setosa | \n",
       "| 4.6    | 3.4    | 1.4    | 0.3    | setosa | \n",
       "| 5.0    | 3.4    | 1.5    | 0.2    | setosa | \n",
       "| 4.4    | 2.9    | 1.4    | 0.2    | setosa | \n",
       "| 4.9    | 3.1    | 1.5    | 0.1    | setosa | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n",
       "1  5.1          3.5         1.4          0.2         setosa \n",
       "2  4.9          3.0         1.4          0.2         setosa \n",
       "3  4.7          3.2         1.3          0.2         setosa \n",
       "4  4.6          3.1         1.5          0.2         setosa \n",
       "5  5.0          3.6         1.4          0.2         setosa \n",
       "6  5.4          3.9         1.7          0.4         setosa \n",
       "7  4.6          3.4         1.4          0.3         setosa \n",
       "8  5.0          3.4         1.5          0.2         setosa \n",
       "9  4.4          2.9         1.4          0.2         setosa \n",
       "10 4.9          3.1         1.5          0.1         setosa "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA8AAAAJYCAMAAACaSn8zAAABKVBMVEUAAAABuzkEuzsEuzwH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\nZQYb3AAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "single_plot = function(df, colx, coly){\n",
    "    ggplot(df, aes_string(colx,coly)) +\n",
    "          geom_point(aes(color = factor(df$Species)), alpha = 0.4)\n",
    "}\n",
    "\n",
    "plot_iris = function(df){\n",
    "    options(repr.plot.width=8, repr.plot.height=5)\n",
    "    grid.arrange(\n",
    "        single_plot(df, 'Sepal.Length', 'Sepal.Width'),\n",
    "        single_plot(df, 'Sepal.Length', 'Petal.Length'),\n",
    "        single_plot(df, 'Petal.Length', 'Petal.Width'),\n",
    "        single_plot(df, 'Sepal.Width', 'Petal.Length'),\n",
    "        nrow = 2)\n",
    "}\n",
    "\n",
    "head(iris, 10)   \n",
    "plot_iris(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that Setosa (in blue) is well separated from the other two categories. The Versicolor (in orange) and the Virginica (in green) show considerable overlap. The question is how well our classifier will separate these categories. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset\n",
    "\n",
    "You must now create randomly sampled training and test data sets. The `createDataPartition` function from the R caret package is used  to create indices for the training data sample. In this case 75% of the data will be used  for training the model. Since this data set is small only 48 cases will be included in the test dataset. Execute this code and note the dimensions of the resulting data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>75</li>\n",
       "\t<li>5</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 75\n",
       "\\item 5\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 75\n",
       "2. 5\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 75  5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>75</li>\n",
       "\t<li>5</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 75\n",
       "\\item 5\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 75\n",
       "2. 5\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 75  5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set.seed(1955)\n",
    "## Randomly sample cases to create independent training and test data\n",
    "partition = createDataPartition(iris[,'Species'], times = 1, p = 0.50, list = FALSE)\n",
    "training = iris[partition,] # Create the training sample\n",
    "dim(training)\n",
    "test = iris[-partition,] # Create the test sample\n",
    "dim(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale numeric features\n",
    "\n",
    "Numeric features must be rescaled so they have a similar range of values. Rescaling prevents features from having an undue influence on model training simply because then have a larger range of numeric variables. \n",
    "\n",
    "The code in the cell below uses the `preProcess` function from the caret function. The processing is as follows:\n",
    "1. The preprocessing model object is computed. In this case the processing includes centering and scaling the numeric feature. Notice that this model is fit only ot the training data.\n",
    "2. The scalling is appled both the test and training partitions.\n",
    "\n",
    "Execute the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>Sepal.Length</th><th scope=col>Sepal.Width</th><th scope=col>Petal.Length</th><th scope=col>Petal.Width</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>3</th><td>-1.3126284</td><td> 0.4057850</td><td>-1.372230 </td><td>-1.268093 </td></tr>\n",
       "\t<tr><th scope=row>5</th><td>-0.9618666</td><td> 1.3142589</td><td>-1.316478 </td><td>-1.268093 </td></tr>\n",
       "\t<tr><th scope=row>6</th><td>-0.4941843</td><td> 1.9956143</td><td>-1.149224 </td><td>-1.012772 </td></tr>\n",
       "\t<tr><th scope=row>9</th><td>-1.6633901</td><td>-0.2755704</td><td>-1.316478 </td><td>-1.268093 </td></tr>\n",
       "\t<tr><th scope=row>10</th><td>-1.0787872</td><td> 0.1786665</td><td>-1.260727 </td><td>-1.395753 </td></tr>\n",
       "\t<tr><th scope=row>11</th><td>-0.4941843</td><td> 1.5413774</td><td>-1.260727 </td><td>-1.268093 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       "  & Sepal.Length & Sepal.Width & Petal.Length & Petal.Width\\\\\n",
       "\\hline\n",
       "\t3 & -1.3126284 &  0.4057850 & -1.372230  & -1.268093 \\\\\n",
       "\t5 & -0.9618666 &  1.3142589 & -1.316478  & -1.268093 \\\\\n",
       "\t6 & -0.4941843 &  1.9956143 & -1.149224  & -1.012772 \\\\\n",
       "\t9 & -1.6633901 & -0.2755704 & -1.316478  & -1.268093 \\\\\n",
       "\t10 & -1.0787872 &  0.1786665 & -1.260727  & -1.395753 \\\\\n",
       "\t11 & -0.4941843 &  1.5413774 & -1.260727  & -1.268093 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | Sepal.Length | Sepal.Width | Petal.Length | Petal.Width | \n",
       "|---|---|---|---|---|---|\n",
       "| 3 | -1.3126284 |  0.4057850 | -1.372230  | -1.268093  | \n",
       "| 5 | -0.9618666 |  1.3142589 | -1.316478  | -1.268093  | \n",
       "| 6 | -0.4941843 |  1.9956143 | -1.149224  | -1.012772  | \n",
       "| 9 | -1.6633901 | -0.2755704 | -1.316478  | -1.268093  | \n",
       "| 10 | -1.0787872 |  0.1786665 | -1.260727  | -1.395753  | \n",
       "| 11 | -0.4941843 |  1.5413774 | -1.260727  | -1.268093  | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "   Sepal.Length Sepal.Width Petal.Length Petal.Width\n",
       "3  -1.3126284    0.4057850  -1.372230    -1.268093  \n",
       "5  -0.9618666    1.3142589  -1.316478    -1.268093  \n",
       "6  -0.4941843    1.9956143  -1.149224    -1.012772  \n",
       "9  -1.6633901   -0.2755704  -1.316478    -1.268093  \n",
       "10 -1.0787872    0.1786665  -1.260727    -1.395753  \n",
       "11 -0.4941843    1.5413774  -1.260727    -1.268093  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_cols = c('Sepal.Length', 'Sepal.Width', 'Petal.Length', 'Petal.Width')\n",
    "preProcValues <- preProcess(training[,num_cols], method = c(\"center\", \"scale\"))\n",
    "\n",
    "training[,num_cols] = predict(preProcValues, training[,num_cols])\n",
    "test[,num_cols] = predict(preProcValues, test[,num_cols])\n",
    "head(training[,num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will define and fit a random forest model. The code in the cell below defines random forest model with 5 trees using the `RandomForestClassifer` function from the Scikit Learn ensemble  package, and then fits the model. Execute this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(1115)\n",
    "rf_mod = randomForest(Species ~ ., data = training, ntrees = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the many hyperparameters of the random forest model object are displayed. \n",
    "\n",
    "Next, the code in the cell below performs the following processing to score the test data subset:\n",
    "1. The test features are scaled using the scaler computed for the training features. \n",
    "2. The `predict` method is used to compute the scores from the scaled features. \n",
    "\n",
    "Execute this code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Sepal.Length</th><th scope=col>Sepal.Width</th><th scope=col>Petal.Length</th><th scope=col>Petal.Width</th><th scope=col>Species</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>5.1   </td><td>3.5   </td><td>1.4   </td><td>0.2   </td><td>setosa</td></tr>\n",
       "\t<tr><td>4.9   </td><td>3.0   </td><td>1.4   </td><td>0.2   </td><td>setosa</td></tr>\n",
       "\t<tr><td>4.7   </td><td>3.2   </td><td>1.3   </td><td>0.2   </td><td>setosa</td></tr>\n",
       "\t<tr><td>4.6   </td><td>3.1   </td><td>1.5   </td><td>0.2   </td><td>setosa</td></tr>\n",
       "\t<tr><td>5.0   </td><td>3.6   </td><td>1.4   </td><td>0.2   </td><td>setosa</td></tr>\n",
       "\t<tr><td>5.4   </td><td>3.9   </td><td>1.7   </td><td>0.4   </td><td>setosa</td></tr>\n",
       "\t<tr><td>4.6   </td><td>3.4   </td><td>1.4   </td><td>0.3   </td><td>setosa</td></tr>\n",
       "\t<tr><td>5.0   </td><td>3.4   </td><td>1.5   </td><td>0.2   </td><td>setosa</td></tr>\n",
       "\t<tr><td>4.4   </td><td>2.9   </td><td>1.4   </td><td>0.2   </td><td>setosa</td></tr>\n",
       "\t<tr><td>4.9   </td><td>3.1   </td><td>1.5   </td><td>0.1   </td><td>setosa</td></tr>\n",
       "\t<tr><td>5.4   </td><td>3.7   </td><td>1.5   </td><td>0.2   </td><td>setosa</td></tr>\n",
       "\t<tr><td>4.8   </td><td>3.4   </td><td>1.6   </td><td>0.2   </td><td>setosa</td></tr>\n",
       "\t<tr><td>4.8   </td><td>3.0   </td><td>1.4   </td><td>0.1   </td><td>setosa</td></tr>\n",
       "\t<tr><td>4.3   </td><td>3.0   </td><td>1.1   </td><td>0.1   </td><td>setosa</td></tr>\n",
       "\t<tr><td>5.8   </td><td>4.0   </td><td>1.2   </td><td>0.2   </td><td>setosa</td></tr>\n",
       "\t<tr><td>5.7   </td><td>4.4   </td><td>1.5   </td><td>0.4   </td><td>setosa</td></tr>\n",
       "\t<tr><td>5.4   </td><td>3.9   </td><td>1.3   </td><td>0.4   </td><td>setosa</td></tr>\n",
       "\t<tr><td>5.1   </td><td>3.5   </td><td>1.4   </td><td>0.3   </td><td>setosa</td></tr>\n",
       "\t<tr><td>5.7   </td><td>3.8   </td><td>1.7   </td><td>0.3   </td><td>setosa</td></tr>\n",
       "\t<tr><td>5.1   </td><td>3.8   </td><td>1.5   </td><td>0.3   </td><td>setosa</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllll}\n",
       " Sepal.Length & Sepal.Width & Petal.Length & Petal.Width & Species\\\\\n",
       "\\hline\n",
       "\t 5.1    & 3.5    & 1.4    & 0.2    & setosa\\\\\n",
       "\t 4.9    & 3.0    & 1.4    & 0.2    & setosa\\\\\n",
       "\t 4.7    & 3.2    & 1.3    & 0.2    & setosa\\\\\n",
       "\t 4.6    & 3.1    & 1.5    & 0.2    & setosa\\\\\n",
       "\t 5.0    & 3.6    & 1.4    & 0.2    & setosa\\\\\n",
       "\t 5.4    & 3.9    & 1.7    & 0.4    & setosa\\\\\n",
       "\t 4.6    & 3.4    & 1.4    & 0.3    & setosa\\\\\n",
       "\t 5.0    & 3.4    & 1.5    & 0.2    & setosa\\\\\n",
       "\t 4.4    & 2.9    & 1.4    & 0.2    & setosa\\\\\n",
       "\t 4.9    & 3.1    & 1.5    & 0.1    & setosa\\\\\n",
       "\t 5.4    & 3.7    & 1.5    & 0.2    & setosa\\\\\n",
       "\t 4.8    & 3.4    & 1.6    & 0.2    & setosa\\\\\n",
       "\t 4.8    & 3.0    & 1.4    & 0.1    & setosa\\\\\n",
       "\t 4.3    & 3.0    & 1.1    & 0.1    & setosa\\\\\n",
       "\t 5.8    & 4.0    & 1.2    & 0.2    & setosa\\\\\n",
       "\t 5.7    & 4.4    & 1.5    & 0.4    & setosa\\\\\n",
       "\t 5.4    & 3.9    & 1.3    & 0.4    & setosa\\\\\n",
       "\t 5.1    & 3.5    & 1.4    & 0.3    & setosa\\\\\n",
       "\t 5.7    & 3.8    & 1.7    & 0.3    & setosa\\\\\n",
       "\t 5.1    & 3.8    & 1.5    & 0.3    & setosa\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "Sepal.Length | Sepal.Width | Petal.Length | Petal.Width | Species | \n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 5.1    | 3.5    | 1.4    | 0.2    | setosa | \n",
       "| 4.9    | 3.0    | 1.4    | 0.2    | setosa | \n",
       "| 4.7    | 3.2    | 1.3    | 0.2    | setosa | \n",
       "| 4.6    | 3.1    | 1.5    | 0.2    | setosa | \n",
       "| 5.0    | 3.6    | 1.4    | 0.2    | setosa | \n",
       "| 5.4    | 3.9    | 1.7    | 0.4    | setosa | \n",
       "| 4.6    | 3.4    | 1.4    | 0.3    | setosa | \n",
       "| 5.0    | 3.4    | 1.5    | 0.2    | setosa | \n",
       "| 4.4    | 2.9    | 1.4    | 0.2    | setosa | \n",
       "| 4.9    | 3.1    | 1.5    | 0.1    | setosa | \n",
       "| 5.4    | 3.7    | 1.5    | 0.2    | setosa | \n",
       "| 4.8    | 3.4    | 1.6    | 0.2    | setosa | \n",
       "| 4.8    | 3.0    | 1.4    | 0.1    | setosa | \n",
       "| 4.3    | 3.0    | 1.1    | 0.1    | setosa | \n",
       "| 5.8    | 4.0    | 1.2    | 0.2    | setosa | \n",
       "| 5.7    | 4.4    | 1.5    | 0.4    | setosa | \n",
       "| 5.4    | 3.9    | 1.3    | 0.4    | setosa | \n",
       "| 5.1    | 3.5    | 1.4    | 0.3    | setosa | \n",
       "| 5.7    | 3.8    | 1.7    | 0.3    | setosa | \n",
       "| 5.1    | 3.8    | 1.5    | 0.3    | setosa | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n",
       "1  5.1          3.5         1.4          0.2         setosa \n",
       "2  4.9          3.0         1.4          0.2         setosa \n",
       "3  4.7          3.2         1.3          0.2         setosa \n",
       "4  4.6          3.1         1.5          0.2         setosa \n",
       "5  5.0          3.6         1.4          0.2         setosa \n",
       "6  5.4          3.9         1.7          0.4         setosa \n",
       "7  4.6          3.4         1.4          0.3         setosa \n",
       "8  5.0          3.4         1.5          0.2         setosa \n",
       "9  4.4          2.9         1.4          0.2         setosa \n",
       "10 4.9          3.1         1.5          0.1         setosa \n",
       "11 5.4          3.7         1.5          0.2         setosa \n",
       "12 4.8          3.4         1.6          0.2         setosa \n",
       "13 4.8          3.0         1.4          0.1         setosa \n",
       "14 4.3          3.0         1.1          0.1         setosa \n",
       "15 5.8          4.0         1.2          0.2         setosa \n",
       "16 5.7          4.4         1.5          0.4         setosa \n",
       "17 5.4          3.9         1.3          0.4         setosa \n",
       "18 5.1          3.5         1.4          0.3         setosa \n",
       "19 5.7          3.8         1.7          0.3         setosa \n",
       "20 5.1          3.8         1.5          0.3         setosa "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test$scores = predict(rf_mod, newdata = test)\n",
    "head(iris, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to evaluate the model results. Keep in mind that the problem has been made difficult deliberately, by having more test cases than training cases. \n",
    "\n",
    "The iris data has three species categories. Therefore it is necessary to use evaluation code for a three category problem. The function in the cell below extends code from pervious labs to deal with a three category problem. Execute this code and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Predicted\n",
      "Actual       setosa versicolor virginica\n",
      "  setosa         25          0         0\n",
      "  versicolor      0         23         2\n",
      "  virginica       0          1        24\n",
      "\n",
      "Accuracy =  0.96 \n",
      " \n",
      "          setosa versicolor virginica\n",
      "Precision      1      0.920     0.960\n",
      "Recall         1      0.958     0.923\n",
      "F1             1      0.939     0.941\n"
     ]
    }
   ],
   "source": [
    "print_metrics = function(df, label){\n",
    "    ## Compute and print the confusion matrix\n",
    "    cm = as.matrix(table(Actual = df$Species, Predicted = df$scores))\n",
    "    print(cm)\n",
    "\n",
    "    ## Compute and print accuracy \n",
    "    accuracy = round(sum(sapply(1:nrow(cm), function(i) cm[i,i]))/sum(cm), 3)\n",
    "    cat('\\n')\n",
    "    cat(paste('Accuracy = ', as.character(accuracy)), '\\n \\n')                           \n",
    "\n",
    "    ## Compute and print precision, recall and F1\n",
    "    precision = sapply(1:nrow(cm), function(i) cm[i,i]/sum(cm[i,]))\n",
    "    recall = sapply(1:nrow(cm), function(i) cm[i,i]/sum(cm[,i]))    \n",
    "    F1 = sapply(1:nrow(cm), function(i) 2*(recall[i] * precision[i])/(recall[i] + precision[i]))    \n",
    "    metrics = sapply(c(precision, recall, F1), round, 3)        \n",
    "    metrics = t(matrix(metrics, nrow = nrow(cm), ncol = ncol(cm)))       \n",
    "    dimnames(metrics) = list(c('Precision', 'Recall', 'F1'), unique(test$Species))      \n",
    "    print(metrics)\n",
    "}  \n",
    "print_metrics(test, 'Species')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine these results. Notice the following:\n",
    "1. The confusion matrix has dimension 3X3. You can see that most cases are correctly classified with only a few errors. \n",
    "2. The overall accuracy is 0.94. Since the classes are roughly balanced, this metric indicates relatively good performance of the classifier, particularly since it was only trained on 50 cases. \n",
    "3. The precision, recall and  F1 for each of the classes is quite good.\n",
    "|\n",
    "To get a better feel for what the classifier is doing, the code in the cell below displays a set of plots showing correctly (as '+') and incorrectly (as 'o') cases, with the species color-coded. Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine these plots. You can see how the classifier has divided the feature space between the classes. Notice that most of the errors occur in the overlap region between Virginica and Versicolor. This behavior is to be expected.  \n",
    "\n",
    "Is it possible that a random forest model with more trees would separate these cases better? The code in the cell below uses a model with 40 trees (estimators). This model is fit with the training data and displays the evaluation of the model. \n",
    "\n",
    "Execute this code and answer **Question 1** on the course page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Predicted\n",
      "Actual       setosa versicolor virginica\n",
      "  setosa         25          0         0\n",
      "  versicolor      0         23         2\n",
      "  virginica       0          1        24\n",
      "\n",
      "Accuracy =  0.96 \n",
      " \n",
      "          setosa versicolor virginica\n",
      "Precision      1      0.920     0.960\n",
      "Recall         1      0.958     0.923\n",
      "F1             1      0.939     0.941\n"
     ]
    }
   ],
   "source": [
    "set.seed(1115)\n",
    "rf_mod = randomForest(Species ~ ., data = training, ntrees = 100)\n",
    "test$scores = predict(rf_mod, newdata = test)\n",
    "print_metrics(test, 'Species')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are slightly better than for the model with 5 trees. However, this small difference in performance is unlikely to be significant. \n",
    "\n",
    "Like most tree-based models, random forest models have a nice property that **feature importance** is computed during model training. Feature importance can be used as a feature selection method. The `varImp` function from the Caret package performs the calculation.  \n",
    "\n",
    "Execute the code in the cell below to display a plot of the feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAFoCAMAAAC46dgSAAAANlBMVEUAAAAzMzNNTU1oaGh8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\ng4LB438fMJwsT+XL+wAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "options(repr.plot.width=4, repr.plot.height=3)\n",
    "imp = varImp(rf_mod)\n",
    "imp[,'Feature'] = row.names(imp)\n",
    "ggplot(imp, aes(x = Feature, y = Overall)) + geom_point(size = 4) +\n",
    "       ggtitle('Variable importance for iris features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the plot displayed above. Notice that the Speal_Lenght and Sepal_Width have rather low importance. \n",
    "\n",
    "Should these features be dropped from the model? To find out, you will create a model with a reduced feature set and compare the results. As a first step, execute the code in the cell below to create training and test datasets using the reduced features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Predicted\n",
      "Actual       setosa versicolor virginica\n",
      "  setosa         25          0         0\n",
      "  versicolor      0         24         1\n",
      "  virginica       0          1        24\n",
      "\n",
      "Accuracy =  0.973 \n",
      " \n",
      "          setosa versicolor virginica\n",
      "Precision      1       0.96      0.96\n",
      "Recall         1       0.96      0.96\n",
      "F1             1       0.96      0.96\n"
     ]
    }
   ],
   "source": [
    "set.seed(1115)\n",
    "rf_mod = randomForest(Species ~ Petal.Length + Petal.Width, data = training, ntrees = 100)\n",
    "test$scores = predict(rf_mod, newdata = test)\n",
    "print_metrics(test, 'Species')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have executed the code, answer **Question 2** on the course page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are a bit better which may indicate the original model was overfit. Given that a simpler model is more likely to generalize, this model is preferred. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>1</dt>\n",
       "\t\t<dd>0</dd>\n",
       "\t<dt>2</dt>\n",
       "\t\t<dd>0</dd>\n",
       "\t<dt>4</dt>\n",
       "\t\t<dd>0</dd>\n",
       "\t<dt>7</dt>\n",
       "\t\t<dd>0</dd>\n",
       "\t<dt>8</dt>\n",
       "\t\t<dd>0</dd>\n",
       "\t<dt>12</dt>\n",
       "\t\t<dd>0</dd>\n",
       "\t<dt>14</dt>\n",
       "\t\t<dd>0</dd>\n",
       "\t<dt>16</dt>\n",
       "\t\t<dd>0</dd>\n",
       "\t<dt>19</dt>\n",
       "\t\t<dd>0</dd>\n",
       "\t<dt>20</dt>\n",
       "\t\t<dd>0</dd>\n",
       "\t<dt>23</dt>\n",
       "\t\t<dd>0</dd>\n",
       "\t<dt>24</dt>\n",
       "\t\t<dd>0</dd>\n",
       "\t<dt>26</dt>\n",
       "\t\t<dd>0</dd>\n",
       "\t<dt>29</dt>\n",
       "\t\t<dd>0</dd>\n",
       "\t<dt>30</dt>\n",
       "\t\t<dd>0</dd>\n",
       "\t<dt>32</dt>\n",
       "\t\t<dd>0</dd>\n",
       "\t<dt>33</dt>\n",
       "\t\t<dd>0</dd>\n",
       "\t<dt>34</dt>\n",
       "\t\t<dd>0</dd>\n",
       "\t<dt>35</dt>\n",
       "\t\t<dd>0</dd>\n",
       "\t<dt>36</dt>\n",
       "\t\t<dd>0</dd>\n",
       "\t<dt>41</dt>\n",
       "\t\t<dd>0</dd>\n",
       "\t<dt>44</dt>\n",
       "\t\t<dd>0</dd>\n",
       "\t<dt>45</dt>\n",
       "\t\t<dd>0</dd>\n",
       "\t<dt>49</dt>\n",
       "\t\t<dd>0</dd>\n",
       "\t<dt>50</dt>\n",
       "\t\t<dd>0</dd>\n",
       "\t<dt>51</dt>\n",
       "\t\t<dd>0.036</dd>\n",
       "\t<dt>52</dt>\n",
       "\t\t<dd>0</dd>\n",
       "\t<dt>53</dt>\n",
       "\t\t<dd>0.276</dd>\n",
       "\t<dt>54</dt>\n",
       "\t\t<dd>0</dd>\n",
       "\t<dt>60</dt>\n",
       "\t\t<dd>0.024</dd>\n",
       "\t<dt>62</dt>\n",
       "\t\t<dd>0</dd>\n",
       "\t<dt>65</dt>\n",
       "\t\t<dd>0</dd>\n",
       "\t<dt>67</dt>\n",
       "\t\t<dd>0</dd>\n",
       "\t<dt>68</dt>\n",
       "\t\t<dd>0</dd>\n",
       "\t<dt>71</dt>\n",
       "\t\t<dd>0.858</dd>\n",
       "\t<dt>74</dt>\n",
       "\t\t<dd>0.012</dd>\n",
       "\t<dt>76</dt>\n",
       "\t\t<dd>0.024</dd>\n",
       "\t<dt>77</dt>\n",
       "\t\t<dd>0.208</dd>\n",
       "\t<dt>82</dt>\n",
       "\t\t<dd>0</dd>\n",
       "\t<dt>83</dt>\n",
       "\t\t<dd>0</dd>\n",
       "\t<dt>84</dt>\n",
       "\t\t<dd>0.48</dd>\n",
       "\t<dt>86</dt>\n",
       "\t\t<dd>0</dd>\n",
       "\t<dt>87</dt>\n",
       "\t\t<dd>0.012</dd>\n",
       "\t<dt>88</dt>\n",
       "\t\t<dd>0</dd>\n",
       "\t<dt>90</dt>\n",
       "\t\t<dd>0</dd>\n",
       "\t<dt>92</dt>\n",
       "\t\t<dd>0.024</dd>\n",
       "\t<dt>95</dt>\n",
       "\t\t<dd>0</dd>\n",
       "\t<dt>96</dt>\n",
       "\t\t<dd>0</dd>\n",
       "\t<dt>98</dt>\n",
       "\t\t<dd>0</dd>\n",
       "\t<dt>99</dt>\n",
       "\t\t<dd>0</dd>\n",
       "\t<dt>105</dt>\n",
       "\t\t<dd>1</dd>\n",
       "\t<dt>107</dt>\n",
       "\t\t<dd>0.072</dd>\n",
       "\t<dt>109</dt>\n",
       "\t\t<dd>1</dd>\n",
       "\t<dt>111</dt>\n",
       "\t\t<dd>1</dd>\n",
       "\t<dt>112</dt>\n",
       "\t\t<dd>1</dd>\n",
       "\t<dt>113</dt>\n",
       "\t\t<dd>1</dd>\n",
       "\t<dt>114</dt>\n",
       "\t\t<dd>0.914</dd>\n",
       "\t<dt>116</dt>\n",
       "\t\t<dd>1</dd>\n",
       "\t<dt>117</dt>\n",
       "\t\t<dd>1</dd>\n",
       "\t<dt>119</dt>\n",
       "\t\t<dd>1</dd>\n",
       "\t<dt>121</dt>\n",
       "\t\t<dd>1</dd>\n",
       "\t<dt>124</dt>\n",
       "\t\t<dd>0.942</dd>\n",
       "\t<dt>125</dt>\n",
       "\t\t<dd>1</dd>\n",
       "\t<dt>129</dt>\n",
       "\t\t<dd>1</dd>\n",
       "\t<dt>130</dt>\n",
       "\t\t<dd>0.664</dd>\n",
       "\t<dt>131</dt>\n",
       "\t\t<dd>1</dd>\n",
       "\t<dt>132</dt>\n",
       "\t\t<dd>1</dd>\n",
       "\t<dt>134</dt>\n",
       "\t\t<dd>0.688</dd>\n",
       "\t<dt>137</dt>\n",
       "\t\t<dd>1</dd>\n",
       "\t<dt>138</dt>\n",
       "\t\t<dd>1</dd>\n",
       "\t<dt>139</dt>\n",
       "\t\t<dd>0.858</dd>\n",
       "\t<dt>144</dt>\n",
       "\t\t<dd>1</dd>\n",
       "\t<dt>146</dt>\n",
       "\t\t<dd>1</dd>\n",
       "\t<dt>147</dt>\n",
       "\t\t<dd>0.914</dd>\n",
       "\t<dt>148</dt>\n",
       "\t\t<dd>1</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[1] 0\n",
       "\\item[2] 0\n",
       "\\item[4] 0\n",
       "\\item[7] 0\n",
       "\\item[8] 0\n",
       "\\item[12] 0\n",
       "\\item[14] 0\n",
       "\\item[16] 0\n",
       "\\item[19] 0\n",
       "\\item[20] 0\n",
       "\\item[23] 0\n",
       "\\item[24] 0\n",
       "\\item[26] 0\n",
       "\\item[29] 0\n",
       "\\item[30] 0\n",
       "\\item[32] 0\n",
       "\\item[33] 0\n",
       "\\item[34] 0\n",
       "\\item[35] 0\n",
       "\\item[36] 0\n",
       "\\item[41] 0\n",
       "\\item[44] 0\n",
       "\\item[45] 0\n",
       "\\item[49] 0\n",
       "\\item[50] 0\n",
       "\\item[51] 0.036\n",
       "\\item[52] 0\n",
       "\\item[53] 0.276\n",
       "\\item[54] 0\n",
       "\\item[60] 0.024\n",
       "\\item[62] 0\n",
       "\\item[65] 0\n",
       "\\item[67] 0\n",
       "\\item[68] 0\n",
       "\\item[71] 0.858\n",
       "\\item[74] 0.012\n",
       "\\item[76] 0.024\n",
       "\\item[77] 0.208\n",
       "\\item[82] 0\n",
       "\\item[83] 0\n",
       "\\item[84] 0.48\n",
       "\\item[86] 0\n",
       "\\item[87] 0.012\n",
       "\\item[88] 0\n",
       "\\item[90] 0\n",
       "\\item[92] 0.024\n",
       "\\item[95] 0\n",
       "\\item[96] 0\n",
       "\\item[98] 0\n",
       "\\item[99] 0\n",
       "\\item[105] 1\n",
       "\\item[107] 0.072\n",
       "\\item[109] 1\n",
       "\\item[111] 1\n",
       "\\item[112] 1\n",
       "\\item[113] 1\n",
       "\\item[114] 0.914\n",
       "\\item[116] 1\n",
       "\\item[117] 1\n",
       "\\item[119] 1\n",
       "\\item[121] 1\n",
       "\\item[124] 0.942\n",
       "\\item[125] 1\n",
       "\\item[129] 1\n",
       "\\item[130] 0.664\n",
       "\\item[131] 1\n",
       "\\item[132] 1\n",
       "\\item[134] 0.688\n",
       "\\item[137] 1\n",
       "\\item[138] 1\n",
       "\\item[139] 0.858\n",
       "\\item[144] 1\n",
       "\\item[146] 1\n",
       "\\item[147] 0.914\n",
       "\\item[148] 1\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "1\n",
       ":   02\n",
       ":   04\n",
       ":   07\n",
       ":   08\n",
       ":   012\n",
       ":   014\n",
       ":   016\n",
       ":   019\n",
       ":   020\n",
       ":   023\n",
       ":   024\n",
       ":   026\n",
       ":   029\n",
       ":   030\n",
       ":   032\n",
       ":   033\n",
       ":   034\n",
       ":   035\n",
       ":   036\n",
       ":   041\n",
       ":   044\n",
       ":   045\n",
       ":   049\n",
       ":   050\n",
       ":   051\n",
       ":   0.03652\n",
       ":   053\n",
       ":   0.27654\n",
       ":   060\n",
       ":   0.02462\n",
       ":   065\n",
       ":   067\n",
       ":   068\n",
       ":   071\n",
       ":   0.85874\n",
       ":   0.01276\n",
       ":   0.02477\n",
       ":   0.20882\n",
       ":   083\n",
       ":   084\n",
       ":   0.4886\n",
       ":   087\n",
       ":   0.01288\n",
       ":   090\n",
       ":   092\n",
       ":   0.02495\n",
       ":   096\n",
       ":   098\n",
       ":   099\n",
       ":   0105\n",
       ":   1107\n",
       ":   0.072109\n",
       ":   1111\n",
       ":   1112\n",
       ":   1113\n",
       ":   1114\n",
       ":   0.914116\n",
       ":   1117\n",
       ":   1119\n",
       ":   1121\n",
       ":   1124\n",
       ":   0.942125\n",
       ":   1129\n",
       ":   1130\n",
       ":   0.664131\n",
       ":   1132\n",
       ":   1134\n",
       ":   0.688137\n",
       ":   1138\n",
       ":   1139\n",
       ":   0.858144\n",
       ":   1146\n",
       ":   1147\n",
       ":   0.914148\n",
       ":   1\n",
       "\n"
      ],
      "text/plain": [
       "    1     2     4     7     8    12    14    16    19    20    23    24    26 \n",
       "0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 \n",
       "   29    30    32    33    34    35    36    41    44    45    49    50    51 \n",
       "0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.036 \n",
       "   52    53    54    60    62    65    67    68    71    74    76    77    82 \n",
       "0.000 0.276 0.000 0.024 0.000 0.000 0.000 0.000 0.858 0.012 0.024 0.208 0.000 \n",
       "   83    84    86    87    88    90    92    95    96    98    99   105   107 \n",
       "0.000 0.480 0.000 0.012 0.000 0.000 0.024 0.000 0.000 0.000 0.000 1.000 0.072 \n",
       "  109   111   112   113   114   116   117   119   121   124   125   129   130 \n",
       "1.000 1.000 1.000 1.000 0.914 1.000 1.000 1.000 1.000 0.942 1.000 1.000 0.664 \n",
       "  131   132   134   137   138   139   144   146   147   148 \n",
       "1.000 1.000 0.688 1.000 1.000 0.858 1.000 1.000 0.914 1.000 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict(rf_mod, newdata = test, type = 'prob')[,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another example\n",
    "\n",
    "Now, you will try a more complex example using the credit scoring data. You will use the prepared data which had the following preprocessing:\n",
    "1. Cleaning missing values.\n",
    "2. Aggregating categories of certain categorical variables. \n",
    "\n",
    "Execute the code in the cell below to load the features and labels as numpy arrays for the example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 999  12\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'checking_account_status'</li>\n",
       "\t<li>'loan_duration_mo'</li>\n",
       "\t<li>'credit_history'</li>\n",
       "\t<li>'loan_amount'</li>\n",
       "\t<li>'time_employed_yrs'</li>\n",
       "\t<li>'payment_pcnt_income'</li>\n",
       "\t<li>'time_in_residence'</li>\n",
       "\t<li>'property'</li>\n",
       "\t<li>'other_credit_outstanding'</li>\n",
       "\t<li>'number_loans'</li>\n",
       "\t<li>'job_category'</li>\n",
       "\t<li>'bad_credit'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'checking\\_account\\_status'\n",
       "\\item 'loan\\_duration\\_mo'\n",
       "\\item 'credit\\_history'\n",
       "\\item 'loan\\_amount'\n",
       "\\item 'time\\_employed\\_yrs'\n",
       "\\item 'payment\\_pcnt\\_income'\n",
       "\\item 'time\\_in\\_residence'\n",
       "\\item 'property'\n",
       "\\item 'other\\_credit\\_outstanding'\n",
       "\\item 'number\\_loans'\n",
       "\\item 'job\\_category'\n",
       "\\item 'bad\\_credit'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'checking_account_status'\n",
       "2. 'loan_duration_mo'\n",
       "3. 'credit_history'\n",
       "4. 'loan_amount'\n",
       "5. 'time_employed_yrs'\n",
       "6. 'payment_pcnt_income'\n",
       "7. 'time_in_residence'\n",
       "8. 'property'\n",
       "9. 'other_credit_outstanding'\n",
       "10. 'number_loans'\n",
       "11. 'job_category'\n",
       "12. 'bad_credit'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"checking_account_status\"  \"loan_duration_mo\"        \n",
       " [3] \"credit_history\"           \"loan_amount\"             \n",
       " [5] \"time_employed_yrs\"        \"payment_pcnt_income\"     \n",
       " [7] \"time_in_residence\"        \"property\"                \n",
       " [9] \"other_credit_outstanding\" \"number_loans\"            \n",
       "[11] \"job_category\"             \"bad_credit\"              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "credit = read.csv('German_Credit_Preped.csv', header = TRUE)\n",
    "credit = credit[,c('checking_account_status', 'loan_duration_mo', 'credit_history', 'loan_amount',\n",
    "                   'time_employed_yrs', 'payment_pcnt_income', 'time_in_residence', 'property', \n",
    "                   'other_credit_outstanding', 'number_loans', 'job_category', 'bad_credit' )]\n",
    "credit[,'CustomerID'] = NULL\n",
    "print(dim(credit))\n",
    "names(credit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nested cross validation is used to estimate the optimal hyperparameters and perform model selection for the random forest model. Since random forest models are efficient to train, 10 fold cross validation is used. Execute the code in the cell below to define inside and outside fold objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set.seed(1955)\n",
    "## Randomly sample cases to create independent training and test data\n",
    "#partition = createDataPartition(credit[,'bad_credit'], times = 1, p = 0.7, list = FALSE)\n",
    "#training = credit[partition,] # Create the training sample\n",
    "#dim(training)\n",
    "#test = credit[-partition,] # Create the test sample\n",
    "#dim(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>loan_duration_mo</th><th scope=col>loan_amount</th><th scope=col>payment_pcnt_income</th><th scope=col>time_in_residence</th><th scope=col>number_loans</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td> 2.2464282 </td><td> 0.9483849 </td><td>-0.86876113</td><td>-0.7645835 </td><td>-0.7035652 </td></tr>\n",
       "\t<tr><td>-0.7397312 </td><td>-0.4170067 </td><td>-0.86876113</td><td> 0.1414888 </td><td>-0.7035652 </td></tr>\n",
       "\t<tr><td> 1.7487350 </td><td> 1.6323204 </td><td>-0.86876113</td><td> 1.0475610 </td><td>-0.7035652 </td></tr>\n",
       "\t<tr><td> 0.2556552 </td><td> 0.5655086 </td><td> 0.02505181</td><td> 1.0475610 </td><td> 1.0276211 </td></tr>\n",
       "\t<tr><td> 1.2510417 </td><td> 2.0477820 </td><td>-0.86876113</td><td> 1.0475610 </td><td>-0.7035652 </td></tr>\n",
       "\t<tr><td> 0.2556552 </td><td>-0.1552623 </td><td> 0.02505181</td><td> 1.0475610 </td><td>-0.7035652 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllll}\n",
       " loan\\_duration\\_mo & loan\\_amount & payment\\_pcnt\\_income & time\\_in\\_residence & number\\_loans\\\\\n",
       "\\hline\n",
       "\t  2.2464282  &  0.9483849  & -0.86876113 & -0.7645835  & -0.7035652 \\\\\n",
       "\t -0.7397312  & -0.4170067  & -0.86876113 &  0.1414888  & -0.7035652 \\\\\n",
       "\t  1.7487350  &  1.6323204  & -0.86876113 &  1.0475610  & -0.7035652 \\\\\n",
       "\t  0.2556552  &  0.5655086  &  0.02505181 &  1.0475610  &  1.0276211 \\\\\n",
       "\t  1.2510417  &  2.0477820  & -0.86876113 &  1.0475610  & -0.7035652 \\\\\n",
       "\t  0.2556552  & -0.1552623  &  0.02505181 &  1.0475610  & -0.7035652 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "loan_duration_mo | loan_amount | payment_pcnt_income | time_in_residence | number_loans | \n",
       "|---|---|---|---|---|---|\n",
       "|  2.2464282  |  0.9483849  | -0.86876113 | -0.7645835  | -0.7035652  | \n",
       "| -0.7397312  | -0.4170067  | -0.86876113 |  0.1414888  | -0.7035652  | \n",
       "|  1.7487350  |  1.6323204  | -0.86876113 |  1.0475610  | -0.7035652  | \n",
       "|  0.2556552  |  0.5655086  |  0.02505181 |  1.0475610  |  1.0276211  | \n",
       "|  1.2510417  |  2.0477820  | -0.86876113 |  1.0475610  | -0.7035652  | \n",
       "|  0.2556552  | -0.1552623  |  0.02505181 |  1.0475610  | -0.7035652  | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  loan_duration_mo loan_amount payment_pcnt_income time_in_residence\n",
       "1  2.2464282        0.9483849  -0.86876113         -0.7645835       \n",
       "2 -0.7397312       -0.4170067  -0.86876113          0.1414888       \n",
       "3  1.7487350        1.6323204  -0.86876113          1.0475610       \n",
       "4  0.2556552        0.5655086   0.02505181          1.0475610       \n",
       "5  1.2510417        2.0477820  -0.86876113          1.0475610       \n",
       "6  0.2556552       -0.1552623   0.02505181          1.0475610       \n",
       "  number_loans\n",
       "1 -0.7035652  \n",
       "2 -0.7035652  \n",
       "3 -0.7035652  \n",
       "4  1.0276211  \n",
       "5 -0.7035652  \n",
       "6 -0.7035652  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_cols = c('loan_duration_mo', 'loan_amount', 'payment_pcnt_income',\n",
    "             'time_in_residence', 'number_loans')\n",
    "#preProcValues <- preProcess(training[,num_cols], method = c(\"center\", \"scale\"))\n",
    "\n",
    "#training[,num_cols] = predict(preProcValues, training[,num_cols])\n",
    "#test[,num_cols] = predict(preProcValues, test[,num_cols])\n",
    "\n",
    "preProcValues <- preProcess(credit[,num_cols], method = c(\"center\", \"scale\"))\n",
    "credit[,num_cols] = predict(preProcValues, credit[,num_cols])\n",
    "head(credit[,num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below estimates the best hyperparameters using 10 fold cross validation. There are a few points to note here:\n",
    "1. In this case, a grid of two hyperparameters is searched: \n",
    "  - max_features determines the maximum number of features used to determine the splits. Minimizing the number of features can prevent model over-fitted by induces bias. \n",
    "  - min_samples_leaf determines the minimum number of samples or leaves which must be on each terminal node of the tree. Maintaining the minimum number of samples per terminal node is a regularization method. Having too few samples on terminal leaves allows the model training to memorize the data, leading to high variance. Forcing too many samples on the terminal nodes leads to biased predictions. \n",
    "2. Since there is a class imbalance and a difference in the cost to the bank of misclassification of a bad credit risk customer, the \"balanced\" argument is used. The balanced argument ensures that the subsamples used to train each tree have balanced cases. \n",
    "3. The model is fit on each set of hyperparameters from the grid. \n",
    "4. The best estimated hyperparameters are printed. \n",
    "\n",
    "Notice that the model uses regularization rather than feature selection. The hyperparameter search is intended to optimize the level of regularization. \n",
    "\n",
    "Execute this code, examine the result, and answer **Question 3** on the course page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Random Forest \n",
       "\n",
       "999 samples\n",
       " 11 predictor\n",
       "  2 classes: '0', '1' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold, repeated 5 times) \n",
       "Summary of sample sizes: 899, 899, 899, 899, 899, 899, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  mtry  recall   \n",
       "   2    0.7241492\n",
       "  13    0.7840424\n",
       "  24    0.7865709\n",
       "\n",
       "recall was used to select the optimal model using  the largest value.\n",
       "The final value used for the model was mtry = 24."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "recallSummary = function (data, lev = NULL, model = NULL) {\n",
    "                    out = recall(data$obs, data$pred)  \n",
    "                    names(out) <- \"recall\"\n",
    "                    out\n",
    "}\n",
    "\n",
    "fitControl = trainControl(method = \"repeatedcv\",\n",
    "                           number = 10,\n",
    "                           repeats = 5,\n",
    "                           summaryFunction = recallSummary)\n",
    "\n",
    "set.seed(3344)\n",
    "rf_fit <- train(factor(bad_credit) ~ ., # make label a factor since classification model\n",
    "                 data = credit, \n",
    "                 metric = \"recall\", \n",
    "                 method = \"rf\", # Random forest model\n",
    "                 trControl = fitControl,\n",
    "                 weight = c(0.9,0.1),\n",
    "                 verbose = FALSE)\n",
    "rf_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you will run the code in the cell below to perform the outer cross validation of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>fold</th><th scope=col>accuracy</th><th scope=col>precision</th><th scope=col>recall</th><th scope=col>F1</th><th scope=col>AUC</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>1         </td><td>0.75925926</td><td>0.69230769</td><td>0.5000000 </td><td>0.58064516</td><td>0.7606096 </td></tr>\n",
       "\t<tr><td>2         </td><td>0.72727273</td><td>0.50000000</td><td>0.3703704 </td><td>0.42553191</td><td>0.8011831 </td></tr>\n",
       "\t<tr><td>3         </td><td>0.73737374</td><td>0.60714286</td><td>0.5312500 </td><td>0.56666667</td><td>0.7406716 </td></tr>\n",
       "\t<tr><td>4         </td><td>0.70707071</td><td>0.46153846</td><td>0.4444444 </td><td>0.45283019</td><td>0.7337963 </td></tr>\n",
       "\t<tr><td>5         </td><td>0.64646465</td><td>0.39285714</td><td>0.3793103 </td><td>0.38596491</td><td>0.7054187 </td></tr>\n",
       "\t<tr><td>6         </td><td>0.76767677</td><td>0.63636364</td><td>0.4827586 </td><td>0.54901961</td><td>0.7433498 </td></tr>\n",
       "\t<tr><td>7         </td><td>0.68686869</td><td>0.53571429</td><td>0.4545455 </td><td>0.49180328</td><td>0.6648301 </td></tr>\n",
       "\t<tr><td>8         </td><td>0.73737374</td><td>0.60714286</td><td>0.5312500 </td><td>0.56666667</td><td>0.7395056 </td></tr>\n",
       "\t<tr><td>9         </td><td>0.79797980</td><td>0.65384615</td><td>0.6071429 </td><td>0.62962963</td><td>0.8578974 </td></tr>\n",
       "\t<tr><td>10        </td><td>0.76767677</td><td>0.56666667</td><td>0.6296296 </td><td>0.59649123</td><td>0.7705761 </td></tr>\n",
       "\t<tr><td>Mean      </td><td>0.73350168</td><td>0.56535798</td><td>0.4930702 </td><td>0.52452493</td><td>0.7517838 </td></tr>\n",
       "\t<tr><td>std       </td><td>0.04204071</td><td>0.08841227</td><td>0.0815941 </td><td>0.07663994</td><td>0.0495086 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllll}\n",
       " fold & accuracy & precision & recall & F1 & AUC\\\\\n",
       "\\hline\n",
       "\t 1          & 0.75925926 & 0.69230769 & 0.5000000  & 0.58064516 & 0.7606096 \\\\\n",
       "\t 2          & 0.72727273 & 0.50000000 & 0.3703704  & 0.42553191 & 0.8011831 \\\\\n",
       "\t 3          & 0.73737374 & 0.60714286 & 0.5312500  & 0.56666667 & 0.7406716 \\\\\n",
       "\t 4          & 0.70707071 & 0.46153846 & 0.4444444  & 0.45283019 & 0.7337963 \\\\\n",
       "\t 5          & 0.64646465 & 0.39285714 & 0.3793103  & 0.38596491 & 0.7054187 \\\\\n",
       "\t 6          & 0.76767677 & 0.63636364 & 0.4827586  & 0.54901961 & 0.7433498 \\\\\n",
       "\t 7          & 0.68686869 & 0.53571429 & 0.4545455  & 0.49180328 & 0.6648301 \\\\\n",
       "\t 8          & 0.73737374 & 0.60714286 & 0.5312500  & 0.56666667 & 0.7395056 \\\\\n",
       "\t 9          & 0.79797980 & 0.65384615 & 0.6071429  & 0.62962963 & 0.8578974 \\\\\n",
       "\t 10         & 0.76767677 & 0.56666667 & 0.6296296  & 0.59649123 & 0.7705761 \\\\\n",
       "\t Mean       & 0.73350168 & 0.56535798 & 0.4930702  & 0.52452493 & 0.7517838 \\\\\n",
       "\t std        & 0.04204071 & 0.08841227 & 0.0815941  & 0.07663994 & 0.0495086 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "fold | accuracy | precision | recall | F1 | AUC | \n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1          | 0.75925926 | 0.69230769 | 0.5000000  | 0.58064516 | 0.7606096  | \n",
       "| 2          | 0.72727273 | 0.50000000 | 0.3703704  | 0.42553191 | 0.8011831  | \n",
       "| 3          | 0.73737374 | 0.60714286 | 0.5312500  | 0.56666667 | 0.7406716  | \n",
       "| 4          | 0.70707071 | 0.46153846 | 0.4444444  | 0.45283019 | 0.7337963  | \n",
       "| 5          | 0.64646465 | 0.39285714 | 0.3793103  | 0.38596491 | 0.7054187  | \n",
       "| 6          | 0.76767677 | 0.63636364 | 0.4827586  | 0.54901961 | 0.7433498  | \n",
       "| 7          | 0.68686869 | 0.53571429 | 0.4545455  | 0.49180328 | 0.6648301  | \n",
       "| 8          | 0.73737374 | 0.60714286 | 0.5312500  | 0.56666667 | 0.7395056  | \n",
       "| 9          | 0.79797980 | 0.65384615 | 0.6071429  | 0.62962963 | 0.8578974  | \n",
       "| 10         | 0.76767677 | 0.56666667 | 0.6296296  | 0.59649123 | 0.7705761  | \n",
       "| Mean       | 0.73350168 | 0.56535798 | 0.4930702  | 0.52452493 | 0.7517838  | \n",
       "| std        | 0.04204071 | 0.08841227 | 0.0815941  | 0.07663994 | 0.0495086  | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "   fold accuracy   precision  recall    F1         AUC      \n",
       "1  1    0.75925926 0.69230769 0.5000000 0.58064516 0.7606096\n",
       "2  2    0.72727273 0.50000000 0.3703704 0.42553191 0.8011831\n",
       "3  3    0.73737374 0.60714286 0.5312500 0.56666667 0.7406716\n",
       "4  4    0.70707071 0.46153846 0.4444444 0.45283019 0.7337963\n",
       "5  5    0.64646465 0.39285714 0.3793103 0.38596491 0.7054187\n",
       "6  6    0.76767677 0.63636364 0.4827586 0.54901961 0.7433498\n",
       "7  7    0.68686869 0.53571429 0.4545455 0.49180328 0.6648301\n",
       "8  8    0.73737374 0.60714286 0.5312500 0.56666667 0.7395056\n",
       "9  9    0.79797980 0.65384615 0.6071429 0.62962963 0.8578974\n",
       "10 10   0.76767677 0.56666667 0.6296296 0.59649123 0.7705761\n",
       "11 Mean 0.73350168 0.56535798 0.4930702 0.52452493 0.7517838\n",
       "12 std  0.04204071 0.08841227 0.0815941 0.07663994 0.0495086"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "score_model = function(df, threshold){\n",
    "    df$score = ifelse(df$probs[,2] > threshold, 1, 0)\n",
    "    df\n",
    "}\n",
    "\n",
    "binary.eval <- function(df, fold){ \n",
    "  # First step is to find the TP, FP, TN, FN cases\n",
    "  df$conf = ifelse(df$bad_credit == 1 & df$score == 1, 'TP',\n",
    "                    ifelse(df$bad_credit == 0 & df$score == 1, 'FP',\n",
    "                           ifelse(df$bad_credit == 0 & df$score == 0, 'TN', 'FN')))\n",
    "  # Elements of the confusion matrix\n",
    "  TP = length(df[df$conf == 'TP', 'conf'])\n",
    "  FP = length(df[df$conf == 'FP', 'conf'])\n",
    "  TN = length(df[df$conf == 'TN', 'conf'])\n",
    "  FN = length(df[df$conf == 'FN', 'conf'])\n",
    "  \n",
    "  ## Confusion matrix as data frame\n",
    "  out = data.frame(Negative = c(TN, FN), Positive = c(FP, TP))\n",
    "  row.names(out) = c('TrueNeg', 'TruePos')\n",
    "    \n",
    "  # Compute AUC with ROCR package\n",
    "  pred_obj = prediction(df$probs[,2], df$bad_credit)\n",
    "  AUC = performance(pred_obj,\"auc\")@y.values[[1]]  \n",
    "  \n",
    "  # Compute and print metrics\n",
    "  P = TP/(TP + FP)\n",
    "  R = TP/(TP + FN)  \n",
    "  F1 = 2*P*R/(P+R) \n",
    "  data.frame = data.frame(fold = as.character(fold),\n",
    "                          accuracy = (TP + TN)/(TP + TN + FP + FN),\n",
    "                          precision = P,\n",
    "                          recall = R,\n",
    "                          F1 = F1,\n",
    "                          AUC = AUC)\n",
    " }\n",
    "\n",
    "\n",
    "Create_Folds = function(df, folds){\n",
    "    ## Create a vector of the fold assignments\n",
    "    nrows = nrow(df)\n",
    "    ncount = nrows/folds\n",
    "    ## Concatenate vectors of fold number\n",
    "    fold = rep(1, ncount)\n",
    "    for(i in seq(2, folds, by = 1)){\n",
    "        fold = c(fold, rep(i, ncount))\n",
    "    }\n",
    "    fold\n",
    "}\n",
    "\n",
    "Fit_Mod = function(training, test){\n",
    "    set.seed(5566)\n",
    "    rf_mod = randomForest(factor(bad_credit) ~ ., data = training, mtry = 10,\n",
    "                         classwt = c(0.9, 0.1))\n",
    "    test$probs = predict(rf_mod, newdata = test, type = 'prob')\n",
    "    test = score_model(test, 0.5)\n",
    "    test\n",
    "}\n",
    "\n",
    "Cross_Validate_Mod = function(df, folds){\n",
    "    ## Create a vector of the fold assignments\n",
    "    fold = Create_Folds(df, folds)\n",
    "    \n",
    "    ## Randomly shuffle the rows of the data frame\n",
    "    shuffle = sample(seq(1, nrow(df), by = 1))\n",
    "    df = df[shuffle,]\n",
    "    \n",
    "    ## Loop over number of folds to fit and evaluate the model\n",
    "    training = df[fold != 1,]\n",
    "    test = df[fold == 1, ]\n",
    "    test = Fit_Mod(training, test)\n",
    "    evals = binary.eval(test, 1)\n",
    "    for(i in seq(2, folds, by = 1)){\n",
    "        training = df[fold != i,]\n",
    "        test = df[fold == i, ]\n",
    "        test = Fit_Mod(training, test)\n",
    "        evals = rbind(evals, binary.eval(test, i))\n",
    "    }\n",
    "    \n",
    "    ## Compute some summary statistics and append to the data rame\n",
    "    evals = rbind(evals, data.frame(fold = 'Mean',\n",
    "                          accuracy = mean(evals[,2]),\n",
    "                          precision = mean(evals[,3]),\n",
    "                          recall = mean(evals[,4]),\n",
    "                          F1 = mean(evals[,5]),\n",
    "                          AUC = mean(evals[,6])))\n",
    "    \n",
    "    evals = rbind(evals, data.frame(fold = 'std',\n",
    "                          accuracy = sd(evals[,2]),\n",
    "                          precision = sd(evals[,3]),\n",
    "                          recall = sd(evals[,4]),\n",
    "                          F1 = sd(evals[,5]),\n",
    "                          AUC = sd(evals[,6])))\n",
    "    evals\n",
    "}\n",
    "\n",
    "Cross_Validate_Mod(credit, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine these results. Notice that the standard deviation of the mean of the AUC is more than an order of magnitude smaller than the mean. This indicates that this model is likely to generalize well. \n",
    "\n",
    "Now, you will build and test a model using the estimated optimal hyperparameters. As a first step, execute the code in the cell below to create training and testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Randomly sample cases to create independent training and test data\n",
    "nr.seed(1115)\n",
    "indx = range(Features.shape[0])\n",
    "indx = ms.train_test_split(indx, test_size = 300)\n",
    "x_train = Features[indx[0],:]\n",
    "y_train = np.ravel(Labels[indx[0]])\n",
    "x_test = Features[indx[1],:]\n",
    "y_test = np.ravel(Labels[indx[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below defines a random forest model object using the estimated optimal model hyperparameters and then fits the model to the training data. Execute this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed(1115)\n",
    "rf_mod = RandomForestClassifier(n_estimators=40, class_weight = \"balanced\", max_features = 10, min_samples_leaf = 10) \n",
    "rf_mod.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the hyperparemeters of the random forest model object reflect those specified. \n",
    "\n",
    "The code in the cell below scores and prints evaluation metrics for the model, using the test data subset. \n",
    "\n",
    "Execute this code, examine the results, and answer **Question 4** on the course page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(labels, scores):\n",
    "    metrics = sklm.precision_recall_fscore_support(labels, scores)\n",
    "    conf = sklm.confusion_matrix(labels, scores)\n",
    "    print('                 Confusion matrix')\n",
    "    print('                 Score positive    Score negative')\n",
    "    print('True positive    %6d' % conf[0,0] + '          %5d' % conf[0,1])\n",
    "    print('True negative    %6d' % conf[1,0] + '          %5d' % conf[1,1])\n",
    "    print('')\n",
    "    print('Accuracy        %0.2f' % sklm.accuracy_score(labels, scores))\n",
    "    print('AUC             %0.2f' % sklm.roc_auc_score(labels, scores))\n",
    "    print('Macro precision %0.2f' % float((float(metrics[0][0]) + float(metrics[0][1]))/2.0))\n",
    "    print('Macro recall    %0.2f' % float((float(metrics[1][0]) + float(metrics[1][1]))/2.0))\n",
    "    print(' ')\n",
    "    print('             Positive   Negative')\n",
    "    print('Num case    %0.2f' % metrics[3][0] + '       %0.2f' % metrics[3][1])\n",
    "    print('Precision   %0.2f' % metrics[0][0] + '          %0.2f' % metrics[0][1])\n",
    "    print('Recall      %0.2f' % metrics[1][0] + '          %0.2f' % metrics[1][1])\n",
    "    print('F1          %0.2f' % metrics[2][0] + '          %0.2f' % metrics[2][1])\n",
    "\n",
    "scores = rf_mod.predict(x_test)\n",
    "print_metrics(y_test, scores)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, these performance metrics look quite good. A large majority of negative (bad credit) cases are identified at the expense of significant false positives. The reported AUC is within a standard deviation of the figure obtained with cross validation indicating that the model is generalizing well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab you have accomplished the following:\n",
    "1. Used a random forest model to classify the cases of the iris data. A model with more trees had marginally lower error rates, but likely not significantly different.\n",
    "2. Applied feature importance was used for feature selection with the iris data. The model created and evaluated with the reduced feature set had essentially the same performance as the model with more features.  \n",
    "2. Used 10 fold to find estimated optimal hyperparameters for a random forest model to classify credit risk cases. The model appears to generalize well.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
